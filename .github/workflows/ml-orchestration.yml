name: ML Orchestration

# =============================================================================
# CONSOLIDATED ML PIPELINE
# =============================================================================
# Runs the nightly ML suite including forecasts, options processing, model
# health checks, and evaluation. Triggers automatically after Daily Data Refresh
# or on a nightly schedule.
#
# Consolidates:
#   - ml-forecast.yml (nightly forecasts)
#   - ml-evaluation.yml (feedback loop)
#   - data-quality-monitor.yml (quality checks)
#   - drift-monitoring.yml (staleness detection)
#   - options-nightly.yml (options processing)
#
# Note: daily-options-scrape.yml runs separately during market hours for
# real-time options data and is NOT consolidated into this workflow.
#
# Data Flow:
#   Daily Data Refresh (success) â†’ ML Orchestration
#                                    â”œâ”€â”€ ml-forecast
#                                    â”œâ”€â”€ options-processing
#                                    â”œâ”€â”€ model-health
#                                    â””â”€â”€ smoke-tests
# =============================================================================

on:
  # Trigger after daily data refresh completes
  workflow_run:
    workflows: ["Daily Data Refresh"]
    types:
      - completed
  # Nightly schedule for post-market processing
  schedule:
    - cron: "0 4 * * 1-5"  # 4:00 UTC = 22:00 CST (after market close)
  workflow_dispatch:
    inputs:
      job_filter:
        description: 'Specific job to run (leave empty for all)'
        required: false
        type: choice
        options:
          - ''
          - ml-forecast
          - options-processing
          - model-health
          - smoke-tests
      symbol:
        description: 'Single symbol to process (leave empty for all)'
        required: false
        type: string

concurrency:
  group: ml-orchestration-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel nightly processing

permissions:
  contents: read

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  ALPACA_API_KEY: ${{ secrets.ALPACA_API_KEY }}
  ALPACA_API_SECRET: ${{ secrets.ALPACA_API_SECRET }}

jobs:
  # ===========================================================================
  # CHECK TRIGGER
  # ===========================================================================
  check-trigger:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      trigger_source: ${{ steps.check.outputs.trigger_source }}
    steps:
      - name: Check workflow_run status
        id: check
        run: |
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            if [ "${{ github.event.workflow_run.conclusion }}" != "success" ]; then
              echo "Daily Data Refresh did not succeed - skipping ML orchestration"
              echo "should_run=false" >> $GITHUB_OUTPUT
              exit 0
            fi
            echo "trigger_source=workflow_run" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            echo "trigger_source=schedule" >> $GITHUB_OUTPUT
          else
            echo "trigger_source=manual" >> $GITHUB_OUTPUT
          fi
          echo "should_run=true" >> $GITHUB_OUTPUT

  # ===========================================================================
  # ML FORECAST (Ensemble predictions)
  # ===========================================================================
  ml-forecast:
    needs: check-trigger
    if: |
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'ml-forecast')
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Validate OHLC data quality before training
        run: |
          cd ml
          python -c "
          import os
          from dotenv import load_dotenv
          load_dotenv()
          
          from src.data.data_validator import OHLCValidator
          from src.data.supabase_db import db
          from src.scripts.universe_utils import get_symbol_universe
          
          validator = OHLCValidator()
          universe = get_symbol_universe()
          symbols = universe.get('symbols', []) or ['SPY', 'AAPL', 'NVDA', 'MSFT']
          
          # Limit to top 10 symbols for performance
          symbols_to_check = symbols[:10]
          validation_errors = []
          
          for symbol in symbols_to_check:
              symbol = symbol.strip()
              if not symbol:
                  continue
              try:
                  # Check daily timeframe (used for training)
                  df = db.fetch_ohlc_bars(symbol, timeframe='d1', limit=252)
                  if df.empty:
                      print(f'âš ï¸ {symbol}: No data found')
                      continue
                  
                  df, result = validator.validate(df, fix_issues=False)
                  if not result.is_valid:
                      error_msg = f'{symbol}: {result.issues}'
                      validation_errors.append(error_msg)
                      print(f'âŒ {error_msg}')
                  else:
                      print(f'âœ… {symbol}: OHLC validation passed ({len(df)} bars)')
              except Exception as e:
                  validation_errors.append(f'{symbol}: {str(e)}')
                  print(f'âŒ {symbol}: Validation error - {e}')
          
          if validation_errors:
              print('')
              print('âŒ OHLC validation failed for some symbols:')
              for error in validation_errors:
                  print(f'  - {error}')
              print('')
              print('::error::OHLC data quality issues detected. ML training may produce unreliable results.')
              exit(1)
          
          print('')
          print('âœ… OHLC validation passed for all checked symbols')
          "

      - name: Generate ML forecasts
        env:
          USE_ENSEMBLE_FORECASTER: "true"
          MIN_BARS_FOR_TRAINING: "50"
        run: |
          cd ml
          
          if [ -n "${{ inputs.symbol }}" ]; then
            echo "ðŸ“Š Forecasting single symbol: ${{ inputs.symbol }}"
            python -m src.forecast_job --symbol "${{ inputs.symbol }}"
          else
            echo "ðŸ“Š Forecasting all watchlist symbols"
            python -m src.forecast_job
          fi

      - name: Job summary
        if: always()
        run: |
          echo "## ML Forecast (Ensemble)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model**: Ensemble (RF + XGBoost)" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # OPTIONS PROCESSING (Nightly backfill and snapshots)
  # ===========================================================================
  options-processing:
    needs: check-trigger
    if: |
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'options-processing')
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Process options data
        run: |
          cd ml
          
          if [ -n "${{ inputs.symbol }}" ]; then
            echo "ðŸ“Š Processing options for: ${{ inputs.symbol }}"
            python src/scripts/backfill_options.py --symbol "${{ inputs.symbol }}" || echo "âš ï¸ Options backfill warning"
          else
            echo "ðŸ“Š Processing options for all watchlist"
            python src/scripts/backfill_options.py --all || echo "âš ï¸ Options backfill warning"
          fi

      - name: Capture options snapshots
        run: |
          cd ml
          python src/options_snapshot_job.py || echo "âš ï¸ Snapshot capture warning"

      - name: Job summary
        if: always()
        run: |
          echo "## Options Processing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Updated" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_chain_snapshots\`" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_ranks\`" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # MODEL HEALTH (Evaluation, drift detection, data quality)
  # ===========================================================================
  model-health:
    needs: [check-trigger, ml-forecast]
    if: |
      always() &&
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'model-health')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run ML evaluation (feedback loop)
        run: |
          cd ml
          echo "ðŸ“Š Running ML evaluation..."
          python -m src.evaluation_job || echo "âš ï¸ Evaluation completed with warnings"

      - name: Run unified validation
        run: |
          cd ml
          echo "ðŸ“Š Running unified validation with real database scores..."
          python -c "
          import os
          import asyncio
          import sys
          from dotenv import load_dotenv
          load_dotenv()
          
          sys.path.insert(0, 'src')
          
          from src.services.validation_service import ValidationService
          
          # Core symbols to validate
          SYMBOLS = ['AAPL', 'NVDA', 'MSFT', 'TSLA', 'META', 'AMD', 'CRWD', 'GOOGL', 'AMZN']
          
          service = ValidationService()
          
          print('=' * 60)
          print('UNIFIED VALIDATION REPORT (Real Database Scores)')
          print('=' * 60)
          
          drift_alerts = []
          validation_errors = []
          
          async def validate_symbol(symbol):
              try:
                  # Fetch actual validation from database
                  # Use BULLISH as default direction (can be enhanced to fetch actual direction)
                  result = await service.get_live_validation(symbol, 'BULLISH')
                  
                  status = result.get_status_emoji()
                  print(f'{status} {symbol}: {result.unified_confidence:.1%} confidence')
                  print(f'   Drift: {result.drift_severity} ({result.drift_magnitude:.0%})')
                  print(f'   Consensus: {result.consensus_direction}')
                  
                  if result.drift_detected:
                      drift_alerts.append({
                          'symbol': symbol,
                          'severity': result.drift_severity,
                          'magnitude': result.drift_magnitude,
                          'recommendation': result.recommendation
                      })
                  
                  if result.retraining_trigger:
                      print(f'   âš ï¸ RETRAINING TRIGGERED: {result.retraining_reason}')
                  
                  return None
              except Exception as e:
                  error_msg = f'{symbol}: {str(e)}'
                  validation_errors.append(error_msg)
                  print(f'âš ï¸ {error_msg}')
                  return error_msg
          
          # Run async validation for all symbols
          async def run_all_validations():
              tasks = [validate_symbol(symbol) for symbol in SYMBOLS]
              await asyncio.gather(*tasks)
          
          asyncio.run(run_all_validations())
          
          print('')
          print('=' * 60)
          
          if validation_errors:
              print(f'âš ï¸ VALIDATION ERRORS: {len(validation_errors)} symbols')
              for error in validation_errors:
                  print(f'   - {error}')
              print('')
          
          if drift_alerts:
              print(f'âš ï¸ DRIFT ALERTS: {len(drift_alerts)} symbols')
              for alert in drift_alerts:
                  print(f'   - {alert[\"symbol\"]}: {alert[\"severity\"]} drift ({alert[\"magnitude\"]:.0%})')
          else:
              print('âœ… No drift alerts')
          
          print('=' * 60)
          print('âœ… Unified validation complete')
          " || echo "âš ï¸ Unified validation completed with warnings"

      - name: Update model weights
        run: |
          curl -s -X POST \
            "${SUPABASE_URL}/rest/v1/rpc/trigger_weight_update" \
            -H "apikey: ${SUPABASE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_KEY}" \
            -H "Content-Type: application/json" \
            -d '{}' | jq . || echo "Weight update skipped"

      - name: Check drift and staleness
        run: |
          cd ml
          echo "ðŸ“Š Checking model drift and data staleness..."
          python -c "
          import sys
          sys.path.insert(0, '.')
          from src.monitoring.forecast_staleness import check_all_staleness
          
          print('ðŸ“Š Checking data staleness...')
          try:
              staleness = check_all_staleness()
              any_stale = False
              for name, result in staleness.items():
                  print(f'{result.icon} {name}: {result.message}')
                  if not result.is_ok:
                      any_stale = True
              if any_stale:
                  print('::warning::Some data sources are stale!')
          except Exception as e:
              print(f'âš ï¸ Staleness check error: {e}')
          
          print('âœ… Drift monitoring complete')
          " || echo "âš ï¸ Drift check completed with warnings"

      - name: Data quality validation
        continue-on-error: true
        run: |
          SYMBOLS="AAPL,MSFT,NVDA,TSLA,META"
          
          echo "ðŸ“Š Validating data quality for: $SYMBOLS"
          
          chmod +x scripts/validate_data_quality.sh 2>/dev/null || true
          if [ -f scripts/validate_data_quality.sh ]; then
            ./scripts/validate_data_quality.sh "$SYMBOLS" || echo "âš ï¸ Data quality issues detected"
          else
            echo "Validation script not found, skipping..."
          fi

      - name: Job summary
        if: always()
        run: |
          echo "## Model Health" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Checks Performed" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… ML Evaluation (feedback loop)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Unified validation (confidence reconciliation)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Model weight updates" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Drift detection (25%/50%/75% thresholds)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Multi-TF reconciliation" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Data staleness check" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Data quality validation" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # SMOKE TESTS (Basic validation)
  # ===========================================================================
  smoke-tests:
    needs: [check-trigger, ml-forecast, options-processing, model-health]
    if: |
      always() &&
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'smoke-tests')
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Run smoke tests
        run: |
          cd ml
          
          echo "ðŸ“Š Running quick smoke tests..."
          
          # Test that key tables are accessible
          python -c "
          import os
          from dotenv import load_dotenv
          load_dotenv()
          
          from src.data.supabase_db import db
          
          # Check OHLC data
          df = db.fetch_ohlc_bars('SPY', timeframe='d1', limit=5)
          assert len(df) > 0, 'No OHLC data found'
          print(f'âœ… OHLC bars: {len(df)} records')
          
          # Check forecasts exist
          try:
              forecasts = db.client.table('ml_forecasts').select('*').limit(1).execute()
              print(f'âœ… ML forecasts table accessible')
          except Exception as e:
              print(f'âš ï¸ ML forecasts check: {e}')
          
          print('âœ… Smoke tests passed')
          "

      - name: Log provider coverage snapshot
        if: always()
        env:
          WORKFLOW_SYMBOL: ${{ inputs.symbol }}
        run: >-
          set -euo pipefail;
          SYMBOL_SOURCE="${WORKFLOW_SYMBOL:-${SWIFTBOLT_SYMBOLS:-}}";
          SYMBOL_SOURCE="${SYMBOL_SOURCE#,}";
          SYMBOL="${SYMBOL_SOURCE%%,*}";
          if [ -z "$SYMBOL" ]; then SYMBOL="SPY"; fi;
          TIMEFRAME_SOURCE="${SWIFTBOLT_TIMEFRAMES:-m15,h1,h4,d1,w1}";
          SUMMARY_FILE="${GITHUB_STEP_SUMMARY:-/tmp/provider-coverage.txt}";
          {
            echo "## Provider Coverage Snapshot";
            echo "";
            echo "| Symbol | Timeframe | Hist Provider | Hist Bars | Intraday Provider | Intraday Bars |";
            echo "|--------|-----------|---------------|-----------|-------------------|---------------|";
          } >> "$SUMMARY_FILE";
          IFS=',' read -ra TIMEFRAMES <<< "$TIMEFRAME_SOURCE";
          for TF in "${TIMEFRAMES[@]}"; do
            TF_TRIMMED="$(echo "$TF" | xargs)";
            [ -z "$TF_TRIMMED" ] && continue;
            case "$TF_TRIMMED" in
              w1) DAYS=730 ;;
              d1) DAYS=365 ;;
              *) DAYS=120 ;;
            esac;
            PAYLOAD=$(printf '{"symbol":"%s","timeframe":"%s","days":%s}' "$SYMBOL" "$TF_TRIMMED" "$DAYS");
            RESPONSE=$(curl -sS \
              -H "apikey: ${SUPABASE_KEY}" \
              -H "Authorization: Bearer ${SUPABASE_KEY}" \
              -H "Content-Type: application/json" \
              -X POST \
              -d "$PAYLOAD" \
              "${SUPABASE_URL}/functions/v1/chart-data-v2") || true;
            if [ -z "$RESPONSE" ]; then
              printf "| %s | %s | %s | %s | %s | %s |\n" "$SYMBOL" "$TF_TRIMMED" "error" "0" "error" "0" >> "$SUMMARY_FILE";
              echo "::warning::No response for $SYMBOL/$TF_TRIMMED";
              continue;
            fi;
            HIST_PROVIDER=$(echo "$RESPONSE" | jq -r '.layers.historical.provider // "unknown"') || HIST_PROVIDER="unknown";
            HIST_COUNT=$(echo "$RESPONSE" | jq -r '.layers.historical.count // 0' || echo "0");
            INTRA_PROVIDER=$(echo "$RESPONSE" | jq -r '.layers.intraday.provider // "unknown"') || INTRA_PROVIDER="unknown";
            INTRA_COUNT=$(echo "$RESPONSE" | jq -r '.layers.intraday.count // 0' || echo "0");
            printf "| %s | %s | %s | %s | %s | %s |\n" \
              "$SYMBOL" "$TF_TRIMMED" "$HIST_PROVIDER" "$HIST_COUNT" "$INTRA_PROVIDER" "$INTRA_COUNT" >> "$SUMMARY_FILE";
            echo "::notice::[$SYMBOL/$TF_TRIMMED] historical=$HIST_PROVIDER($HIST_COUNT) intraday=$INTRA_PROVIDER($INTRA_COUNT)";
          done

      - name: Final orchestration summary
        if: always()
        run: |
          echo "## ML Orchestration Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ML Forecast | ${{ needs.ml-forecast.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Options Processing | ${{ needs.options-processing.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Health | ${{ needs.model-health.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ job.status }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Trigger Information" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: ${{ needs.check-trigger.outputs.trigger_source }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Filter**: ${{ inputs.job_filter || 'All jobs' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Tables Updated" >> $GITHUB_STEP_SUMMARY
          echo "- \`ml_forecasts\` - Directional predictions" >> $GITHUB_STEP_SUMMARY
          echo "- \`ml_forecasts (unified)\` - Unified validation with drift metadata" >> $GITHUB_STEP_SUMMARY
          echo "- \`forecast_evaluations\` - Historical accuracy" >> $GITHUB_STEP_SUMMARY
          echo "- \`model_weights\` - Ensemble weights" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_chain_snapshots\` - Options data" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_ranks\` - ML-scored contracts" >> $GITHUB_STEP_SUMMARY
