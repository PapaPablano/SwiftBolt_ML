name: ML Orchestration

# =============================================================================
# CONSOLIDATED ML PIPELINE
# =============================================================================
# Runs the nightly ML suite including forecasts, options processing, model
# health checks, and evaluation. Runs on schedule or manual trigger.
#
# Consolidates:
#   - ml-forecast.yml (nightly forecasts)
#   - ml-evaluation.yml (feedback loop)
#   - data-quality-monitor.yml (quality checks)
#   - drift-monitoring.yml (staleness detection)
#   - options-nightly.yml (options processing)
#
# Note: daily-options-scrape.yml runs separately during market hours for
# real-time options data and is NOT consolidated into this workflow.
#
# Data Flow:
#   ML Orchestration
#     â”œâ”€â”€ ml-forecast
#     â”œâ”€â”€ options-processing
#     â”œâ”€â”€ model-health
#     â””â”€â”€ smoke-tests
# =============================================================================

on:
  # Nightly schedule for post-market processing
  schedule:
    - cron: "0 4 * * 1-5"  # 4:00 UTC = 22:00 CST (after market close)
  workflow_dispatch:
    inputs:
      job_filter:
        description: 'Specific job to run (leave empty for all)'
        required: false
        type: choice
        options:
          - ''
          - ml-forecast
          - options-processing
          - model-health
          - smoke-tests
      symbol:
        description: 'Single symbol to process (leave empty for all)'
        required: false
        type: string

concurrency:
  group: ml-orchestration-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel nightly processing

permissions:
  contents: read

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  ALPACA_API_KEY: ${{ secrets.ALPACA_API_KEY }}
  ALPACA_API_SECRET: ${{ secrets.ALPACA_API_SECRET }}

jobs:
  # ===========================================================================
  # CHECK TRIGGER
  # ===========================================================================
  check-trigger:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      trigger_source: ${{ steps.check.outputs.trigger_source }}
    steps:
      - name: Check trigger status
        id: check
        run: |
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "trigger_source=schedule" >> $GITHUB_OUTPUT
          else
            echo "trigger_source=manual" >> $GITHUB_OUTPUT
          fi
          echo "should_run=true" >> $GITHUB_OUTPUT

  # ===========================================================================
  # ML FORECAST (Ensemble predictions)
  # ===========================================================================
  ml-forecast:
    needs: check-trigger
    if: |
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'ml-forecast')
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('ml/requirements.txt', 'ml/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Validate OHLC data quality before training
        run: |
          cd ml
          python -m src.scripts.validate_ohlc_before_training --limit 10

      - name: Generate ML forecasts (Unified)
        env:
          USE_ENSEMBLE_FORECASTER: "true"
          MIN_BARS_FOR_TRAINING: "50"
          REDIS_FEATURE_CACHE: "false"  # Enable when Redis is available
          ENABLE_ADVANCED_ENSEMBLE: ${{ vars.ENABLE_ADVANCED_ENSEMBLE || 'true' }}  # Enable 6-model ensemble
          ENABLE_RF: ${{ vars.ENABLE_RF || 'false' }}  # Align with framework (disable RF)
          ENABLE_GB: ${{ vars.ENABLE_GB || 'true' }}  # XGBoost
          ENABLE_ARIMA_GARCH: ${{ vars.ENABLE_ARIMA_GARCH || 'true' }}
          ENABLE_PROPHET: ${{ vars.ENABLE_PROPHET || 'false' }}
          ENABLE_LSTM: ${{ vars.ENABLE_LSTM || 'true' }}
          ENABLE_TRANSFORMER: ${{ vars.ENABLE_TRANSFORMER || 'false' }}  # Disabled: causes workflow failures (TensorFlow not available)
        run: |
          cd ml
          
          if [ -n "${{ inputs.symbol }}" ]; then
            echo "ðŸ“Š Forecasting single symbol: ${{ inputs.symbol }}"
            python -m src.unified_forecast_job --symbol "${{ inputs.symbol }}"
          else
            echo "ðŸ“Š Forecasting resolved symbols: ${SWIFTBOLT_SYMBOLS}"
            python -m src.unified_forecast_job --symbols "${SWIFTBOLT_SYMBOLS}"
          fi

      - name: Job summary
        if: always()
        run: |
          echo "## ML Forecast (Ensemble)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model**: Ensemble (XGBoost + ARIMA-GARCH + LSTM + Transformer)" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # OPTIONS PROCESSING (Nightly backfill and snapshots)
  # ===========================================================================
  options-processing:
    needs: check-trigger
    if: |
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'options-processing')
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('ml/requirements.txt', 'ml/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}
          tradier-api-key: ${{ secrets.TRADIER_API_KEY }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Process options data
        run: |
          cd ml

          if [ -n "${{ inputs.symbol }}" ]; then
            echo "ðŸ“Š Processing options for: ${{ inputs.symbol }}"
            python src/scripts/backfill_options.py --symbol "${{ inputs.symbol }}"
          else
            echo "ðŸ“Š Processing options for all watchlist"
            python src/scripts/backfill_options.py --all
          fi

      - name: Capture options snapshots
        run: |
          cd ml
          python src/options_snapshot_job.py

      - name: Job summary
        if: always()
        run: |
          echo "## Options Processing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Updated" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_chain_snapshots\`" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_ranks\`" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # MODEL HEALTH (Evaluation, drift detection, data quality)
  # ===========================================================================
  model-health:
    needs: [check-trigger, ml-forecast]
    if: |
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'model-health') &&
      (needs.ml-forecast.result == 'success' || needs.ml-forecast.result == 'skipped')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('ml/requirements.txt', 'ml/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run Daily ML evaluation (feedback loop)
        run: |
          cd ml
          echo "ðŸ“Š Running daily ML evaluation (1D, 1W, 1M)..."
          python -m src.evaluation_job_daily

      - name: Populate live_predictions from evaluations
        run: |
          cd ml
          echo "ðŸ“Š Populating live_predictions table from recent evaluations..."
          python -m src.scripts.populate_live_predictions --days-back 30

      - name: Run unified validation
        run: |
          cd ml
          echo "ðŸ“Š Running unified validation with real database scores..."
          python -m src.scripts.run_unified_validation_report

      - name: Update model weights
        continue-on-error: true
        run: |
          echo "ðŸ“Š Updating model weights based on recent performance..."
          response=$(curl -s -X POST \
            "${SUPABASE_URL}/rest/v1/rpc/trigger_weight_update" \
            -H "apikey: ${SUPABASE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_KEY}" \
            -H "Content-Type: application/json" \
            -d '{}' 2>&1)
          
          if echo "$response" | grep -q "PGRST"; then
            echo "âš ï¸ Weight update RPC returned error (may need evaluation data first)"
            echo "$response" | head -3
          else
            echo "$response" | jq . || echo "$response"
          fi

      - name: Check drift and staleness
        continue-on-error: true
        run: |
          cd ml
          python -m src.scripts.check_model_health

      - name: Data quality validation
        continue-on-error: true
        run: |
          SYMBOLS="AAPL,MSFT,NVDA,TSLA,META"
          
          echo "ðŸ“Š Validating data quality for: $SYMBOLS"
          
          # Set DATABASE_URL if not set (for validation script)
          if [ -z "$DATABASE_URL" ] && [ -n "$SUPABASE_URL" ] && [ -n "$SUPABASE_KEY" ]; then
            # Extract connection string from SUPABASE_URL if possible
            # For now, skip if DATABASE_URL not available
            echo "â„¹ï¸  DATABASE_URL not set - skipping external validation script"
            echo "   (OHLC validation above is sufficient)"
          else
            chmod +x scripts/validate_data_quality.sh 2>/dev/null || true
            if [ -f scripts/validate_data_quality.sh ]; then
              ./scripts/validate_data_quality.sh "$SYMBOLS" || echo "âš ï¸ Data quality issues detected"
            else
              echo "Validation script not found, skipping..."
            fi
          fi

      - name: Job summary
        if: always()
        run: |
          echo "## Model Health" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Checks Performed" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… ML Evaluation (feedback loop)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Unified validation (confidence reconciliation)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Model weight updates" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Drift detection (25%/50%/75% thresholds)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Multi-TF reconciliation" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Data staleness check" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Data quality validation" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # SMOKE TESTS (Basic validation)
  # ===========================================================================
  smoke-tests:
    needs: [check-trigger, ml-forecast, options-processing, model-health]
    if: |
      !cancelled() &&
      needs.check-trigger.outputs.should_run == 'true' &&
      (inputs.job_filter == '' || inputs.job_filter == 'smoke-tests')
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('ml/requirements.txt', 'ml/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Setup ML Environment
        uses: ./.github/actions/setup-ml-env
        with:
          supabase-url: ${{ secrets.SUPABASE_URL }}
          supabase-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          database-url: ${{ secrets.DATABASE_URL }}
          alpaca-api-key: ${{ secrets.ALPACA_API_KEY }}
          alpaca-api-secret: ${{ secrets.ALPACA_API_SECRET }}

      - name: Resolve processing universe
        env:
          INPUT_SYMBOLS: ${{ inputs.symbol }}
        run: |
          cd ml
          python -m src.scripts.resolve_universe --output env >> $GITHUB_ENV

      - name: Run smoke tests
        run: |
          cd ml
          python -m src.scripts.smoke_tests

      - name: Log provider coverage snapshot
        if: always()
        env:
          WORKFLOW_SYMBOL: ${{ inputs.symbol }}
        run: >-
          set -euo pipefail;
          SYMBOL_SOURCE="${WORKFLOW_SYMBOL:-${SWIFTBOLT_SYMBOLS:-}}";
          SYMBOL_SOURCE="${SYMBOL_SOURCE#,}";
          SYMBOL="${SYMBOL_SOURCE%%,*}";
          if [ -z "$SYMBOL" ]; then SYMBOL="SPY"; fi;
          TIMEFRAME_SOURCE="${SWIFTBOLT_TIMEFRAMES:-m15,h1,h4,d1,w1}";
          SUMMARY_FILE="${GITHUB_STEP_SUMMARY:-/tmp/provider-coverage.txt}";
          {
            echo "## Provider Coverage Snapshot";
            echo "";
            echo "| Symbol | Timeframe | Hist Provider | Hist Bars | Intraday Provider | Intraday Bars |";
            echo "|--------|-----------|---------------|-----------|-------------------|---------------|";
          } >> "$SUMMARY_FILE";
          IFS=',' read -ra TIMEFRAMES <<< "$TIMEFRAME_SOURCE";
          for TF in "${TIMEFRAMES[@]}"; do
            TF_TRIMMED="$(echo "$TF" | xargs)";
            [ -z "$TF_TRIMMED" ] && continue;
            case "$TF_TRIMMED" in
              w1) DAYS=730 ;;
              d1) DAYS=365 ;;
              *) DAYS=120 ;;
            esac;
            PAYLOAD=$(printf '{"symbol":"%s","timeframe":"%s","days":%s}' "$SYMBOL" "$TF_TRIMMED" "$DAYS");
            RESPONSE=$(curl -sS \
              -H "apikey: ${SUPABASE_KEY}" \
              -H "Authorization: Bearer ${SUPABASE_KEY}" \
              -H "Content-Type: application/json" \
              -X POST \
              -d "$PAYLOAD" \
              "${SUPABASE_URL}/functions/v1/chart-data-v2" 2>&1) || RESPONSE="";
            
            # Check for errors in response
            if [ -z "$RESPONSE" ]; then
              printf "| %s | %s | %s | %s | %s | %s |\n" "$SYMBOL" "$TF_TRIMMED" "no_response" "0" "no_response" "0" >> "$SUMMARY_FILE";
              echo "::warning::No response for $SYMBOL/$TF_TRIMMED";
              continue;
            fi;
            
            # Check if response is an error
            if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
              ERROR_MSG=$(echo "$RESPONSE" | jq -r '.error // "unknown error"');
              printf "| %s | %s | %s | %s | %s | %s |\n" "$SYMBOL" "$TF_TRIMMED" "error" "0" "error" "0" >> "$SUMMARY_FILE";
              echo "::warning::[$SYMBOL/$TF_TRIMMED] API error: $ERROR_MSG";
              continue;
            fi;
            
            # Parse response with better error handling
            # Note: Edge function returns 'none' when no data, not 'unknown'
            HIST_PROVIDER=$(echo "$RESPONSE" | jq -r '.layers.historical.provider // "unknown"' 2>/dev/null) || HIST_PROVIDER="parse_error";
            HIST_COUNT=$(echo "$RESPONSE" | jq -r '.layers.historical.count // 0' 2>/dev/null) || HIST_COUNT="0";
            INTRA_PROVIDER=$(echo "$RESPONSE" | jq -r '.layers.intraday.provider // "unknown"' 2>/dev/null) || INTRA_PROVIDER="parse_error";
            INTRA_COUNT=$(echo "$RESPONSE" | jq -r '.layers.intraday.count // 0' 2>/dev/null) || INTRA_COUNT="0";
            
            # Normalize 'none' to 'no_data' for clarity
            if [ "$HIST_PROVIDER" = "none" ]; then
              HIST_PROVIDER="no_data";
            fi;
            if [ "$INTRA_PROVIDER" = "none" ]; then
              INTRA_PROVIDER="no_data";
            fi;
            
            # If parsing failed, try to get total bars as fallback
            if [ "$HIST_PROVIDER" = "parse_error" ] || [ "$INTRA_PROVIDER" = "parse_error" ]; then
              TOTAL_BARS=$(echo "$RESPONSE" | jq -r '.metadata.total_bars // 0' 2>/dev/null) || TOTAL_BARS="0";
              if [ "$TOTAL_BARS" != "0" ] && [ -n "$TOTAL_BARS" ]; then
                echo "::warning::[$SYMBOL/$TF_TRIMMED] Response structure unexpected, but found $TOTAL_BARS total bars";
                HIST_COUNT="$TOTAL_BARS";
                INTRA_COUNT="0";
              else
                # Debug: Show first 200 chars of response if parsing fails
                echo "::warning::[$SYMBOL/$TF_TRIMMED] Failed to parse response. First 200 chars: ${RESPONSE:0:200}";
              fi;
            fi;
            
            printf "| %s | %s | %s | %s | %s | %s |\n" \
              "$SYMBOL" "$TF_TRIMMED" "$HIST_PROVIDER" "$HIST_COUNT" "$INTRA_PROVIDER" "$INTRA_COUNT" >> "$SUMMARY_FILE";
            echo "::notice::[$SYMBOL/$TF_TRIMMED] historical=$HIST_PROVIDER($HIST_COUNT) intraday=$INTRA_PROVIDER($INTRA_COUNT)";
          done

      - name: Final orchestration summary
        if: always()
        run: |
          echo "## ML Orchestration Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ML Forecast | ${{ needs.ml-forecast.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Options Processing | ${{ needs.options-processing.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Health | ${{ needs.model-health.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ job.status }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Trigger Information" >> $GITHUB_STEP_SUMMARY
          echo "- **Source**: ${{ needs.check-trigger.outputs.trigger_source }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Filter**: ${{ inputs.job_filter || 'All jobs' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Symbol**: ${{ inputs.symbol || 'All watchlist' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Tables Updated" >> $GITHUB_STEP_SUMMARY
          echo "- \`ml_forecasts\` - Directional predictions" >> $GITHUB_STEP_SUMMARY
          echo "- \`ml_forecasts (unified)\` - Unified validation with drift metadata" >> $GITHUB_STEP_SUMMARY
          echo "- \`forecast_evaluations\` - Historical accuracy" >> $GITHUB_STEP_SUMMARY
          echo "- \`model_weights\` - Ensemble weights" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_chain_snapshots\` - Options data" >> $GITHUB_STEP_SUMMARY
          echo "- \`options_ranks\` - ML-scored contracts" >> $GITHUB_STEP_SUMMARY
