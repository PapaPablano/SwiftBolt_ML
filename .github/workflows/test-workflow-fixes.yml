name: Test Workflow Fixes

# =============================================================================
# COMPREHENSIVE TEST WORKFLOW FOR VALIDATING FIXES
# =============================================================================
# This workflow provides a unified way to test various components:
# - Database connectivity (Supabase)
# - API connectivity (Alpaca)
# - ML tests and code quality
# - Edge function validation
# =============================================================================

on:
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        type: choice
        options:
          - all
          - db-connection
          - alpaca-api
          - ml-tests
          - supabase-functions
        default: 'all'
      symbols:
        description: 'Comma-separated symbols to test (leave empty for defaults)'
        required: false
        type: string
        default: 'AAPL,SPY,TSLA'
      verbose:
        description: 'Enable verbose output'
        required: false
        type: boolean
        default: true

permissions:
  contents: read

concurrency:
  group: test-workflow-fixes-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================================================
  # Validate environment configuration
  # ===========================================================================
  validate-environment:
    runs-on: ubuntu-latest
    outputs:
      has_supabase: ${{ steps.check.outputs.has_supabase }}
      has_alpaca: ${{ steps.check.outputs.has_alpaca }}
      has_database_url: ${{ steps.check.outputs.has_database_url }}
    steps:
      - name: Check Environment Configuration
        id: check
        run: |
          echo "## Environment Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Secret | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|" >> $GITHUB_STEP_SUMMARY
          
          has_supabase=false
          has_alpaca=false
          has_database_url=false
          
          if [ -n "$SUPABASE_URL" ] && [ -n "$SUPABASE_SERVICE_ROLE_KEY" ]; then
            echo "| SUPABASE_URL | ✅ Configured |" >> $GITHUB_STEP_SUMMARY
            echo "| SUPABASE_SERVICE_ROLE_KEY | ✅ Configured |" >> $GITHUB_STEP_SUMMARY
            has_supabase=true
          else
            echo "| SUPABASE_URL | ❌ Missing |" >> $GITHUB_STEP_SUMMARY
            echo "| SUPABASE_SERVICE_ROLE_KEY | ❌ Missing |" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -n "$ALPACA_API_KEY" ] && [ -n "$ALPACA_API_SECRET" ]; then
            echo "| ALPACA_API_KEY | ✅ Configured |" >> $GITHUB_STEP_SUMMARY
            echo "| ALPACA_API_SECRET | ✅ Configured |" >> $GITHUB_STEP_SUMMARY
            has_alpaca=true
          else
            echo "| ALPACA_API_KEY | ❌ Missing |" >> $GITHUB_STEP_SUMMARY
            echo "| ALPACA_API_SECRET | ❌ Missing |" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -n "$DATABASE_URL" ]; then
            echo "| DATABASE_URL | ✅ Configured |" >> $GITHUB_STEP_SUMMARY
            has_database_url=true
          else
            echo "| DATABASE_URL | ⚠️ Not set (optional) |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "has_supabase=$has_supabase" >> $GITHUB_OUTPUT
          echo "has_alpaca=$has_alpaca" >> $GITHUB_OUTPUT
          echo "has_database_url=$has_database_url" >> $GITHUB_OUTPUT
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          ALPACA_API_KEY: ${{ secrets.ALPACA_API_KEY }}
          ALPACA_API_SECRET: ${{ secrets.ALPACA_API_SECRET }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

  # ===========================================================================
  # Test database connectivity
  # ===========================================================================
  test-db-connection:
    needs: validate-environment
    if: |
      needs.validate-environment.outputs.has_supabase == 'true' && 
      (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'db-connection')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      DATABASE_URL: ${{ secrets.DATABASE_URL }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'ml/requirements.txt'

      - name: Install dependencies
        run: |
          cd ml
          pip install -r requirements.txt

      - name: Configure environment
        run: |
          {
            echo "SUPABASE_URL=${SUPABASE_URL}"
            echo "SUPABASE_KEY=${SUPABASE_KEY}"
            echo "DATABASE_URL=${DATABASE_URL}"
          } > .env

      - name: Test Supabase Connection
        id: supabase_test
        run: |
          cd ml
          echo "## Database Connection Test" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python - <<'PY'
          import os
          import sys
          from pathlib import Path
          from dotenv import load_dotenv
          from datetime import datetime, timezone
          
          repo_root = Path.cwd().parent
          load_dotenv(repo_root / ".env")
          
          from src.data.supabase_db import db
          
          tests_passed = 0
          tests_failed = 0
          
          print("=" * 60)
          print("SUPABASE CONNECTION TEST")
          print("=" * 60)
          
          # Test 1: Basic connectivity
          try:
              result = db.client.table("ohlc_bars").select("symbol_id").limit(1).execute()
              print("✅ Test 1: Basic connectivity - PASSED")
              tests_passed += 1
          except Exception as e:
              print(f"❌ Test 1: Basic connectivity - FAILED: {e}")
              tests_failed += 1
          
          # Test 2: Query ohlc_bars table
          try:
              result = db.client.table("ohlc_bars").select("*").limit(5).execute()
              row_count = len(result.data or [])
              print(f"✅ Test 2: Query ohlc_bars - PASSED (rows: {row_count})")
              tests_passed += 1
          except Exception as e:
              print(f"❌ Test 2: Query ohlc_bars - FAILED: {e}")
              tests_failed += 1
          
          # Test 3: Check symbols table
          try:
              result = db.client.table("symbols").select("symbol,name").limit(5).execute()
              symbols = [r.get("symbol") for r in (result.data or [])]
              print(f"✅ Test 3: Query symbols - PASSED (sample: {symbols[:3]})")
              tests_passed += 1
          except Exception as e:
              print(f"❌ Test 3: Query symbols - FAILED: {e}")
              tests_failed += 1
          
          # Test 4: Check data freshness
          test_symbols = os.getenv("INPUT_SYMBOLS", "AAPL,SPY,TSLA").split(",")
          for sym in test_symbols[:3]:
              sym = sym.strip()
              try:
                  df = db.fetch_ohlc_bars(sym, timeframe="h1", limit=10)
                  if not df.empty:
                      latest = df["ts"].max()
                      age_hours = (datetime.now(timezone.utc) - latest).total_seconds() / 3600
                      print(f"✅ Test 4: Data freshness for {sym} - {len(df)} bars, age: {age_hours:.1f}h")
                      tests_passed += 1
                  else:
                      print(f"⚠️ Test 4: No data for {sym}")
                      tests_passed += 1  # Not a failure, just no data
              except Exception as e:
                  print(f"❌ Test 4: Data freshness for {sym} - FAILED: {e}")
                  tests_failed += 1
          
          print("=" * 60)
          print(f"RESULTS: {tests_passed} passed, {tests_failed} failed")
          print("=" * 60)
          
          # Write summary
          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              with open(summary_path, "a", encoding="utf-8") as fh:
                  fh.write(f"| Test Results | {tests_passed} passed, {tests_failed} failed |\n")
                  fh.write(f"| Status | {'✅ All tests passed' if tests_failed == 0 else '❌ Some tests failed'} |\n")
          
          sys.exit(0 if tests_failed == 0 else 1)
          PY
        env:
          INPUT_SYMBOLS: ${{ github.event.inputs.symbols }}

  # ===========================================================================
  # Test Alpaca API connectivity
  # ===========================================================================
  test-alpaca-api:
    needs: validate-environment
    if: |
      needs.validate-environment.outputs.has_alpaca == 'true' && 
      (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'alpaca-api')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      ALPACA_API_KEY: ${{ secrets.ALPACA_API_KEY }}
      ALPACA_API_SECRET: ${{ secrets.ALPACA_API_SECRET }}
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'ml/requirements.txt'

      - name: Install dependencies
        run: |
          cd ml
          pip install -r requirements.txt

      - name: Configure environment
        run: |
          {
            echo "ALPACA_API_KEY=${ALPACA_API_KEY}"
            echo "ALPACA_API_SECRET=${ALPACA_API_SECRET}"
            echo "SUPABASE_URL=${SUPABASE_URL}"
            echo "SUPABASE_KEY=${SUPABASE_KEY}"
          } > .env

      - name: Test Alpaca API Connection
        id: alpaca_test
        run: |
          cd ml
          echo "## Alpaca API Connection Test" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python - <<'PY'
          import os
          import sys
          from pathlib import Path
          from dotenv import load_dotenv
          from datetime import datetime, timedelta, timezone
          
          repo_root = Path.cwd().parent
          load_dotenv(repo_root / ".env")
          
          tests_passed = 0
          tests_failed = 0
          
          print("=" * 60)
          print("ALPACA API CONNECTION TEST")
          print("=" * 60)
          
          # Test 1: Import and initialize Alpaca client
          try:
              from alpaca.data.historical import StockHistoricalDataClient
              from alpaca.data.requests import StockBarsRequest
              from alpaca.data.timeframe import TimeFrame
              
              api_key = os.getenv("ALPACA_API_KEY")
              api_secret = os.getenv("ALPACA_API_SECRET")
              
              if not api_key or not api_secret:
                  raise ValueError("Alpaca API credentials not configured")
              
              client = StockHistoricalDataClient(api_key, api_secret)
              print("✅ Test 1: Alpaca client initialization - PASSED")
              tests_passed += 1
          except Exception as e:
              print(f"❌ Test 1: Alpaca client initialization - FAILED: {e}")
              tests_failed += 1
              sys.exit(1)
          
          # Test 2: Fetch market data
          test_symbols = os.getenv("INPUT_SYMBOLS", "AAPL,SPY").split(",")[:2]
          for sym in test_symbols:
              sym = sym.strip()
              try:
                  end = datetime.now(timezone.utc)
                  start = end - timedelta(days=5)
                  
                  request = StockBarsRequest(
                      symbol_or_symbols=[sym],
                      timeframe=TimeFrame.Hour,
                      start=start,
                      end=end
                  )
                  
                  bars = client.get_stock_bars(request)
                  bar_count = len(bars[sym]) if sym in bars else 0
                  print(f"✅ Test 2: Fetch {sym} bars - PASSED ({bar_count} bars)")
                  tests_passed += 1
              except Exception as e:
                  print(f"❌ Test 2: Fetch {sym} bars - FAILED: {e}")
                  tests_failed += 1
          
          # Test 3: Market status check
          try:
              from alpaca.trading.client import TradingClient
              trading_client = TradingClient(api_key, api_secret, paper=True)
              clock = trading_client.get_clock()
              print(f"✅ Test 3: Market status - PASSED (is_open: {clock.is_open})")
              tests_passed += 1
          except Exception as e:
              print(f"❌ Test 3: Market status - FAILED: {e}")
              tests_failed += 1
          
          print("=" * 60)
          print(f"RESULTS: {tests_passed} passed, {tests_failed} failed")
          print("=" * 60)
          
          # Write summary
          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              with open(summary_path, "a", encoding="utf-8") as fh:
                  fh.write(f"| Test Results | {tests_passed} passed, {tests_failed} failed |\n")
                  fh.write(f"| Status | {'✅ All tests passed' if tests_failed == 0 else '❌ Some tests failed'} |\n")
          
          sys.exit(0 if tests_failed == 0 else 1)
          PY
        env:
          INPUT_SYMBOLS: ${{ github.event.inputs.symbols }}

  # ===========================================================================
  # Run ML tests
  # ===========================================================================
  test-ml:
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'ml-tests'
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'ml/requirements.txt'

      - name: Install dependencies
        run: |
          cd ml
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run pytest
        run: |
          cd ml
          echo "## ML Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Run tests with verbose output if requested
          if [ "${{ github.event.inputs.verbose }}" = "true" ]; then
            pytest tests/ -v --tb=short --no-header -q 2>&1 | tee test_output.txt || true
          else
            pytest tests/ --tb=short --no-header -q 2>&1 | tee test_output.txt || true
          fi
          
          # Extract summary
          passed=$(grep -oP '\d+(?= passed)' test_output.txt || echo "0")
          failed=$(grep -oP '\d+(?= failed)' test_output.txt || echo "0")
          skipped=$(grep -oP '\d+(?= skipped)' test_output.txt || echo "0")
          
          echo "| Passed | Failed | Skipped |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| $passed | $failed | $skipped |" >> $GITHUB_STEP_SUMMARY
          
          # Show last 50 lines of output
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "<details><summary>Test Output (last 50 lines)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -50 test_output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
          
          # Exit with failure if tests failed
          if [ "$failed" != "0" ] && [ "$failed" != "" ]; then
            echo "❌ $failed test(s) failed"
            exit 1
          fi
          
          echo "✅ All tests passed"

  # ===========================================================================
  # Validate Supabase Edge Functions
  # ===========================================================================
  test-supabase-functions:
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'supabase-functions'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Deno
        uses: denoland/setup-deno@v1
        with:
          deno-version: v1.x

      - name: Validate Edge Functions Syntax
        run: |
          echo "## Supabase Edge Functions Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Function | Syntax Check |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------------|" >> $GITHUB_STEP_SUMMARY
          
          for func_dir in supabase/functions/*/; do
            func_name=$(basename "$func_dir")
            
            # Skip shared folder
            if [ "$func_name" = "_shared" ]; then
              continue
            fi
            
            index_file="$func_dir/index.ts"
            if [ -f "$index_file" ]; then
              if deno check "$index_file" 2>/dev/null; then
                echo "| $func_name | ✅ Valid |" >> $GITHUB_STEP_SUMMARY
              else
                echo "| $func_name | ⚠️ Check warnings |" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "| $func_name | ❓ No index.ts |" >> $GITHUB_STEP_SUMMARY
            fi
          done

  # ===========================================================================
  # Summary report
  # ===========================================================================
  summary:
    needs: [validate-environment, test-db-connection, test-alpaca-api, test-ml, test-supabase-functions]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Generate Summary Report
        run: |
          echo "# Test Workflow Fixes - Summary Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Type:** ${{ github.event.inputs.test_type }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Environment Validation | ${{ needs.validate-environment.result == 'success' && '✅ Passed' || needs.validate-environment.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database Connection Test | ${{ needs.test-db-connection.result == 'success' && '✅ Passed' || needs.test-db-connection.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Alpaca API Test | ${{ needs.test-alpaca-api.result == 'success' && '✅ Passed' || needs.test-alpaca-api.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ML Tests | ${{ needs.test-ml.result == 'success' && '✅ Passed' || needs.test-ml.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Supabase Functions | ${{ needs.test-supabase-functions.result == 'success' && '✅ Passed' || needs.test-supabase-functions.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall result
          if [ "${{ needs.test-db-connection.result }}" = "failure" ] || \
             [ "${{ needs.test-alpaca-api.result }}" = "failure" ] || \
             [ "${{ needs.test-ml.result }}" = "failure" ] || \
             [ "${{ needs.test-supabase-functions.result }}" = "failure" ]; then
            echo "## ❌ Overall: Some tests failed" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ✅ Overall: All tests passed" >> $GITHUB_STEP_SUMMARY
          fi
