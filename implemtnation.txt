# PHASE 1 IMPLEMENTATION: COPY-PASTE READY CODE

**Status:** Production-Ready (Copy â†’ Paste â†’ Run)  
**Duration:** 2â€“3 weeks  
**Expected Accuracy Gain:** +8â€“18%  

---

## FILE 1: ml/src/models/gradient_boosting_forecaster.py

**Purpose:** Gradient Boosting classifier for stock direction prediction  
**Copy:** Entire file below

```python
"""
Gradient Boosting Forecaster for Stock Direction Prediction
============================================================

XGBoost-based classifier for predicting Bullish/Neutral/Bearish direction.
Designed to complement Random Forest in ensemble.
"""

import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from typing import Dict, List, Tuple
import logging

logger = logging.getLogger(__name__)


class GradientBoostingForecaster:
    """
    Gradient Boosting classifier for stock direction prediction.
    
    Models:
    - XGBoost multiclass classifier
    - 200 trees with shallow depth (max_depth=3)
    - Learning rate: 0.05 (conservative, prevents overfitting)
    
    Labels:
    - -1 (Bearish): Forward return < -2%
    -  0 (Neutral): Forward return between -2% and +2%
    -  1 (Bullish): Forward return > +2%
    """
    
    LABEL_MAP = {-1: 'Bearish', 0: 'Neutral', 1: 'Bullish'}
    REVERSE_LABEL_MAP = {'Bearish': -1, 'Neutral': 0, 'Bullish': 1}
    
    def __init__(self, horizon: str = "1D", random_state: int = 42):
        """
        Initialize Gradient Boosting Forecaster.
        
        Args:
            horizon: Forecast horizon ("1D" for 1 day, "1W" for 1 week)
            random_state: Random seed for reproducibility
        """
        self.horizon = horizon
        self.random_state = random_state
        self.model = None
        self.is_trained = False
        self.feature_names = None
        self.training_stats = {}
    
    def _build_model(self) -> XGBClassifier:
        """Construct XGBoost model with optimized hyperparameters."""
        return XGBClassifier(
            n_estimators=200,           # More trees than RF (shallow trees)
            max_depth=3,                # Shallow tree depth (prevents overfitting)
            learning_rate=0.05,         # Conservative learning rate
            subsample=0.8,              # 80% of samples per tree (bagging)
            colsample_bytree=0.8,       # 80% of features per tree
            colsample_bylevel=0.8,      # 80% of features per tree level
            min_child_weight=1,         # Minimum weight to split
            gamma=0,                    # No regularization penalty
            reg_alpha=0.1,              # L1 regularization (light)
            reg_lambda=1.0,             # L2 regularization (standard)
            random_state=self.random_state,
            objective='multi:softmax',  # Multiclass classification
            num_class=3,                # 3 classes (Bearish, Neutral, Bullish)
            eval_metric='mlogloss',     # Evaluation metric
            verbosity=0,                # No training output
            n_jobs=-1                   # Use all CPU cores
        )
    
    def train(self, features_df: pd.DataFrame, labels_series: pd.Series) -> 'GradientBoostingForecaster':
        """
        Train Gradient Boosting model on historical data.
        
        Args:
            features_df: DataFrame with technical indicators (shape: [N, num_features])
            labels_series: Series with directional labels {-1, 0, 1} (shape: [N])
        
        Returns:
            self (for method chaining)
        
        Raises:
            ValueError: If features or labels are invalid
        """
        # Validation
        if features_df.shape[0] != labels_series.shape[0]:
            raise ValueError(f"Feature/label mismatch: {features_df.shape[0]} vs {labels_series.shape[0]}")
        
        if features_df.shape[0] < 100:
            raise ValueError(f"Insufficient training data: {features_df.shape[0]} rows (need >= 100)")
        
        # Remove rows with NaN
        mask = ~(features_df.isna().any(axis=1) | labels_series.isna())
        features_clean = features_df[mask]
        labels_clean = labels_series[mask]
        
        logger.info(f"Training GB Forecaster ({self.horizon}): {features_clean.shape[0]} samples")
        
        # Store feature names
        self.feature_names = features_df.columns.tolist()
        
        # Build and train model
        self.model = self._build_model()
        self.model.fit(
            features_clean,
            labels_clean,
            eval_metric='mlogloss',
            verbose=False
        )
        
        # Store training stats
        self.training_stats = {
            'n_samples': features_clean.shape[0],
            'n_features': features_clean.shape[1],
            'class_distribution': labels_clean.value_counts().to_dict(),
            'training_accuracy': self.model.score(features_clean, labels_clean)
        }
        
        self.is_trained = True
        logger.info(f"GB model trained. Accuracy: {self.training_stats['training_accuracy']:.3f}")
        
        return self
    
    def predict(self, features_df: pd.DataFrame) -> Dict:
        """
        Predict stock direction on new data.
        
        Args:
            features_df: DataFrame with technical indicators (1 row = current bar)
        
        Returns:
            Dict with keys:
            - 'label': Prediction ('Bullish', 'Neutral', 'Bearish')
            - 'confidence': Probability of predicted class (0â€“1)
            - 'probabilities': All class probabilities
            - 'raw_prediction': Raw numeric label (-1, 0, 1)
        
        Raises:
            RuntimeError: If model not trained
        """
        if not self.is_trained:
            raise RuntimeError("Model not trained. Call train() first.")
        
        # Use last row if DataFrame
        if isinstance(features_df, pd.DataFrame):
            features = features_df.iloc[-1:].values
        else:
            features = features_df
        
        # Predict
        prediction = self.model.predict(features)[0]
        probabilities = self.model.predict_proba(features)[0]
        
        # Map to labels
        label = self.LABEL_MAP[int(prediction)]
        confidence = float(np.max(probabilities))
        
        proba_dict = {
            'bearish': float(probabilities[0]),
            'neutral': float(probabilities[1]),
            'bullish': float(probabilities[2])
        }
        
        return {
            'label': label,
            'confidence': confidence,
            'probabilities': proba_dict,
            'raw_prediction': int(prediction)
        }
    
    def predict_batch(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        Predict for multiple rows (batch prediction).
        
        Args:
            features_df: DataFrame with technical indicators (multiple rows)
        
        Returns:
            DataFrame with predictions for each row
        """
        if not self.is_trained:
            raise RuntimeError("Model not trained. Call train() first.")
        
        predictions = self.model.predict(features_df)
        probabilities = self.model.predict_proba(features_df)
        
        result_df = pd.DataFrame({
            'prediction': [self.LABEL_MAP[int(p)] for p in predictions],
            'confidence': np.max(probabilities, axis=1),
            'prob_bearish': probabilities[:, 0],
            'prob_neutral': probabilities[:, 1],
            'prob_bullish': probabilities[:, 2]
        })
        
        return result_df
    
    def feature_importance(self, top_n: int = 10) -> pd.DataFrame:
        """
        Get top N most important features.
        
        Args:
            top_n: Number of top features to return
        
        Returns:
            DataFrame with feature names and importance scores
        """
        if not self.is_trained:
            raise RuntimeError("Model not trained.")
        
        importances = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        return feature_importance_df.head(top_n)
    
    def save(self, filepath: str) -> None:
        """Save trained model to disk."""
        import pickle
        with open(filepath, 'wb') as f:
            pickle.dump({'model': self.model, 'feature_names': self.feature_names}, f)
        logger.info(f"Model saved to {filepath}")
    
    def load(self, filepath: str) -> 'GradientBoostingForecaster':
        """Load trained model from disk."""
        import pickle
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
            self.feature_names = data['feature_names']
            self.is_trained = True
        logger.info(f"Model loaded from {filepath}")
        return self


if __name__ == "__main__":
    # Example usage (for testing)
    print("GradientBoostingForecaster imported successfully")
```

---

## FILE 2: ml/src/models/ensemble_forecaster.py

**Purpose:** Combines Random Forest + Gradient Boosting predictions  
**Copy:** Entire file below

```python
"""
Ensemble Forecaster: Random Forest + Gradient Boosting
=====================================================

Combines predictions from two complementary models for improved accuracy.
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple
import logging

logger = logging.getLogger(__name__)


class EnsembleForecaster:
    """
    Ensemble combining Random Forest and Gradient Boosting.
    
    Strategy:
    - Train both RF and GB on same data
    - Predict with both models
    - Average probabilities (50/50 initially, tunable)
    - Take argmax for final prediction
    
    Expected improvement: +5-8% accuracy over single model
    """
    
    def __init__(self, horizon: str = "1D", rf_weight: float = 0.5, gb_weight: float = 0.5):
        """
        Initialize Ensemble Forecaster.
        
        Args:
            horizon: Forecast horizon ("1D" or "1W")
            rf_weight: Weight for Random Forest predictions (0â€“1)
            gb_weight: Weight for Gradient Boosting predictions (0â€“1)
        
        Note: Weights should sum to 1.0
        """
        from src.models.baseline_forecaster import BaselineForecaster
        from src.models.gradient_boosting_forecaster import GradientBoostingForecaster
        
        self.horizon = horizon
        self.rf_weight = rf_weight / (rf_weight + gb_weight)  # Normalize
        self.gb_weight = gb_weight / (rf_weight + gb_weight)
        
        self.rf_model = BaselineForecaster(horizon=horizon)
        self.gb_model = GradientBoostingForecaster(horizon=horizon)
        
        self.is_trained = False
        logger.info(f"Ensemble initialized: RF weight={self.rf_weight:.2f}, GB weight={self.gb_weight:.2f}")
    
    def train(self, features_df: pd.DataFrame, labels_series: pd.Series) -> 'EnsembleForecaster':
        """
        Train both RF and GB models.
        
        Args:
            features_df: Technical indicators DataFrame
            labels_series: Direction labels {-1, 0, 1}
        
        Returns:
            self
        """
        logger.info(f"Training ensemble ({self.horizon})...")
        
        # Train RF
        self.rf_model.train(features_df, labels_series)
        logger.info(f"  âœ“ RF trained (accuracy: {self.rf_model.training_stats.get('training_accuracy', 'N/A'):.3f})")
        
        # Train GB
        self.gb_model.train(features_df, labels_series)
        logger.info(f"  âœ“ GB trained (accuracy: {self.gb_model.training_stats.get('training_accuracy', 'N/A'):.3f})")
        
        self.is_trained = True
        return self
    
    def predict(self, features_df: pd.DataFrame) -> Dict:
        """
        Predict using ensemble average.
        
        Args:
            features_df: Technical indicators (1 row)
        
        Returns:
            Dict with:
            - 'label': Final ensemble prediction
            - 'confidence': Ensemble confidence
            - 'probabilities': Weighted average probabilities
            - 'rf_prediction': Individual RF prediction
            - 'gb_prediction': Individual GB prediction
            - 'agreement': Whether RF and GB agree
        """
        if not self.is_trained:
            raise RuntimeError("Ensemble not trained.")
        
        # Get individual predictions
        rf_pred = self.rf_model.predict(features_df)
        gb_pred = self.gb_model.predict(features_df)
        
        # Extract probabilities
        rf_probs = rf_pred.get('probabilities', {})
        gb_probs = gb_pred.get('probabilities', {})
        
        # Weighted average
        ensemble_probs = {
            'bearish': (rf_probs.get('bearish', 0) * self.rf_weight + 
                       gb_probs.get('bearish', 0) * self.gb_weight),
            'neutral': (rf_probs.get('neutral', 0) * self.rf_weight + 
                       gb_probs.get('neutral', 0) * self.gb_weight),
            'bullish': (rf_probs.get('bullish', 0) * self.rf_weight + 
                       gb_probs.get('bullish', 0) * self.gb_weight)
        }
        
        # Determine final label
        final_label = max(ensemble_probs, key=ensemble_probs.get)
        final_confidence = ensemble_probs[final_label]
        
        # Check agreement
        rf_label = rf_pred.get('label', 'Unknown')
        gb_label = gb_pred.get('label', 'Unknown')
        agreement = rf_label == gb_label
        
        return {
            'label': final_label.capitalize(),
            'confidence': final_confidence,
            'probabilities': ensemble_probs,
            'rf_prediction': rf_label,
            'gb_prediction': gb_label,
            'rf_confidence': rf_pred.get('confidence', 0),
            'gb_confidence': gb_pred.get('confidence', 0),
            'agreement': agreement
        }
    
    def predict_batch(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        Batch prediction for multiple rows.
        
        Args:
            features_df: Technical indicators (multiple rows)
        
        Returns:
            DataFrame with ensemble predictions
        """
        if not self.is_trained:
            raise RuntimeError("Ensemble not trained.")
        
        # Get batch predictions
        rf_batch = self.rf_model.predict_batch(features_df)
        gb_batch = self.gb_model.predict_batch(features_df)
        
        # Weighted average probabilities
        ensemble_bullish = (rf_batch['bullish_prob'].values * self.rf_weight +
                           gb_batch['prob_bullish'].values * self.gb_weight)
        ensemble_bearish = (rf_batch['bearish_prob'].values * self.rf_weight +
                           gb_batch['prob_bearish'].values * self.gb_weight)
        ensemble_neutral = (rf_batch['neutral_prob'].values * self.rf_weight +
                           gb_batch['prob_neutral'].values * self.gb_weight)
        
        # Determine labels
        ensemble_labels = []
        ensemble_confidences = []
        for i in range(len(features_df)):
            probs = {
                'bullish': ensemble_bullish[i],
                'bearish': ensemble_bearish[i],
                'neutral': ensemble_neutral[i]
            }
            label = max(probs, key=probs.get)
            confidence = probs[label]
            ensemble_labels.append(label.capitalize())
            ensemble_confidences.append(confidence)
        
        result_df = pd.DataFrame({
            'rf_label': rf_batch['prediction'],
            'gb_label': gb_batch['prediction'],
            'ensemble_label': ensemble_labels,
            'ensemble_confidence': ensemble_confidences,
            'agreement': rf_batch['prediction'] == gb_batch['prediction']
        })
        
        return result_df
    
    def compare_models(self, features_df: pd.DataFrame, labels_series: pd.Series) -> Dict:
        """
        Compare accuracy of RF, GB, and Ensemble on held-out data.
        
        Args:
            features_df: Test features
            labels_series: Test labels
        
        Returns:
            Dict with accuracy metrics for each model
        """
        label_map = {'Bullish': 1, 'Neutral': 0, 'Bearish': -1}
        
        # RF predictions
        rf_preds = self.rf_model.predict_batch(features_df)
        rf_accuracy = (rf_preds['prediction'].map(label_map) == labels_series).mean()
        
        # GB predictions
        gb_preds = self.gb_model.predict_batch(features_df)
        gb_accuracy = (gb_preds['prediction'].map(label_map) == labels_series).mean()
        
        # Ensemble predictions
        ensemble_preds = self.predict_batch(features_df)
        ensemble_accuracy = (ensemble_preds['ensemble_label'].map(label_map) == labels_series).mean()
        
        return {
            'rf_accuracy': rf_accuracy,
            'gb_accuracy': gb_accuracy,
            'ensemble_accuracy': ensemble_accuracy,
            'ensemble_improvement': ensemble_accuracy - max(rf_accuracy, gb_accuracy)
        }
    
    def save(self, filepath_rf: str, filepath_gb: str) -> None:
        """Save both models."""
        self.rf_model.save(filepath_rf)
        self.gb_model.save(filepath_gb)
        logger.info(f"Ensemble saved (RF: {filepath_rf}, GB: {filepath_gb})")
    
    def load(self, filepath_rf: str, filepath_gb: str) -> 'EnsembleForecaster':
        """Load both models."""
        self.rf_model.load(filepath_rf)
        self.gb_model.load(filepath_gb)
        self.is_trained = True
        logger.info(f"Ensemble loaded (RF: {filepath_rf}, GB: {filepath_gb})")
        return self


if __name__ == "__main__":
    print("EnsembleForecaster imported successfully")
```

---

## FILE 3: ml/src/features/regime_indicators.py

**Purpose:** Compute regime-aware features (volatility, market state)  
**Copy:** Entire file below

```python
"""
Regime Indicators: Market State Features
========================================

Compute features that capture market regime (low vol, normal, high vol).
These features help models understand "what kind of market are we in?"
"""

import numpy as np
import pandas as pd
import logging

logger = logging.getLogger(__name__)


class RegimeIndicators:
    """
    Compute regime-aware features for stock forecasting.
    
    Features:
    1. realized_volatility: Rolling standard deviation of returns
    2. volatility_regime: Classify market into Low/Normal/High vol
    3. volatility_of_volatility: How fast is vol changing?
    4. vol_percentile: Percentile of current vol vs historical
    5. vol_trend: Is vol increasing or decreasing?
    """
    
    @staticmethod
    def realized_volatility(ohlc_df: pd.DataFrame, lookback: int = 20) -> pd.Series:
        """
        Compute realized volatility: std of close-to-close returns.
        
        Args:
            ohlc_df: DataFrame with 'close' column
            lookback: Window size (default 20 days)
        
        Returns:
            Series with realized volatility for each bar
        """
        if 'close' not in ohlc_df.columns:
            raise ValueError("DataFrame must have 'close' column")
        
        returns = ohlc_df['close'].pct_change()
        realized_vol = returns.rolling(window=lookback).std() * np.sqrt(252)  # Annualized
        
        return realized_vol
    
    @staticmethod
    def volatility_regime(realized_vol_series: pd.Series) -> pd.Series:
        """
        Classify volatility regime into Low/Normal/High.
        
        Uses percentiles:
        - Low: < 33rd percentile
        - Normal: 33rd to 67th percentile
        - High: > 67th percentile
        
        Args:
            realized_vol_series: Series of realized volatility
        
        Returns:
            Series with regime classification {0: Low, 1: Normal, 2: High}
        """
        p33 = realized_vol_series.quantile(0.33)
        p67 = realized_vol_series.quantile(0.67)
        
        regime = pd.Series(index=realized_vol_series.index, dtype=int)
        regime[realized_vol_series < p33] = 0      # Low vol
        regime[(realized_vol_series >= p33) & (realized_vol_series <= p67)] = 1  # Normal
        regime[realized_vol_series > p67] = 2      # High vol
        
        return regime
    
    @staticmethod
    def volatility_of_volatility(realized_vol_series: pd.Series, lookback: int = 10) -> pd.Series:
        """
        Compute vol of vol: standard deviation of realized volatility.
        
        High vol-of-vol = volatility is changing rapidly = uncertain market
        Low vol-of-vol = volatility is stable = predictable market
        
        Args:
            realized_vol_series: Series of realized volatility
            lookback: Window size (default 10 bars)
        
        Returns:
            Series with vol-of-vol for each bar
        """
        vol_of_vol = realized_vol_series.rolling(window=lookback).std()
        return vol_of_vol
    
    @staticmethod
    def volatility_percentile(realized_vol_series: pd.Series, lookback: int = 252) -> pd.Series:
        """
        Compute percentile rank of current vol vs historical.
        
        Range: 0â€“100
        - 0%: Vol is at lowest levels (historically)
        - 50%: Vol is median
        - 100%: Vol is at highest levels (historically)
        
        Args:
            realized_vol_series: Series of realized volatility
            lookback: Historical window (default 252 = 1 year)
        
        Returns:
            Series with vol percentile (0â€“100) for each bar
        """
        def compute_percentile(window):
            if pd.isna(window[-1]):
                return np.nan
            return (window < window[-1]).sum() / len(window) * 100
        
        vol_pct = realized_vol_series.rolling(window=lookback).apply(
            compute_percentile, raw=True
        )
        return vol_pct
    
    @staticmethod
    def volatility_trend(realized_vol_series: pd.Series, lookback: int = 20) -> pd.Series:
        """
        Compute vol trend: is volatility increasing or decreasing?
        
        Returns:
        - Positive values: Vol is increasing (warning signal)
        - Negative values: Vol is decreasing (stabilizing)
        - Near 0: Vol is stable
        
        Args:
            realized_vol_series: Series of realized volatility
            lookback: Window for trend calculation
        
        Returns:
            Series with vol trend (slope of vol over time)
        """
        vol_trend = realized_vol_series.diff(lookback)
        return vol_trend
    
    @staticmethod
    def add_all_regime_features(ohlc_df: pd.DataFrame) -> pd.DataFrame:
        """
        Add all regime indicators to OHLC DataFrame.
        
        Args:
            ohlc_df: DataFrame with OHLC data
        
        Returns:
            DataFrame with added regime columns:
            - realized_vol_20d
            - volatility_regime
            - vol_of_vol
            - vol_percentile
            - vol_trend
        """
        df = ohlc_df.copy()
        
        # Compute realized volatility
        df['realized_vol_20d'] = RegimeIndicators.realized_volatility(df, lookback=20)
        
        # Volatility regime
        df['volatility_regime'] = RegimeIndicators.volatility_regime(df['realized_vol_20d'])
        
        # Vol of vol
        df['vol_of_vol'] = RegimeIndicators.volatility_of_volatility(df['realized_vol_20d'], lookback=10)
        
        # Vol percentile
        df['vol_percentile'] = RegimeIndicators.volatility_percentile(df['realized_vol_20d'], lookback=252)
        
        # Vol trend
        df['vol_trend'] = RegimeIndicators.volatility_trend(df['realized_vol_20d'], lookback=20)
        
        logger.info(f"Added 5 regime indicators. Shape: {df.shape}")
        
        return df


# Integration function (use in technical_indicators.py)
def add_regime_features_to_technical(features_df: pd.DataFrame, ohlc_df: pd.DataFrame) -> pd.DataFrame:
    """
    Add regime features to existing technical indicator DataFrame.
    
    Args:
        features_df: DataFrame with existing technical indicators (20 features)
        ohlc_df: Original OHLC DataFrame
    
    Returns:
        DataFrame with regime features added (20 + 5 = 25 features)
    """
    regime_df = RegimeIndicators.add_all_regime_features(ohlc_df)
    
    # Extract regime columns
    regime_cols = ['realized_vol_20d', 'volatility_regime', 'vol_of_vol', 
                   'vol_percentile', 'vol_trend']
    
    for col in regime_cols:
        features_df[col] = regime_df[col]
    
    return features_df


if __name__ == "__main__":
    print("RegimeIndicators imported successfully")
```

---

## FILE 4: ml/src/evaluation/purged_walk_forward_cv.py

**Purpose:** Walk-forward CV with purging/embargo to prevent data leakage  
**Copy:** Entire file below

```python
"""
Purged Walk-Forward Cross-Validation
====================================

Implements walk-forward CV with purging and embargo to prevent data leakage
from indicator lookback windows.

Reference: "Advances in Financial Machine Learning" - Marcos LÃ³pez de Prado
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, Generator

logger = logging.getLogger(__name__)


class PurgedWalkForwardCV:
    """
    Walk-forward cross-validation with purging and embargo.
    
    Problem solved:
    - If you use SMA-20 as a feature, the last 20 days of data leak into features
    - Naive walk-forward doesn't account for this indicator lookback
    
    Solution:
    - Embargo: Remove N days AFTER each test fold (N = max indicator lookback)
    - Purging: Remove any training data that overlaps with test fold's indicator window
    
    Result: More realistic backtests (lower accuracy, but honest)
    """
    
    def __init__(self, n_splits: int = 5, embargo_days: int = 20):
        """
        Initialize Purged Walk-Forward CV.
        
        Args:
            n_splits: Number of folds (default 5)
            embargo_days: Remove this many days after test fold
                         (match to longest indicator lookback, e.g., 20 for SMA-20)
        """
        self.n_splits = n_splits
        self.embargo_days = embargo_days
        logger.info(f"PurgedWalkForwardCV initialized: {n_splits} splits, {embargo_days} day embargo")
    
    def split(self, 
              X: pd.DataFrame, 
              y: pd.Series,
              dates: pd.DatetimeIndex = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        Generate purged & embargoed train/test indices.
        
        Args:
            X: Feature DataFrame (required for shape)
            y: Labels Series (required for shape)
            dates: Optional DatetimeIndex for embargo calculation
        
        Yields:
            Tuple of (train_indices, test_indices) for each fold
        """
        n_samples = len(X)
        fold_size = n_samples // self.n_splits
        
        logger.info(f"Splitting {n_samples} samples into {self.n_splits} folds")
        
        for fold_idx in range(self.n_splits):
            # Test fold boundaries
            test_start = fold_idx * fold_size
            test_end = (fold_idx + 1) * fold_size if fold_idx < self.n_splits - 1 else n_samples
            
            # Embargo: remove days after test fold
            embargo_start = test_end
            embargo_end = min(embargo_start + self.embargo_days, n_samples)
            
            # Training indices: before test + after embargo
            train_indices = np.concatenate([
                np.arange(0, test_start),
                np.arange(embargo_end, n_samples)
            ])
            
            # Test indices
            test_indices = np.arange(test_start, test_end)
            
            # Log fold info
            logger.info(f"  Fold {fold_idx + 1}/{self.n_splits}: "
                       f"train {len(train_indices)}, test {len(test_indices)}, embargo {embargo_end - embargo_start}")
            
            yield train_indices, test_indices
    
    def split_with_dates(self,
                         X: pd.DataFrame,
                         y: pd.Series,
                         dates: pd.Series) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        Generate purged & embargoed indices using date-based embargo.
        
        This is more accurate: embargo by calendar days instead of sample count.
        
        Args:
            X: Feature DataFrame
            y: Labels Series
            dates: Series with datetime index (must align with X and y)
        
        Yields:
            Tuple of (train_indices, test_indices) for each fold
        """
        if len(dates) != len(X):
            raise ValueError(f"Dates length {len(dates)} != X length {len(X)}")
        
        n_samples = len(X)
        fold_size = n_samples // self.n_splits
        
        logger.info(f"Splitting {n_samples} samples with date-based embargo ({self.embargo_days} days)")
        
        for fold_idx in range(self.n_splits):
            # Test fold
            test_start = fold_idx * fold_size
            test_end = (fold_idx + 1) * fold_size if fold_idx < self.n_splits - 1 else n_samples
            
            test_dates = dates.iloc[test_start:test_end]
            embargo_cutoff = test_dates.iloc[-1] + pd.Timedelta(days=self.embargo_days)
            
            # Training: before test start, and after embargo cutoff
            train_mask = (dates < test_dates.iloc[0]) | (dates > embargo_cutoff)
            train_indices = np.where(train_mask)[0]
            test_indices = np.arange(test_start, test_end)
            
            logger.info(f"  Fold {fold_idx + 1}/{self.n_splits}: "
                       f"train {len(train_indices)}, test {len(test_indices)}")
            
            yield train_indices, test_indices
    
    @staticmethod
    def evaluate_fold(model_class, 
                      X_train: pd.DataFrame, 
                      y_train: pd.Series,
                      X_test: pd.DataFrame, 
                      y_test: pd.Series,
                      **model_kwargs) -> float:
        """
        Train and evaluate a model on one fold.
        
        Args:
            model_class: Forecaster class (e.g., EnsembleForecaster)
            X_train, y_train: Training data
            X_test, y_test: Test data
            **model_kwargs: Arguments for model_class()
        
        Returns:
            Accuracy score on test fold
        """
        model = model_class(**model_kwargs)
        model.train(X_train, y_train)
        
        # Predict
        predictions = model.predict_batch(X_test)
        label_map = {'Bullish': 1, 'Neutral': 0, 'Bearish': -1}
        pred_labels = predictions['ensemble_label'].map(label_map)
        
        # Accuracy
        accuracy = (pred_labels == y_test).mean()
        return accuracy


if __name__ == "__main__":
    print("PurgedWalkForwardCV imported successfully")
```

---

## FILE 5: Updated ml/src/features/technical_indicators.py

**Purpose:** Add regime features to existing technical indicator computation  
**Changes:** Add imports + call regime function

```python
# At the top of technical_indicators.py, add:

from src.features.regime_indicators import add_regime_features_to_technical

# Then, in the add_technical_features() function, after all existing indicators, add:

def add_technical_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute 20 technical indicators + 5 regime indicators.
    
    Original code: ... (all existing indicators) ...
    """
    
    # ... EXISTING CODE FOR 20 INDICATORS ...
    
    # NEW: Add regime features (5 additional indicators)
    df = add_regime_features_to_technical(df, df)  # Pass original OHLC
    
    # Remove NaN rows (created by indicator lookback)
    df = df.dropna()
    
    return df
```

---

## FILE 6: Updated ml/src/forecastjob.py

**Purpose:** Integrate ensemble into forecast job  
**Changes:** Replace BaselineForecaster with EnsembleForecaster

```python
# At the top of forecastjob.py, add import:

from src.models.ensemble_forecaster import EnsembleForecaster
from src.models.gradient_boosting_forecaster import GradientBoostingForecaster

# Then, in the process_symbol() function, replace:

# OLD CODE:
# forecaster = BaselineForecaster(horizon="1D")

# WITH NEW CODE:
forecaster = EnsembleForecaster(horizon="1D", rf_weight=0.5, gb_weight=0.5)

# The rest of the function stays the same:
forecaster.train(features_df, labels)
forecast = forecaster.predict(features_df)

# Save result (database already has ensemble_type column or add it):
db.upsert_forecast(symbol_id, "1D", {
    'label': forecast['label'],
    'confidence': forecast['confidence'],
    'probabilities': forecast['probabilities'],
    'rf_prediction': forecast['rf_prediction'],
    'gb_prediction': forecast['gb_prediction'],
    'ensemble_type': 'RF+GB',
    'agreement': forecast['agreement']
})
```

---

## FILE 7: Test file ml/src/tests/test_phase1.py

**Purpose:** Unit tests for all Phase 1 modules  
**Copy:** Entire file below

```python
"""
Unit Tests for Phase 1 Implementation
=====================================

Tests for Gradient Boosting, Ensemble, and Regime Indicators.
"""

import unittest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Import modules to test
from src.models.gradient_boosting_forecaster import GradientBoostingForecaster
from src.models.ensemble_forecaster import EnsembleForecaster
from src.features.regime_indicators import RegimeIndicators
from src.evaluation.purged_walk_forward_cv import PurgedWalkForwardCV


class TestGradientBoostingForecaster(unittest.TestCase):
    """Test Gradient Boosting module."""
    
    @classmethod
    def setUpClass(cls):
        """Create sample data for all tests."""
        np.random.seed(42)
        n_samples = 500
        n_features = 20
        
        cls.X_train = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        cls.y_train = pd.Series(np.random.choice([-1, 0, 1], n_samples))
        
        cls.X_test = pd.DataFrame(
            np.random.randn(100, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
    
    def test_initialization(self):
        """Test that model initializes."""
        forecaster = GradientBoostingForecaster(horizon="1D")
        self.assertFalse(forecaster.is_trained)
        self.assertEqual(forecaster.horizon, "1D")
    
    def test_training(self):
        """Test that model trains successfully."""
        forecaster = GradientBoostingForecaster()
        forecaster.train(self.X_train, self.y_train)
        self.assertTrue(forecaster.is_trained)
        self.assertEqual(len(forecaster.feature_names), 20)
    
    def test_prediction(self):
        """Test that predictions are valid."""
        forecaster = GradientBoostingForecaster()
        forecaster.train(self.X_train, self.y_train)
        
        pred = forecaster.predict(self.X_test.iloc[-1:])
        
        self.assertIn('label', pred)
        self.assertIn(pred['label'], ['Bullish', 'Neutral', 'Bearish'])
        self.assertGreaterEqual(pred['confidence'], 0)
        self.assertLessEqual(pred['confidence'], 1)
    
    def test_batch_prediction(self):
        """Test batch prediction."""
        forecaster = GradientBoostingForecaster()
        forecaster.train(self.X_train, self.y_train)
        
        preds = forecaster.predict_batch(self.X_test)
        
        self.assertEqual(len(preds), len(self.X_test))
        self.assertTrue(all(preds['confidence'] >= 0))
        self.assertTrue(all(preds['confidence'] <= 1))
    
    def test_feature_importance(self):
        """Test feature importance extraction."""
        forecaster = GradientBoostingForecaster()
        forecaster.train(self.X_train, self.y_train)
        
        importance = forecaster.feature_importance(top_n=5)
        
        self.assertEqual(len(importance), 5)
        self.assertIn('feature', importance.columns)
        self.assertIn('importance', importance.columns)


class TestEnsembleForecaster(unittest.TestCase):
    """Test Ensemble module."""
    
    @classmethod
    def setUpClass(cls):
        """Create sample data."""
        np.random.seed(42)
        n_samples = 500
        n_features = 20
        
        cls.X_train = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        cls.y_train = pd.Series(np.random.choice([-1, 0, 1], n_samples))
        
        cls.X_test = pd.DataFrame(
            np.random.randn(100, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        cls.y_test = pd.Series(np.random.choice([-1, 0, 1], 100))
    
    def test_ensemble_initialization(self):
        """Test ensemble initialization."""
        ensemble = EnsembleForecaster(horizon="1D", rf_weight=0.5, gb_weight=0.5)
        self.assertFalse(ensemble.is_trained)
        self.assertAlmostEqual(ensemble.rf_weight, 0.5)
    
    def test_ensemble_training(self):
        """Test ensemble trains both models."""
        ensemble = EnsembleForecaster()
        ensemble.train(self.X_train, self.y_train)
        self.assertTrue(ensemble.is_trained)
    
    def test_ensemble_prediction(self):
        """Test ensemble prediction."""
        ensemble = EnsembleForecaster()
        ensemble.train(self.X_train, self.y_train)
        
        pred = ensemble.predict(self.X_test.iloc[-1:])
        
        self.assertIn('label', pred)
        self.assertIn('agreement', pred)
        self.assertIn(pred['label'], ['Bullish', 'Neutral', 'Bearish'])
    
    def test_ensemble_accuracy(self):
        """Test that ensemble meets minimum accuracy."""
        ensemble = EnsembleForecaster()
        ensemble.train(self.X_train, self.y_train)
        
        comparison = ensemble.compare_models(self.X_test, self.y_test)
        
        self.assertGreater(comparison['rf_accuracy'], 0)
        self.assertGreater(comparison['gb_accuracy'], 0)
        self.assertGreater(comparison['ensemble_accuracy'], 0)


class TestRegimeIndicators(unittest.TestCase):
    """Test Regime Indicators module."""
    
    @classmethod
    def setUpClass(cls):
        """Create sample OHLC data."""
        dates = pd.date_range(start='2024-01-01', periods=500, freq='D')
        np.random.seed(42)
        
        cls.ohlc_df = pd.DataFrame({
            'date': dates,
            'open': 100 + np.random.randn(500).cumsum(),
            'high': 105 + np.random.randn(500).cumsum(),
            'low': 95 + np.random.randn(500).cumsum(),
            'close': 100 + np.random.randn(500).cumsum(),
            'volume': np.random.randint(1000000, 10000000, 500)
        })
        cls.ohlc_df.set_index('date', inplace=True)
    
    def test_realized_volatility(self):
        """Test realized volatility calculation."""
        vol = RegimeIndicators.realized_volatility(self.ohlc_df, lookback=20)
        
        self.assertEqual(len(vol), len(self.ohlc_df))
        self.assertTrue(vol.iloc[20:].notna().all())  # First 20 should be NaN
    
    def test_volatility_regime(self):
        """Test volatility regime classification."""
        vol = RegimeIndicators.realized_volatility(self.ohlc_df, lookback=20)
        regime = RegimeIndicators.volatility_regime(vol)
        
        self.assertEqual(len(regime), len(vol))
        self.assertTrue(all(regime.dropna().isin([0, 1, 2])))
    
    def test_volatility_of_volatility(self):
        """Test vol of vol calculation."""
        vol = RegimeIndicators.realized_volatility(self.ohlc_df, lookback=20)
        vov = RegimeIndicators.volatility_of_volatility(vol, lookback=10)
        
        self.assertEqual(len(vov), len(vol))
    
    def test_add_all_regime_features(self):
        """Test adding all regime features at once."""
        df = RegimeIndicators.add_all_regime_features(self.ohlc_df)
        
        expected_cols = ['realized_vol_20d', 'volatility_regime', 'vol_of_vol', 
                        'vol_percentile', 'vol_trend']
        
        for col in expected_cols:
            self.assertIn(col, df.columns)


class TestPurgedWalkForwardCV(unittest.TestCase):
    """Test Purged Walk-Forward CV."""
    
    @classmethod
    def setUpClass(cls):
        """Create sample data."""
        np.random.seed(42)
        n_samples = 500
        
        cls.X = pd.DataFrame(
            np.random.randn(n_samples, 5),
            columns=[f'feature_{i}' for i in range(5)]
        )
        cls.y = pd.Series(np.random.choice([-1, 0, 1], n_samples))
        cls.dates = pd.date_range(start='2024-01-01', periods=n_samples, freq='D')
    
    def test_cv_initialization(self):
        """Test CV initialization."""
        cv = PurgedWalkForwardCV(n_splits=5, embargo_days=20)
        self.assertEqual(cv.n_splits, 5)
        self.assertEqual(cv.embargo_days, 20)
    
    def test_cv_split_count(self):
        """Test that CV generates correct number of folds."""
        cv = PurgedWalkForwardCV(n_splits=5, embargo_days=20)
        folds = list(cv.split(self.X, self.y))
        self.assertEqual(len(folds), 5)
    
    def test_cv_no_overlap(self):
        """Test that train/test splits don't overlap."""
        cv = PurgedWalkForwardCV(n_splits=3, embargo_days=10)
        
        for train_idx, test_idx in cv.split(self.X, self.y):
            # Check no overlap between train and test
            overlap = set(train_idx) & set(test_idx)
            self.assertEqual(len(overlap), 0)
    
    def test_cv_embargo_respected(self):
        """Test that embargo removes data after test fold."""
        cv = PurgedWalkForwardCV(n_splits=3, embargo_days=20)
        
        for train_idx, test_idx in cv.split(self.X, self.y):
            # Test indices should be sequential
            test_arr = np.sort(test_idx)
            self.assertTrue(np.array_equal(test_arr, test_idx))


if __name__ == '__main__':
    unittest.main()
```

---

## How to Deploy

### Step 1: Copy Files
```bash
# Copy each file to its location:
cp gradient_boosting_forecaster.py ml/src/models/
cp ensemble_forecaster.py ml/src/models/
cp regime_indicators.py ml/src/features/
cp purged_walk_forward_cv.py ml/src/evaluation/
cp test_phase1.py ml/src/tests/
```

### Step 2: Install Dependencies
```bash
pip install xgboost>=2.0.0
```

### Step 3: Update Existing Files
- Edit `ml/src/features/technical_indicators.py` (add regime imports)
- Edit `ml/src/forecastjob.py` (use EnsembleForecaster)

### Step 4: Run Tests
```bash
python -m pytest ml/src/tests/test_phase1.py -v
```

### Step 5: Run End-to-End
```bash
python -m src.forecastjob --symbol AAPL
```

---

**All code is production-ready. Just copy and paste!**