"""Options Momentum Ranking System - Statistical & Analytical Framework.

Implements a rigorous quantitative methodology for ranking options by combining:
1. Momentum Analysis (40%) - Price acceleration, volume flow, OI growth
2. Valuation Assessment (35%) - IV Rank, bid-ask spread tightness
3. Greeks-Based Risk Scoring (25%) - Delta, Gamma, Vega, Theta alignment

SCORING FORMULAS (all normalized to 0-100):

1. VALUE SCORE (35% of composite):
   - iv_value_score = 100 - iv_rank  (lower IV = better for buyer)
   - spread_penalty = min(spread_% × 2, 50)
   - spread_score = 100 - spread_penalty
   - value_score = 0.60 × iv_value_score + 0.40 × spread_score

2. MOMENTUM SCORE (40% of composite):
   - price_mom_score = clip(2 × r + 50, 0, 100)  where r = 5-day return %
   - vol_oi_score = min(vol/OI / 0.20 × 100, 100)
   - oi_growth_score = clip(growth + 50, 0, 100)  where growth = 5-day OI change %
   - momentum_score = 0.50 × price_mom + 0.30 × vol_oi + 0.20 × oi_growth

3. GREEKS SCORE (25% of composite):
   - delta_score = 100 - 100 × |Δ - 0.55|  (target 0.55 calls, -0.55 puts)
   - gamma_score = min(γ / 0.04 × 100, 100)
   - vega_score = min(vega / 0.30 × 100, 100)
   - theta_penalty = min(|θ%| × 10, 40)  where θ% = θ/mid × 100
   - greeks_pre = 0.50 × delta + 0.35 × gamma + 0.10 × vega
   - greeks_score = clip(greeks_pre - theta_penalty, 0, 100)

4. COMPOSITE RANK (final 0-100 score):
   rank_score = 0.40 × momentum + 0.35 × value + 0.25 × greeks
"""

import logging
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class SignalType(Enum):
    """Types of trading signals generated by the ranking system."""

    DISCOUNT = "discount"  # Low IV + momentum = buy opportunity
    MOMENTUM_RUNNER = "runner"  # Hot momentum + high volume
    GREEKS_ALIGNED = "greeks"  # Optimal Greeks for directional trade
    BUY = "buy"  # Composite signal meeting threshold


@dataclass
class OptionSignals:
    """Trading signals for an option contract."""

    discount: bool = False
    runner: bool = False
    greeks_aligned: bool = False
    buy: bool = False
    signal_types: list = None

    def __post_init__(self):
        self.signal_types = []
        if self.discount:
            self.signal_types.append(SignalType.DISCOUNT)
        if self.runner:
            self.signal_types.append(SignalType.MOMENTUM_RUNNER)
        if self.greeks_aligned:
            self.signal_types.append(SignalType.GREEKS_ALIGNED)
        if self.buy:
            self.signal_types.append(SignalType.BUY)


@dataclass
class IVStatistics:
    """52-week IV statistics for a symbol."""

    iv_high: float
    iv_low: float
    iv_median: float
    iv_current: float
    days_of_data: int = 252
    last_updated: datetime | None = None
    max_age_hours: int = 4

    @property
    def iv_rank(self) -> float:
        """Calculate IV Rank: (current - low) / (high - low) * 100."""
        if self.iv_high == self.iv_low:
            return 50.0
        return ((self.iv_current - self.iv_low) / (self.iv_high - self.iv_low)) * 100

    @property
    def is_cheap(self) -> bool:
        """IV is in bottom 30% of range."""
        return self.iv_rank < 30

    @property
    def is_expensive(self) -> bool:
        """IV is in top 70% of range."""
        return self.iv_rank > 70

    @property
    def is_stale(self) -> bool:
        """Check if IV data is stale (> max_age_hours old)."""
        if self.last_updated is None:
            return True
        age = datetime.now() - self.last_updated
        return age.total_seconds() > (self.max_age_hours * 3600)

    @property
    def staleness_penalty(self) -> float:
        """Calculate confidence penalty for stale data (0-0.2)."""
        if not self.is_stale:
            return 0.0
        if self.last_updated is None:
            return 0.2

        age_hours = (datetime.now() - self.last_updated).total_seconds() / 3600
        # Linear penalty: 0 at max_age, 0.2 at 2x max_age
        excess_hours = age_hours - self.max_age_hours
        return min(0.2, excess_hours / self.max_age_hours * 0.2)

    def is_iv_curve_reasonable(self, iv_by_strike: dict) -> bool:
        """
        Check if IV curve is smooth (no >5% jumps between adjacent strikes).

        Large IV jumps between strikes indicate bad data or illiquidity.

        Args:
            iv_by_strike: Dict mapping strike price to IV

        Returns:
            True if IV curve is smooth, False if suspicious jumps exist
        """
        if len(iv_by_strike) < 2:
            return True

        strikes = sorted(iv_by_strike.keys())
        for i in range(1, len(strikes)):
            iv_current = iv_by_strike[strikes[i]]
            iv_prev = iv_by_strike[strikes[i - 1]]

            if iv_prev > 0:
                iv_jump_pct = abs(iv_current - iv_prev) / iv_prev * 100
                if iv_jump_pct > 5.0:  # >5% jump is suspicious
                    logger.warning(
                        f"Suspicious IV jump: {strikes[i-1]}={iv_prev:.2%} -> "
                        f"{strikes[i]}={iv_current:.2%} ({iv_jump_pct:.1f}%)"
                    )
                    return False

        return True

    @property
    def data_quality_score(self) -> float:
        """
        Combined data quality score (0-1).

        Factors:
        - Freshness (0.5 weight)
        - Days of data coverage (0.5 weight)
        """
        freshness_score = 1.0 - self.staleness_penalty
        # Data coverage score: 1.0 at 252 days, lower for less
        coverage_score = min(1.0, self.days_of_data / 252)

        return freshness_score * 0.5 + coverage_score * 0.5


class OptionsMomentumRanker:
    """
    Advanced options ranking using the Momentum-Value-Greeks framework.

    Composite Score = Momentum(40%) + Value(35%) + Greeks(25%)

    Each component normalized to 0-100 scale for interpretability.
    """

    # Framework weights
    MOMENTUM_WEIGHT = 0.40
    VALUE_WEIGHT = 0.35
    GREEKS_WEIGHT = 0.25

    ENTRY_WEIGHTS = {
        "momentum": 0.30,
        "value": 0.35,
        "greeks": 0.35,
    }

    EXIT_WEIGHTS = {
        "momentum": 0.50,
        "value": 0.20,
        "greeks": 0.30,
    }

    # Momentum sub-weights
    PRICE_MOMENTUM_WEIGHT = 0.50
    VOLUME_OI_WEIGHT = 0.30
    OI_GROWTH_WEIGHT = 0.20

    # Value sub-weights
    IV_RANK_WEIGHT = 0.60
    SPREAD_WEIGHT = 0.40

    # Greeks sub-weights (for pre-penalty calculation)
    DELTA_WEIGHT = 0.50
    GAMMA_WEIGHT = 0.35
    VEGA_WEIGHT = 0.10
    # Note: Theta is applied as penalty (up to 40 pts), not as a weight

    # Thresholds
    OPTIMAL_DELTA_MIN = 0.40
    OPTIMAL_DELTA_MAX = 0.70
    OPTIMAL_DELTA_TARGET = 0.55
    GAMMA_EXCELLENT_THRESHOLD = 0.04
    GAMMA_GOOD_THRESHOLD = 0.02
    VOLUME_OI_STRONG = 0.20
    VOLUME_OI_NORMAL = 0.10
    SPREAD_CAP = 50.0  # Cap spread % at 50 for scoring

    # Liquidity confidence thresholds (prevents noisy low-volume signals)
    MIN_VOLUME_FOR_MOMENTUM = 10  # Below this, momentum is heavily discounted
    MIN_OI_FOR_MOMENTUM = 50  # Below this, momentum is heavily discounted
    MIN_PRICE_FOR_MOMENTUM = 0.50  # Below $0.50, momentum is unreliable
    LIQUIDITY_RAMP_VOLUME = 100  # Full confidence at this volume
    LIQUIDITY_RAMP_OI = 500  # Full confidence at this OI
    LIQUIDITY_RAMP_PRICE = 5.0  # Full confidence at this price

    # Signal thresholds
    DISCOUNT_IV_RANK_THRESHOLD = 30
    DISCOUNT_MOMENTUM_THRESHOLD = 50
    DISCOUNT_SPREAD_THRESHOLD = 2.0
    RUNNER_MOMENTUM_THRESHOLD = 75
    RUNNER_VOLUME_THRESHOLD = 100
    RUNNER_VOL_OI_THRESHOLD = 0.10
    RUNNER_SPREAD_THRESHOLD = 3.0
    GREEKS_SPREAD_THRESHOLD = 2.0
    BUY_COMPOSITE_THRESHOLD = 65

    # Temporal smoothing (reduces daily ranking churn)
    MOMENTUM_SMOOTHING_WINDOW = 3  # 3-day EMA for momentum stability
    MOMENTUM_MAX_DAILY_CHANGE = 30.0

    # 7-day underlying metrics defaults
    DEFAULT_VOLATILITY_7D = 25.0  # Default 7-day annualized volatility (%)
    TRADING_DAYS_PER_YEAR = 252  # Standard trading days for annualization

    def __init__(
        self,
        momentum_weight: float = 0.40,
        value_weight: float = 0.35,
        greeks_weight: float = 0.25,
    ):
        """Initialize ranker with configurable weights.

        Args:
            momentum_weight: Weight for momentum score (default 0.40)
            value_weight: Weight for value score (default 0.35)
            greeks_weight: Weight for Greeks score (default 0.25)
        """
        # Validate weights sum to 1
        total = momentum_weight + value_weight + greeks_weight
        if abs(total - 1.0) > 0.001:
            logger.warning(f"Weights sum to {total}, normalizing")
            momentum_weight /= total
            value_weight /= total
            greeks_weight /= total

        self.MOMENTUM_WEIGHT = momentum_weight
        self.VALUE_WEIGHT = value_weight
        self.GREEKS_WEIGHT = greeks_weight

        logger.info(
            f"OptionsMomentumRanker initialized: "
            f"Momentum={momentum_weight:.0%}, Value={value_weight:.0%}, Greeks={greeks_weight:.0%}"
        )

    def rank_options(
        self,
        options_df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
        underlying_trend: str = "neutral",
        previous_rankings: Optional[pd.DataFrame] = None,
        ranking_mode: str = "entry",
        underlying_metrics: Optional[dict] = None,
    ) -> pd.DataFrame:
        """
        Rank options using the Momentum-Value-Greeks framework.

        Args:
            options_df: Current options chain with columns:
                - strike, side (call/put), expiration
                - bid, ask, last, mark
                - volume, openInterest (or open_interest)
                - delta, gamma, theta, vega, rho
                - impliedVolatility (or implied_vol, iv)
            iv_stats: 52-week IV statistics for the underlying
            options_history: Historical options data (5+ days) for momentum
            underlying_trend: bullish/neutral/bearish
            previous_rankings: Previous day's rankings for temporal smoothing
            underlying_metrics: 7-day underlying metrics (ret_7d, vol_7d, etc.)

        Returns:
            DataFrame with added scoring columns and sorted by composite_rank
        """
        if options_df.empty:
            logger.warning("No options data to rank")
            return options_df

        df = self._normalize_columns(options_df.copy())

        # Calculate component scores
        df = self._calculate_value_scores(df, iv_stats)
        df = self._calculate_momentum_scores(df, options_history)
        df = self._calculate_greeks_scores(df, underlying_trend)

        # Integrate 7-day underlying metrics into momentum score
        if underlying_metrics is not None:
            # Determine volatility regime from underlying metrics
            vol_7d = underlying_metrics.get("vol_7d", self.DEFAULT_VOLATILITY_7D) or self.DEFAULT_VOLATILITY_7D
            if vol_7d < 20.0:
                vol_regime = "low"
            elif vol_7d < 40.0:
                vol_regime = "normal"
            else:
                vol_regime = "high"

            df = self._integrate_underlying_metrics(
                df, underlying_metrics, underlying_trend, vol_regime
            )

        for col in ["momentum_score", "value_score", "greeks_score"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce").fillna(50.0)

        df["ranking_mode"] = ranking_mode

        df["relative_value_score"] = self._calculate_relative_value_score(df, options_history)
        df["entry_difficulty_score"] = self._calculate_entry_difficulty_score(df)

        # Apply IV staleness penalty if applicable
        if iv_stats and iv_stats.is_stale:
            staleness_penalty = iv_stats.staleness_penalty
            logger.warning(f"IV data is stale, applying {staleness_penalty:.1%} penalty")
            df["value_score"] *= 1 - staleness_penalty

        mode = (ranking_mode or "entry").lower()
        if mode == "exit":
            weights = self.EXIT_WEIGHTS
        elif mode == "entry":
            weights = self.ENTRY_WEIGHTS
        else:
            weights = {
                "momentum": self.MOMENTUM_WEIGHT,
                "value": self.VALUE_WEIGHT,
                "greeks": self.GREEKS_WEIGHT,
            }

        df["composite_rank"] = (
            df["momentum_score"] * weights["momentum"]
            + df["value_score"] * weights["value"]
            + df["greeks_score"] * weights["greeks"]
        )

        if mode == "entry":
            df["composite_rank"] = np.where(
                df["composite_rank"] > self.BUY_COMPOSITE_THRESHOLD,
                df["composite_rank"] * 0.90 + df["entry_difficulty_score"] * 0.10,
                df["composite_rank"],
            )
            df["composite_rank"] = df["composite_rank"] * 0.90 + df["relative_value_score"] * 0.10
        elif mode == "exit":
            theta_bonus = self._calculate_theta_bonus(df)
            df["composite_rank"] = df["composite_rank"] * 0.90 + theta_bonus * 0.10

        df["composite_rank"] = df["composite_rank"].clip(0, 100)

        # Apply temporal smoothing if previous rankings available
        if previous_rankings is not None and not previous_rankings.empty:
            df = self._apply_temporal_smoothing(df, previous_rankings)

        df["ranking_stability_score"] = self._calculate_ranking_stability_score(
            df, previous_rankings
        )

        # Generate signals
        df = self._generate_signals(df)

        # Sort by composite rank
        df = df.sort_values("composite_rank", ascending=False).reset_index(drop=True)

        # Log statistics
        self._log_ranking_stats(df)

        return df

    def _apply_temporal_smoothing(
        self,
        current: pd.DataFrame,
        previous: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        Apply EMA smoothing to momentum scores to reduce churn.

        Formula: smoothed = alpha * current + (1 - alpha) * previous
        Where alpha = 2 / (window + 1) for EMA

        Only smooths momentum_score and composite_rank, NOT raw Greeks.
        """
        alpha = 2 / (self.MOMENTUM_SMOOTHING_WINDOW + 1)  # ~0.5 for 3-day

        # Match contracts between current and previous
        for idx, row in current.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            # Find previous momentum score
            if "contract_symbol" in previous.columns:
                prev_match = previous[previous["contract_symbol"] == contract_id]
            else:
                prev_match = previous[
                    (previous["strike"] == row["strike"]) & (previous["side"] == row["side"])
                ]

            if len(prev_match) > 0:
                prev_momentum = prev_match.iloc[0].get("momentum_score", row["momentum_score"])

                # EMA smoothing on momentum score
                smoothed = alpha * row["momentum_score"] + (1 - alpha) * prev_momentum

                if abs(smoothed - prev_momentum) > self.MOMENTUM_MAX_DAILY_CHANGE:
                    smoothed = prev_momentum + (
                        np.sign(smoothed - prev_momentum) * self.MOMENTUM_MAX_DAILY_CHANGE
                    )
                current.at[idx, "momentum_score"] = smoothed

                # Recalculate composite with smoothed momentum
                mode = str(row.get("ranking_mode", "entry") or "entry").lower()
                if mode == "exit":
                    weights = self.EXIT_WEIGHTS
                elif mode == "entry":
                    weights = self.ENTRY_WEIGHTS
                else:
                    weights = {
                        "momentum": self.MOMENTUM_WEIGHT,
                        "value": self.VALUE_WEIGHT,
                        "greeks": self.GREEKS_WEIGHT,
                    }

                composite = (
                    smoothed * weights["momentum"]
                    + row["value_score"] * weights["value"]
                    + row["greeks_score"] * weights["greeks"]
                )

                if mode == "entry":
                    entry_diff = row.get("entry_difficulty_score", 50.0)
                    rel_val = row.get("relative_value_score", 50.0)
                    if composite > self.BUY_COMPOSITE_THRESHOLD:
                        composite = composite * 0.90 + entry_diff * 0.10
                    composite = composite * 0.90 + rel_val * 0.10
                elif mode == "exit":
                    theta_bonus = self._calculate_theta_bonus(current.loc[[idx]]).iloc[0]
                    composite = composite * 0.90 + theta_bonus * 0.10

                current.at[idx, "composite_rank"] = float(max(0.0, min(100.0, composite)))

        return current

    def _calculate_relative_value_score(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        if options_history is None or options_history.empty:
            return pd.Series(50.0, index=df.index)

        hist = options_history.copy()
        if "contract_symbol" not in hist.columns:
            return pd.Series(50.0, index=df.index)

        if "iv" not in hist.columns:
            if "implied_vol" in hist.columns:
                hist["iv"] = hist["implied_vol"]
            elif "impliedVolatility" in hist.columns:
                hist["iv"] = hist["impliedVolatility"]

        if "iv" not in hist.columns:
            return pd.Series(50.0, index=df.index)

        if "bid" not in hist.columns or "ask" not in hist.columns:
            hist["spread_pct"] = np.nan
        else:
            mid = (hist["bid"] + hist["ask"]) / 2
            spread = hist["ask"] - hist["bid"]
            hist["spread_pct"] = np.where(mid > 0, (spread / mid) * 100, np.nan)

        iv_avg = hist.groupby("contract_symbol")["iv"].mean()
        spread_avg = hist.groupby("contract_symbol")["spread_pct"].mean()

        contract_ids = df.get("contract_symbol", pd.Series("", index=df.index))
        hist_iv_avg = contract_ids.map(iv_avg)
        hist_spread_avg = contract_ids.map(spread_avg)

        current_iv = df.get("iv", pd.Series(np.nan, index=df.index))
        current_spread = df.get("spread_pct", pd.Series(np.nan, index=df.index))

        iv_discount = np.where(
            (hist_iv_avg > 0) & np.isfinite(hist_iv_avg) & np.isfinite(current_iv),
            1.0 - (current_iv / hist_iv_avg),
            0.0,
        )
        spread_discount = np.where(
            (hist_spread_avg > 0) & np.isfinite(hist_spread_avg) & np.isfinite(current_spread),
            1.0 - (current_spread / hist_spread_avg),
            0.0,
        )

        score = (0.70 * iv_discount + 0.30 * spread_discount) * 100
        return pd.Series(score, index=df.index).clip(0, 100)

    def _calculate_entry_difficulty_score(self, df: pd.DataFrame) -> pd.Series:
        bid = df.get("bid", pd.Series(0.0, index=df.index))
        ask = df.get("ask", pd.Series(0.0, index=df.index))
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(0, index=df.index))

        mid = (bid + ask) / 2
        spread = ask - bid
        spread_pct = np.where(mid > 0, (spread / mid) * 100, 100.0)
        vol_oi_ratio = np.where(oi > 0, volume / oi, 0.0)

        spread_penalty = np.clip(spread_pct / 2.0, 0.0, 1.0)
        vol_bonus = np.minimum(vol_oi_ratio, 1.0) * 0.5

        score = (1.0 - spread_penalty + vol_bonus) * 100
        return pd.Series(score, index=df.index).clip(0, 100)

    def _calculate_theta_bonus(self, df: pd.DataFrame) -> pd.Series:
        theta = df.get("theta", pd.Series(0.0, index=df.index)).abs()
        mid_price = df.get("mid", df.get("mark", pd.Series(1.0, index=df.index)))
        theta_pct = np.where(mid_price > 0, (theta / mid_price) * 100, 100.0)
        bonus = 100 - np.minimum(theta_pct * 10, 100)
        return pd.Series(bonus, index=df.index).clip(0, 100)

    def _calculate_ranking_stability_score(
        self,
        current: pd.DataFrame,
        previous: Optional[pd.DataFrame],
    ) -> pd.Series:
        if previous is None or previous.empty:
            return pd.Series(np.nan, index=current.index)

        scores = pd.Series(np.nan, index=current.index)

        for idx, row in current.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            if "contract_symbol" in previous.columns:
                prev_match = previous[previous["contract_symbol"] == contract_id]
            else:
                prev_match = previous[
                    (previous["strike"] == row["strike"]) & (previous["side"] == row["side"])
                ]

            if len(prev_match) > 0:
                prev_rank = prev_match.iloc[0].get("composite_rank", np.nan)
                curr_rank = row.get("composite_rank", np.nan)
                if np.isfinite(prev_rank) and np.isfinite(curr_rank):
                    delta = abs(curr_rank - prev_rank)
                    scores[idx] = max(0.0, min(100.0, 100.0 - (delta * 2.0)))

        return scores

    def _normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalize column names for consistent processing."""
        rename_map = {
            "impliedVolatility": "iv",
            "implied_vol": "iv",
            "implied_volatility": "iv",
            "openInterest": "open_interest",
            "option_type": "side",
        }
        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

        # Ensure required columns exist
        if "mid" not in df.columns and "bid" in df.columns and "ask" in df.columns:
            df["mid"] = (df["bid"] + df["ask"]) / 2
        if "mark" not in df.columns:
            df["mark"] = df.get("mid", df.get("last", 0))

        return df

    # =========================================================================
    # VALUE SCORING (35% of total)
    # =========================================================================

    def _calculate_value_scores(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
    ) -> pd.DataFrame:
        """Calculate value score from IV Rank and bid-ask spread.

        For a BUYER, lower IV is better, so we invert it:
            iv_value_score = 100 - iv_rank

        For spread:
            spread_penalty = min(spread_% × 2, 50)
            spread_score = 100 - spread_penalty

        Value Score = iv_value_score × 0.60 + spread_score × 0.40
        """
        # IV Rank component (60% of value)
        # iv_rank is already 0-100 percentile
        df["iv_rank"] = self._calculate_iv_rank(df, iv_stats)
        df["iv_rank_score"] = 100 - df["iv_rank"]  # Lower IVR = higher score for buyer

        # Bid-Ask Spread component (40% of value)
        df["spread_pct"] = self._calculate_spread_pct(df)
        df["spread_score"] = self._calculate_spread_score(df["spread_pct"])

        # Combined Value Score (0-100)
        df["value_score"] = (
            df["iv_rank_score"] * self.IV_RANK_WEIGHT + df["spread_score"] * self.SPREAD_WEIGHT
        )

        return df

    def _calculate_iv_rank(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
    ) -> pd.Series:
        """Calculate IV Rank: (IV_current - IV_52low) / (IV_52high - IV_52low) × 100.

        If no IV stats provided, estimate from current chain.
        """
        if "iv" not in df.columns:
            logger.warning("No IV column found, defaulting to 50")
            return pd.Series(50.0, index=df.index)

        if iv_stats:
            # Use provided 52-week statistics
            iv_range = iv_stats.iv_high - iv_stats.iv_low
            if iv_range > 0:
                iv_rank = ((df["iv"] - iv_stats.iv_low) / iv_range) * 100
            else:
                iv_rank = pd.Series(50.0, index=df.index)
        else:
            # Estimate from current chain (ATM IV as proxy)
            iv_min = df["iv"].min()
            iv_max = df["iv"].max()
            iv_range = iv_max - iv_min

            if iv_range > 0:
                iv_rank = ((df["iv"] - iv_min) / iv_range) * 100
            else:
                iv_rank = pd.Series(50.0, index=df.index)

            logger.debug(f"Estimated IV Rank from chain: min={iv_min:.2%}, max={iv_max:.2%}")

        return iv_rank.clip(0, 100)

    def _calculate_spread_pct(self, df: pd.DataFrame) -> pd.Series:
        """Calculate bid-ask spread as percentage of mid price.

        spread_% = (ask - bid) / mid × 100
        """
        if "bid" not in df.columns or "ask" not in df.columns:
            return pd.Series(5.0, index=df.index)  # Default 5% spread

        mid = (df["bid"] + df["ask"]) / 2
        spread = df["ask"] - df["bid"]

        # Avoid division by zero
        spread_pct = np.where(mid > 0, (spread / mid) * 100, 100)

        return pd.Series(spread_pct, index=df.index)

    def _calculate_spread_score(self, spread_pct: pd.Series) -> pd.Series:
        """Calculate spread score from spread percentage.

        Formula:
            spread_penalty = min(spread_% × 2, 50)
            spread_score = 100 - spread_penalty

        Examples:
            - 0% spread → penalty 0 → score 100
            - 10% spread → penalty 20 → score 80
            - 25%+ spread → penalty 50 → score 50
        """
        spread_penalty = (spread_pct * 2).clip(upper=50)
        return 100 - spread_penalty

    # =========================================================================
    # MOMENTUM SCORING (40% of total)
    # =========================================================================

    def _calculate_liquidity_confidence(self, df: pd.DataFrame) -> pd.Series:
        """Calculate liquidity confidence multiplier (0.1 to 1.0).

        Low-volume/low-price options get dampened momentum scores because
        percentage changes are noisy and unreliable.

        Formula uses geometric mean of three confidence factors:
        - Volume confidence: ramps from 0.1 at vol=0 to 1.0 at vol=100
        - OI confidence: ramps from 0.1 at OI=0 to 1.0 at OI=500
        - Price confidence: ramps from 0.1 at $0 to 1.0 at $5

        Examples:
        - Vol=5, OI=20, Price=$0.50 → ~0.25 confidence (heavily dampened)
        - Vol=50, OI=200, Price=$2 → ~0.65 confidence (moderately dampened)
        - Vol=100+, OI=500+, Price=$5+ → 1.0 confidence (full weight)
        """
        # Get volume, OI, and price
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(0, index=df.index))
        price = df.get("mid", df.get("mark", pd.Series(1, index=df.index)))

        # Calculate individual confidence factors (0.1 to 1.0)
        min_conf = 0.1  # Floor to avoid zeroing out completely

        # Volume confidence: linear ramp from min to 1.0
        vol_conf = np.clip(
            min_conf + (1 - min_conf) * volume / self.LIQUIDITY_RAMP_VOLUME, min_conf, 1.0
        )

        # OI confidence
        oi_conf = np.clip(min_conf + (1 - min_conf) * oi / self.LIQUIDITY_RAMP_OI, min_conf, 1.0)

        # Price confidence
        price_conf = np.clip(
            min_conf + (1 - min_conf) * price / self.LIQUIDITY_RAMP_PRICE, min_conf, 1.0
        )

        # Geometric mean of all three factors
        confidence = (vol_conf * oi_conf * price_conf) ** (1 / 3)

        return pd.Series(confidence, index=df.index)

    def _calculate_momentum_scores(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """Calculate momentum score from price change, Vol/OI, and OI growth.

        Momentum Score = (Price_Mom × 0.50 + Vol/OI × 0.30 + OI_Growth × 0.20)
                         × Liquidity_Confidence

        The liquidity confidence multiplier dampens scores for low-volume,
        low-OI, or low-price options where percentage changes are noisy.
        """
        # Calculate liquidity confidence multiplier first
        df["liquidity_confidence"] = self._calculate_liquidity_confidence(df)

        # Price Momentum (50% of momentum)
        df["price_momentum"] = self._calculate_price_momentum(df, options_history)
        df["price_momentum_score"] = self._normalize_momentum(df["price_momentum"])

        # Volume/OI Ratio (30% of momentum)
        df["vol_oi_ratio"] = self._calculate_vol_oi_ratio(df)
        df["vol_oi_score"] = self._score_vol_oi(df["vol_oi_ratio"])

        # OI Growth (20% of momentum)
        df["oi_growth"] = self._calculate_oi_growth(df, options_history)
        df["oi_growth_score"] = self._normalize_oi_growth(df["oi_growth"])

        # Combined Momentum Score with liquidity dampening
        raw_momentum = (
            df["price_momentum_score"] * self.PRICE_MOMENTUM_WEIGHT
            + df["vol_oi_score"] * self.VOLUME_OI_WEIGHT
            + df["oi_growth_score"] * self.OI_GROWTH_WEIGHT
        )

        # Apply liquidity confidence - low liquidity dampens toward neutral
        # Formula: score = 50 + (raw_score - 50) * confidence
        # Pulls extreme scores toward 50 for illiquid options
        liq_conf = df["liquidity_confidence"]
        df["momentum_score"] = 50 + (raw_momentum - 50) * liq_conf

        return df

    def _calculate_underlying_7d_score(
        self,
        underlying_metrics: Optional[dict] = None,
        trend_regime: str = "neutral",
        vol_regime: str = "normal",
    ) -> float:
        """
        Calculate score from underlying 7-day metrics.

        Integrates return, volatility, drawdown, and gap count from
        the underlying asset's 7-day price history.

        Args:
            underlying_metrics: Dict with ret_7d, vol_7d, drawdown_7d, gap_count
            trend_regime: Current trend regime (bullish, bearish, neutral)
            vol_regime: Current volatility regime (low, normal, high)

        Returns:
            Score from 0-100 based on underlying performance
        """
        if underlying_metrics is None:
            return 50.0  # Neutral score if no metrics available

        # Extract metrics with defaults
        ret_7d = underlying_metrics.get("ret_7d", 0.0) or 0.0
        vol_7d = underlying_metrics.get("vol_7d", self.DEFAULT_VOLATILITY_7D) or self.DEFAULT_VOLATILITY_7D
        drawdown_7d = underlying_metrics.get("drawdown_7d", 0.0) or 0.0
        gap_count = underlying_metrics.get("gap_count", 0) or 0

        # Score return component (0-100)
        # >10% return = 100, 0% = 50, <-10% = 0
        if ret_7d >= 10.0:
            return_score = 100.0
        elif ret_7d >= 5.0:
            return_score = 75.0 + (ret_7d - 5.0) * 5.0
        elif ret_7d >= 0.0:
            return_score = 50.0 + ret_7d * 5.0
        elif ret_7d >= -5.0:
            return_score = 50.0 + ret_7d * 5.0
        elif ret_7d >= -10.0:
            return_score = 25.0 + (ret_7d + 5.0) * 5.0
        else:
            return_score = max(0.0, 25.0 + (ret_7d + 10.0) * 2.5)

        # Score volatility component (0-100)
        # Low volatility during uptrend = good, high volatility = risky
        if vol_7d < 15.0:
            vol_score = 80.0  # Low volatility = stable
        elif vol_7d < 30.0:
            vol_score = 60.0  # Normal volatility
        elif vol_7d < 50.0:
            vol_score = 40.0  # Elevated volatility
        else:
            vol_score = 20.0  # High volatility = risky

        # Score drawdown component (0-100)
        # Lower drawdown = better
        if drawdown_7d < 3.0:
            drawdown_score = 90.0  # Minor drawdown
        elif drawdown_7d < 7.0:
            drawdown_score = 70.0  # Moderate drawdown
        elif drawdown_7d < 15.0:
            drawdown_score = 40.0  # Significant drawdown
        else:
            drawdown_score = 20.0  # Severe drawdown

        # Score gap count component (0-100)
        # Fewer gaps = more stable price action
        if gap_count <= 1:
            gap_score = 90.0  # Very stable
        elif gap_count <= 3:
            gap_score = 60.0  # Some discontinuity
        else:
            gap_score = 30.0  # High discontinuity

        # Combine scores with weights from config
        # Default weights: return=0.40, volatility=0.30, drawdown=0.20, gaps=0.10
        combined_score = (
            return_score * 0.40
            + vol_score * 0.30
            + drawdown_score * 0.20
            + gap_score * 0.10
        )

        # Apply regime multipliers if in specific regime
        regime_key = f"{trend_regime}_{vol_regime}".lower()
        regime_multipliers = {
            "bullish_low": 1.1,
            "bullish_normal": 1.0,
            "bullish_high": 0.95,
            "bearish_low": 0.9,
            "bearish_normal": 0.85,
            "bearish_high": 0.8,
            "neutral_low": 1.0,
            "neutral_normal": 1.0,
            "neutral_high": 0.9,
        }

        multiplier = regime_multipliers.get(regime_key, 1.0)
        final_score = combined_score * multiplier

        return max(0.0, min(100.0, final_score))

    def _integrate_underlying_metrics(
        self,
        df: pd.DataFrame,
        underlying_metrics: Optional[dict] = None,
        trend_regime: str = "neutral",
        vol_regime: str = "normal",
    ) -> pd.DataFrame:
        """
        Integrate underlying 7-day metrics into the momentum score.

        Blends the underlying performance score with the existing
        option-specific momentum score.

        Args:
            df: DataFrame with momentum_score column
            underlying_metrics: Dict with ret_7d, vol_7d, drawdown_7d, gap_count
            trend_regime: Current trend regime
            vol_regime: Current volatility regime

        Returns:
            DataFrame with updated momentum_score incorporating underlying metrics
        """
        if underlying_metrics is None:
            # No underlying metrics available, return unchanged
            df["underlying_7d_score"] = 50.0
            return df

        # Calculate underlying 7-day score
        underlying_score = self._calculate_underlying_7d_score(
            underlying_metrics, trend_regime, vol_regime
        )
        df["underlying_7d_score"] = underlying_score

        # Store individual metric values for transparency
        df["underlying_ret_7d"] = underlying_metrics.get("ret_7d", 0.0)
        df["underlying_vol_7d"] = underlying_metrics.get("vol_7d", 0.0)
        df["underlying_drawdown_7d"] = underlying_metrics.get("drawdown_7d", 0.0)
        df["underlying_gap_count"] = underlying_metrics.get("gap_count", 0)

        # Blend underlying score into momentum
        # Default blend: 80% option momentum + 20% underlying momentum
        underlying_weight = 0.20
        df["momentum_score"] = (
            df["momentum_score"] * (1 - underlying_weight)
            + underlying_score * underlying_weight
        )

        return df

    def _calculate_price_momentum(
        self,
        df: pd.DataFrame,
        history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate 5-day price momentum.

        Returns percentage change from 5 days ago.
        """
        if history is None or history.empty:
            # Without history, use volume as momentum proxy
            if "volume" in df.columns:
                vol_normalized = df["volume"] / (df["volume"].max() + 1)
                return vol_normalized * 20 - 10  # Range -10% to +10%
            return pd.Series(0.0, index=df.index)

        # Match current options to history by contract_symbol or strike+side+expiration
        momentum = pd.Series(0.0, index=df.index)

        for idx, row in df.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            # Find matching historical data
            if "contract_symbol" in history.columns:
                hist_match = history[history["contract_symbol"] == contract_id]
            else:
                hist_match = history[
                    (history["strike"] == row["strike"]) & (history["side"] == row["side"])
                ]

            if len(hist_match) >= 2:
                # Calculate price change
                old_price = hist_match.iloc[0].get("last", hist_match.iloc[0].get("mark", 0))
                new_price = row.get("last", row.get("mark", 0))

                if old_price > 0:
                    momentum[idx] = ((new_price - old_price) / old_price) * 100

        return momentum

    def _normalize_momentum(self, momentum: pd.Series) -> pd.Series:
        """Normalize price momentum to 0-100 score.

        Formula: min(max(Momentum × 2 + 50, 0), 100)
        - 100% change → Score ~95-100
        - 25% change → Score ~70-75
        - 0% change → Score 50
        - -25% change → Score ~25-30
        """
        return (momentum * 2 + 50).clip(0, 100)

    def _calculate_vol_oi_ratio(self, df: pd.DataFrame) -> pd.Series:
        """Calculate Volume / Open Interest ratio."""
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(1, index=df.index))

        # Avoid division by zero
        ratio = np.where(oi > 0, volume / oi, 0)

        return pd.Series(ratio, index=df.index)

    def _score_vol_oi(self, ratio: pd.Series) -> pd.Series:
        """Score Volume/OI ratio.

        Formula: min(ratio / 0.20 × 100, 100)
        - Ratio > 0.20: Score 100 (very active)
        - Ratio 0.10: Score 50 (normal)
        - Ratio 0.05: Score 25 (low)
        """
        return (ratio / self.VOLUME_OI_STRONG * 100).clip(0, 100)

    def _calculate_oi_growth(
        self,
        df: pd.DataFrame,
        history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate Open Interest growth over 5 days."""
        if history is None or history.empty:
            return pd.Series(0.0, index=df.index)

        growth = pd.Series(0.0, index=df.index)

        for idx, row in df.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            if "contract_symbol" in history.columns:
                hist_match = history[history["contract_symbol"] == contract_id]
            else:
                hist_match = history[
                    (history["strike"] == row["strike"]) & (history["side"] == row["side"])
                ]

            if len(hist_match) >= 1:
                old_oi = hist_match.iloc[0].get("open_interest", 0)
                new_oi = row.get("open_interest", 0)

                if old_oi > 0:
                    growth[idx] = ((new_oi - old_oi) / old_oi) * 100

        return growth

    def _normalize_oi_growth(self, growth: pd.Series) -> pd.Series:
        """Normalize OI growth to 0-100 score.

        Formula: min(max(growth + 50, 0), 100)
        - 50% growth → Score 100
        - 0% growth → Score 50
        - -50% growth → Score 0
        """
        return (growth + 50).clip(0, 100)

    # =========================================================================
    # GREEKS SCORING (25% of total)
    # =========================================================================

    def _calculate_greeks_scores(
        self,
        df: pd.DataFrame,
        underlying_trend: str = "neutral",
    ) -> pd.DataFrame:
        """Calculate Greeks-based score for directional alignment.

        Formulas:
            delta_score = 100 - 100 × |Δ - 0.55| (target 0.55 for calls, -0.55 for puts)
            gamma_score = min(γ / 0.04 × 100, 100)
            vega_score = min(vega / 0.30 × 100, 100)
            theta_penalty = min(|θ%| × 10, 40) where θ% = θ/mid × 100

        Greeks Pre-Score = Delta × 0.50 + Gamma × 0.35 + Vega × 0.10
        Greeks Score = clip(Greeks_Pre - Theta_Penalty, 0, 100)
        """
        # Delta Score (50%)
        df["delta_score"] = self._score_delta(df, underlying_trend)

        # Gamma Score (35%)
        df["gamma_score"] = self._score_gamma(df)

        # Vega Score (10%)
        df["vega_score"] = self._score_vega(df)

        # Theta Penalty
        df["theta_penalty"] = self._calculate_theta_penalty(df)

        # Combined Greeks Score
        df["greeks_score"] = (
            df["delta_score"] * self.DELTA_WEIGHT
            + df["gamma_score"] * self.GAMMA_WEIGHT
            + df["vega_score"] * self.VEGA_WEIGHT
            - df["theta_penalty"]
        ).clip(0, 100)

        return df

    def _score_delta(self, df: pd.DataFrame, trend: str) -> pd.Series:
        """Score delta based on distance from optimal target.

        Formula:
            Calls (target band 0.4-0.8, sweet spot ~0.55):
                delta_score = 100 - 100 × |Δ - 0.55|

            Puts: same idea with target -0.55
                delta_score = 100 - 100 × |Δ - (-0.55)|

        Examples for calls (target 0.55):
            - Delta 0.55 → score = 100 - 0 = 100
            - Delta 0.45 → score = 100 - 10 = 90
            - Delta 0.70 → score = 100 - 15 = 85
            - Delta 0.30 → score = 100 - 25 = 75

        Trend alignment provides additional multiplier.
        """
        scores = pd.Series(50.0, index=df.index)

        for idx, row in df.iterrows():
            delta = row.get("delta", 0.5)
            side = row.get("side", "call")

            # Target delta based on side
            if side == "call":
                target = self.OPTIMAL_DELTA_TARGET  # 0.55
            else:
                target = -self.OPTIMAL_DELTA_TARGET  # -0.55

            # Core formula: delta_score = 100 - 100 × |Δ - target|
            deviation = abs(delta - target)
            base_score = 100 - (100 * deviation)

            # Trend alignment multiplier
            if trend == "bullish" and side == "call":
                alignment_mult = 1.0
            elif trend == "bearish" and side == "put":
                alignment_mult = 1.0
            elif trend == "neutral":
                alignment_mult = 0.90  # Small penalty for no trend
            else:
                # Counter-trend (bullish but put, or bearish but call)
                alignment_mult = 0.70

            scores[idx] = max(0, min(100, base_score * alignment_mult))

        return scores

    def _score_gamma(self, df: pd.DataFrame) -> pd.Series:
        """Score gamma based on acceleration potential.

        Formula: min(gamma / 0.04 × 100, 100)
        - Gamma > 0.04: Score 100 (excellent)
        - Gamma 0.02: Score 50 (good)
        - Gamma < 0.01: Score 25 (weak)
        """
        gamma = df.get("gamma", pd.Series(0.02, index=df.index))

        return (gamma / self.GAMMA_EXCELLENT_THRESHOLD * 100).clip(0, 100)

    def _score_vega(self, df: pd.DataFrame) -> pd.Series:
        """Score vega for volatility exposure.

        For buying low IV: higher vega = better (benefits from IV expansion)
        Formula: min(vega / 0.30 × 100, 100)
        """
        vega = df.get("vega", pd.Series(0.15, index=df.index))

        return (vega / 0.30 * 100).clip(0, 100)

    def _calculate_theta_penalty(self, df: pd.DataFrame) -> pd.Series:
        """Calculate theta decay penalty.

        Formula:
            θ% = (θ / mid) × 100 (daily decay as % of price)
            theta_penalty = min(|θ%| × 10, 40)

        Examples:
            - θ = -$0.02, mid = $2.00 → θ% = -1% → penalty = 10
            - θ = -$0.05, mid = $1.00 → θ% = -5% → penalty = 40 (capped)

        Capped at 40 to not eliminate high-momentum options.
        """
        theta = df.get("theta", pd.Series(-0.10, index=df.index)).abs()
        mid_price = df.get("mid", df.get("mark", pd.Series(1.0, index=df.index)))

        # Avoid division by zero
        theta_pct = np.where(mid_price > 0, (theta / mid_price) * 100, 10)

        # penalty = min(|θ%| × 10, 40)
        penalty = (pd.Series(theta_pct, index=df.index).abs() * 10).clip(0, 40)

        return penalty

    # =========================================================================
    # SIGNAL GENERATION
    # =========================================================================

    def _generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:
        """Generate trading signals based on framework criteria."""

        # Discount Signal: Low IV + momentum + liquid
        df["signal_discount"] = (
            (df["iv_rank"] < self.DISCOUNT_IV_RANK_THRESHOLD)
            & (df["momentum_score"] > self.DISCOUNT_MOMENTUM_THRESHOLD)
            & (df["spread_pct"] < self.DISCOUNT_SPREAD_THRESHOLD)
        )

        # Runner Signal: High momentum + volume + activity
        df["signal_runner"] = (
            (df["momentum_score"] > self.RUNNER_MOMENTUM_THRESHOLD)
            & (df.get("volume", 0) > self.RUNNER_VOLUME_THRESHOLD)
            & (df["vol_oi_ratio"] > self.RUNNER_VOL_OI_THRESHOLD)
            & (df["spread_pct"] < self.RUNNER_SPREAD_THRESHOLD)
        )

        # Greeks-Aligned Signal: Optimal delta/gamma + liquid
        df["signal_greeks"] = (
            (df["delta"].abs() >= self.OPTIMAL_DELTA_MIN)
            & (df["delta"].abs() <= self.OPTIMAL_DELTA_MAX)
            & (df.get("gamma", 0) > self.GAMMA_GOOD_THRESHOLD)
            & (df["spread_pct"] < self.GREEKS_SPREAD_THRESHOLD)
        )

        # Buy Signal: Composite threshold + at least one signal
        df["signal_buy"] = (df["composite_rank"] > self.BUY_COMPOSITE_THRESHOLD) & (
            df["signal_discount"] | df["signal_runner"] | df["signal_greeks"]
        )

        # Create signal summary
        df["signals"] = df.apply(lambda row: self._summarize_signals(row), axis=1)

        return df

    def _summarize_signals(self, row: pd.Series) -> str:
        """Create comma-separated signal summary."""
        signals = []
        if row.get("signal_discount", False):
            signals.append("DISCOUNT")
        if row.get("signal_runner", False):
            signals.append("RUNNER")
        if row.get("signal_greeks", False):
            signals.append("GREEKS")
        if row.get("signal_buy", False):
            signals.append("BUY")
        return ",".join(signals) if signals else ""

    # =========================================================================
    # UTILITY METHODS
    # =========================================================================

    def _log_ranking_stats(self, df: pd.DataFrame) -> None:
        """Log ranking statistics."""
        logger.info(f"Ranked {len(df)} options contracts")
        logger.info(
            f"Composite range: {df['composite_rank'].min():.1f} - {df['composite_rank'].max():.1f}"
        )

        # Signal counts
        discount_count = df["signal_discount"].sum()
        runner_count = df["signal_runner"].sum()
        greeks_count = df["signal_greeks"].sum()
        buy_count = df["signal_buy"].sum()

        logger.info(
            f"Signals: DISCOUNT={discount_count}, RUNNER={runner_count}, "
            f"GREEKS={greeks_count}, BUY={buy_count}"
        )

        # Top 3 opportunities
        if len(df) >= 3:
            top3 = df.head(3)
            logger.info("Top 3 opportunities:")
            for _, row in top3.iterrows():
                logger.info(
                    f"  {row.get('contract_symbol', 'N/A')}: "
                    f"Rank={row['composite_rank']:.1f}, "
                    f"Signals=[{row['signals']}]"
                )

    def get_interpretation(self, composite_rank: float) -> str:
        """Get human-readable interpretation of composite rank."""
        if composite_rank >= 80:
            return "Exceptional opportunity (all dimensions strong)"
        elif composite_rank >= 65:
            return "Good opportunity (two+ dimensions strong)"
        elif composite_rank >= 50:
            return "Fair opportunity (mixed signals)"
        else:
            return "Poor opportunity (one+ dimension weak)"

    def create_aggressive_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for aggressive momentum trading."""
        return OptionsMomentumRanker(
            momentum_weight=0.60,
            value_weight=0.25,
            greeks_weight=0.15,
        )

    def create_conservative_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for conservative value hunting."""
        return OptionsMomentumRanker(
            momentum_weight=0.20,
            value_weight=0.60,
            greeks_weight=0.20,
        )

    def create_greeks_focused_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for Greeks-focused risk management."""
        return OptionsMomentumRanker(
            momentum_weight=0.25,
            value_weight=0.30,
            greeks_weight=0.45,
        )


class CalibratedMomentumRanker(OptionsMomentumRanker):
    """
    Extended ranker with calibration and regime-conditioning.

    Implements Perplexity's recommendations:
    1. Calibration-to-forward-return using isotonic regression
    2. Regime-conditioning weights based on trend/vol regime
    3. Integration with ranking monitor for alerts
    """

    def __init__(
        self,
        momentum_weight: float = 0.40,
        value_weight: float = 0.35,
        greeks_weight: float = 0.25,
        enable_calibration: bool = True,
        enable_regime_conditioning: bool = True,
    ):
        """
        Initialize calibrated ranker.

        Args:
            momentum_weight: Base momentum weight
            value_weight: Base value weight
            greeks_weight: Base greeks weight
            enable_calibration: Apply isotonic calibration
            enable_regime_conditioning: Adjust weights by regime
        """
        super().__init__(momentum_weight, value_weight, greeks_weight)

        self.enable_calibration = enable_calibration
        self.enable_regime_conditioning = enable_regime_conditioning

        self._calibrator = None
        self._regime_conditioner = None

        if enable_calibration:
            from .ranking_calibrator import IsotonicCalibrator

            self._calibrator = IsotonicCalibrator()

        if enable_regime_conditioning:
            from .regime_conditioner import RegimeConditioner

            self._regime_conditioner = RegimeConditioner()

    def fit_calibrator(
        self,
        historical_scores: np.ndarray,
        forward_returns: np.ndarray,
    ) -> None:
        """
        Fit the calibrator on historical data.

        Should be called periodically (e.g., weekly) with recent data.

        Args:
            historical_scores: Past composite_rank values
            forward_returns: Corresponding forward returns
        """
        if self._calibrator is None:
            logger.warning("Calibration disabled, skipping fit")
            return

        result = self._calibrator.fit(historical_scores, forward_returns)
        logger.info(f"Calibrator fitted: {result}")

    def rank_options_calibrated(
        self,
        options_df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
        underlying_df: Optional[pd.DataFrame] = None,
        underlying_trend: str = "neutral",
        previous_rankings: Optional[pd.DataFrame] = None,
        ranking_mode: str = "entry",
        underlying_metrics: Optional[dict] = None,
    ) -> pd.DataFrame:
        """
        Rank options with calibration and regime-conditioning.

        This is the enhanced entry point that:
        1. Detects market regime from underlying OHLC
        2. Adjusts weights based on regime
        3. Applies base ranking with 7-day underlying metrics
        4. Calibrates scores to forward return percentiles

        Args:
            options_df: Options chain data
            iv_stats: IV statistics
            options_history: Historical options data
            underlying_df: OHLC data for regime detection
            underlying_trend: Fallback trend if no OHLC
            previous_rankings: Previous rankings for smoothing
            ranking_mode: 'entry' or 'exit'
            underlying_metrics: 7-day underlying metrics (ret_7d, vol_7d, etc.)

        Returns:
            DataFrame with calibrated rankings
        """
        # Step 1: Detect regime and get conditioned weights
        regime_state = None
        if self.enable_regime_conditioning and underlying_df is not None:
            if len(underlying_df) >= 20:
                regime_state = self._regime_conditioner.detect_regime(underlying_df)
                weights = self._regime_conditioner.get_regime_weights(regime_state)

                # Temporarily override weights
                orig_momentum = self.MOMENTUM_WEIGHT
                orig_value = self.VALUE_WEIGHT
                orig_greeks = self.GREEKS_WEIGHT

                self.MOMENTUM_WEIGHT = weights.momentum
                self.VALUE_WEIGHT = weights.value
                self.GREEKS_WEIGHT = weights.greeks

                logger.info(
                    f"Regime: {regime_state.trend_regime.value}/"
                    f"{regime_state.vol_regime.value}, "
                    f"Weights: M={weights.momentum:.0%}, "
                    f"V={weights.value:.0%}, G={weights.greeks:.0%}"
                )

        # Step 2: Apply base ranking with underlying metrics
        ranked_df = self.rank_options(
            options_df,
            iv_stats=iv_stats,
            options_history=options_history,
            underlying_trend=underlying_trend,
            previous_rankings=previous_rankings,
            ranking_mode=ranking_mode,
            underlying_metrics=underlying_metrics,
        )

        # Restore original weights if we changed them
        if regime_state is not None:
            self.MOMENTUM_WEIGHT = orig_momentum
            self.VALUE_WEIGHT = orig_value
            self.GREEKS_WEIGHT = orig_greeks

        # Step 3: Apply calibration
        if self.enable_calibration and self._calibrator._is_fitted:
            ranked_df = self._calibrator.calibrate_rankings(ranked_df, score_col="composite_rank")

            # Use calibrated rank as primary sort
            ranked_df = ranked_df.sort_values("calibrated_rank", ascending=False).reset_index(
                drop=True
            )

            logger.info(
                f"Calibrated: range {ranked_df['calibrated_rank'].min():.1f}-"
                f"{ranked_df['calibrated_rank'].max():.1f}, "
                f"P(+) range {ranked_df['calibrated_positive_prob'].min():.2f}-"
                f"{ranked_df['calibrated_positive_prob'].max():.2f}"
            )
        else:
            # Add placeholder columns if not calibrated
            ranked_df["calibrated_return_pct"] = ranked_df["composite_rank"]
            ranked_df["calibrated_positive_prob"] = 0.5
            ranked_df["calibrated_rank"] = ranked_df["composite_rank"]

        # Step 4: Add regime metadata
        if regime_state is not None:
            ranked_df["trend_regime"] = regime_state.trend_regime.value
            ranked_df["vol_regime"] = regime_state.vol_regime.value
            ranked_df["regime_adx"] = regime_state.adx
            ranked_df["regime_atr_pct"] = regime_state.atr_pct

            # Apply signal emphasis based on regime
            emphasis = self._regime_conditioner.get_signal_emphasis(regime_state)
            ranked_df["signal_emphasis_runner"] = emphasis["runner"]
            ranked_df["signal_emphasis_discount"] = emphasis["discount"]
            ranked_df["signal_emphasis_greeks"] = emphasis["greeks"]

        return ranked_df

    def save_calibrator(self, path: str) -> None:
        """Save calibrator state to file."""
        if self._calibrator is not None:
            self._calibrator.save(path)

    def load_calibrator(self, path: str) -> None:
        """Load calibrator state from file."""
        if self._calibrator is not None:
            from .ranking_calibrator import IsotonicCalibrator

            self._calibrator = IsotonicCalibrator.load(path)


class IVHistoryCalculator:
    """Calculate IV statistics from historical data."""

    @staticmethod
    def calculate_iv_stats(
        iv_history: pd.DataFrame,
        current_iv: float,
        lookback_days: int = 252,
    ) -> IVStatistics:
        """Calculate 52-week IV statistics.

        Args:
            iv_history: DataFrame with 'date' and 'iv' columns
            current_iv: Current implied volatility
            lookback_days: Days to look back (default 252 = 1 year)

        Returns:
            IVStatistics with high, low, median, current, and rank
        """
        if iv_history is None or iv_history.empty:
            # Return default stats if no history
            return IVStatistics(
                iv_high=current_iv * 1.5,
                iv_low=current_iv * 0.5,
                iv_median=current_iv,
                iv_current=current_iv,
                days_of_data=0,
            )

        # Sort by date and take last N days
        df = iv_history.copy()
        if "date" in df.columns:
            df = df.sort_values("date", ascending=False)
        df = df.head(lookback_days)

        iv_col = "iv" if "iv" in df.columns else "implied_volatility"

        return IVStatistics(
            iv_high=df[iv_col].max(),
            iv_low=df[iv_col].min(),
            iv_median=df[iv_col].median(),
            iv_current=current_iv,
            days_of_data=len(df),
        )

    @staticmethod
    def calculate_iv_percentile(
        iv_history: pd.DataFrame,
        current_iv: float,
    ) -> float:
        """Calculate IV Percentile: % of days IV was below current.

        IVP = (Number of days with IV < current) / Total days × 100
        """
        if iv_history is None or iv_history.empty:
            return 50.0

        iv_col = "iv" if "iv" in iv_history.columns else "implied_volatility"
        below_count = (iv_history[iv_col] < current_iv).sum()
        total = len(iv_history)

        if total == 0:
            return 50.0

        return (below_count / total) * 100
