"""Options Momentum Ranking System - Statistical & Analytical Framework.

Implements a rigorous quantitative methodology for ranking options by combining:
1. Momentum Analysis (40%) - Price acceleration, volume flow, OI growth
2. Valuation Assessment (35%) - IV Rank, bid-ask spread tightness
3. Greeks-Based Risk Scoring (25%) - Delta, Gamma, Vega, Theta alignment

WEIGHTS STANDARDIZED (2026-01-23):
- Momentum: 40% (captures price action and activity)
- Value: 35% (entry quality via IV and spread)
- Greeks: 25% (directional alignment and risk)

SCORE CAPPING (2026-01-23):
All component scores are clipped to [0, 100] BEFORE applying weights to prevent:
- Component domination (one score overwhelming others)
- Weight distortion (weights lose intended proportional effect)
- Interpretability loss (scores >100 confuse "perfect" score meaning)

SCORING FORMULAS (all normalized to 0-100):

1. VALUE SCORE (35% of composite):
   - iv_value_score = 100 - iv_rank  (lower IV = better for buyer)
   - spread_penalty = min(spread_% × 2, 50)
   - spread_score = 100 - spread_penalty
   - value_score = clip(0.60 × iv_value_score + 0.40 × spread_score, 0, 100)

2. MOMENTUM SCORE (40% of composite):
   - price_mom_score = clip(2 × r + 50, 0, 100)  where r = 5-day return %
   - vol_oi_score = min(vol/OI / 0.20 × 100, 100)
   - oi_growth_score = clip(growth + 50, 0, 100)  where growth = 5-day OI change %
   - momentum_score = clip(0.50 × price_mom + 0.30 × vol_oi + 0.20 × oi_growth, 0, 100)

3. GREEKS SCORE (25% of composite):
   - delta_score = 100 - 100 × |Δ - 0.55|  (target 0.55 calls, -0.55 puts)
   - gamma_score = min(γ / 0.04 × 100, 100)
   - vega_score = min(vega / 0.30 × 100, 100)
   - theta_penalty = min(|θ%| × 10, 40)  where θ% = θ/mid × 100
   - greeks_pre = 0.50 × delta + 0.35 × gamma + 0.10 × vega
   - greeks_score = clip(greeks_pre - theta_penalty, 0, 100)

4. COMPOSITE RANK (final 0-100 score):
   rank_score = clip(0.40 × momentum + 0.35 × value + 0.25 × greeks, 0, 100)
   Maximum possible: (100 × 0.40) + (100 × 0.35) + (100 × 0.25) = 100
"""

import logging
import os
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class RankingMode(Enum):
    """Ranking optimization modes for different trading phases."""
    
    ENTRY = "entry"      # Optimized for finding undervalued opportunities (Value 40%, Catalyst 35%, Greeks 25%)
    EXIT = "exit"        # Optimized for profit-taking and decay detection (Profit 50%, Deterioration 30%, Time 20%)
    MONITOR = "monitor"  # Balanced view for general screening (Momentum 40%, Value 35%, Greeks 25%)


class StrategyIntent(Enum):
    """Strategy intent for dual-intent scoring (long vs short premium)."""

    LONG_PREMIUM = "long_premium"    # Buyer: favor low IV, backwardation, negative VRP
    SHORT_PREMIUM = "short_premium"  # Seller: favor high IV, contango, positive VRP


class SignalType(Enum):
    """Types of trading signals generated by the ranking system."""

    DISCOUNT = "discount"  # Low IV + momentum = buy opportunity
    MOMENTUM_RUNNER = "runner"  # Hot momentum + high volume
    GREEKS_ALIGNED = "greeks"  # Optimal Greeks for directional trade
    BUY = "buy"  # Composite signal meeting threshold


def _get_iv_jump_threshold() -> float:
    try:
        return float(os.getenv("IV_JUMP_THRESHOLD", "5.0"))
    except Exception:
        return 5.0


@dataclass
class OptionSignals:
    """Trading signals for an option contract."""

    discount: bool = False
    runner: bool = False
    greeks_aligned: bool = False
    buy: bool = False
    signal_types: list = None

    def __post_init__(self):
        self.signal_types = []
        if self.discount:
            self.signal_types.append(SignalType.DISCOUNT)
        if self.runner:
            self.signal_types.append(SignalType.MOMENTUM_RUNNER)
        if self.greeks_aligned:
            self.signal_types.append(SignalType.GREEKS_ALIGNED)
        if self.buy:
            self.signal_types.append(SignalType.BUY)


@dataclass
class IVStatistics:
    """52-week IV statistics for a symbol."""

    iv_high: float
    iv_low: float
    iv_median: float
    iv_current: float
    days_of_data: int = 252
    last_updated: datetime | None = None
    max_age_hours: int = 4

    @property
    def iv_rank(self) -> float:
        """Calculate IV Rank: (current - low) / (high - low) * 100."""
        if self.iv_high == self.iv_low:
            return 50.0
        return ((self.iv_current - self.iv_low) / (self.iv_high - self.iv_low)) * 100

    @property
    def is_cheap(self) -> bool:
        """IV is in bottom 30% of range."""
        return self.iv_rank < 30

    @property
    def is_expensive(self) -> bool:
        """IV is in top 70% of range."""
        return self.iv_rank > 70

    @property
    def is_stale(self) -> bool:
        """Check if IV data is stale (> max_age_hours old)."""
        if self.last_updated is None:
            return True
        age = datetime.now() - self.last_updated
        return age.total_seconds() > (self.max_age_hours * 3600)

    @property
    def staleness_penalty(self) -> float:
        """Calculate confidence penalty for stale data (0-0.2)."""
        if not self.is_stale:
            return 0.0
        if self.last_updated is None:
            return 0.2

        age_hours = (datetime.now() - self.last_updated).total_seconds() / 3600
        # Linear penalty: 0 at max_age, 0.2 at 2x max_age
        excess_hours = age_hours - self.max_age_hours
        return min(0.2, excess_hours / self.max_age_hours * 0.2)

    def is_iv_curve_reasonable(self, iv_by_strike: dict) -> bool:
        """
        Check if IV curve is smooth (no >5% jumps between adjacent strikes).

        Large IV jumps between strikes indicate bad data or illiquidity.

        Args:
            iv_by_strike: Dict mapping strike price to IV

        Returns:
            True if IV curve is smooth, False if suspicious jumps exist
        """
        if len(iv_by_strike) < 2:
            return True

        strikes = sorted(iv_by_strike.keys())
        for i in range(1, len(strikes)):
            current = iv_by_strike[strikes[i]]
            previous = iv_by_strike[strikes[i - 1]]

            iv_current = current.get("iv") if isinstance(current, dict) else current
            iv_prev = previous.get("iv") if isinstance(previous, dict) else previous

            if iv_prev and iv_prev > 0:
                iv_jump_pct = abs(iv_current - iv_prev) / iv_prev * 100
                threshold = _get_iv_jump_threshold()

                if isinstance(current, dict):
                    delta = current.get("delta")
                    volume = current.get("volume")
                    open_interest = current.get("open_interest")
                    if volume is not None and volume < 10:
                        threshold += 2.5
                    if open_interest is not None and open_interest < 50:
                        threshold += 2.5
                    if delta is not None and abs(delta) < 0.2:
                        threshold += 2.5
                    if (
                        volume is not None
                        and volume >= 100
                        and open_interest is not None
                        and open_interest >= 500
                        and delta is not None
                        and abs(delta) >= 0.3
                    ):
                        threshold = 4.0

                if iv_jump_pct > threshold:
                    logger.warning(
                        "Suspicious IV jump: %s=%.2f%% -> %s=%.2f%% (%.1f%%)",
                        strikes[i - 1],
                        iv_prev * 100,
                        strikes[i],
                        iv_current * 100,
                        iv_jump_pct,
                    )
                    return False

        return True

    @property
    def data_quality_score(self) -> float:
        """
        Combined data quality score (0-1).

        Factors:
        - Freshness (0.4 weight)
        - Days of data coverage (0.4 weight)
        - IV curve smoothness (0.2 weight)
        """
        freshness_score = 1.0 - self.staleness_penalty
        # Data coverage score: 1.0 at 252 days, lower for less
        coverage_score = min(1.0, self.days_of_data / 252)
        curve_score = 1.0 if getattr(self, "_iv_curve_ok", True) else 0.7

        return freshness_score * 0.4 + coverage_score * 0.4 + curve_score * 0.2


class OptionsMomentumRanker:
    """
    Advanced options ranking using the Momentum-Value-Greeks framework.

    Composite Score = Momentum(40%) + Value(35%) + Greeks(25%)

    Each component normalized to 0-100 scale for interpretability.
    """

    # Framework weights (MONITOR mode - balanced view)
    MOMENTUM_WEIGHT = 0.40
    VALUE_WEIGHT = 0.35
    GREEKS_WEIGHT = 0.25

    # ENTRY mode weights - optimized for finding undervalued opportunities
    ENTRY_MODE_WEIGHTS = {
        "value": 0.40,      # Is it cheap? (IV percentile, discount, spread)
        "catalyst": 0.35,   # Why will it move? (momentum, volume surge, OI build)
        "greeks": 0.25,     # Is it positioned right? (delta, gamma, vega, theta)
    }
    
    # EXIT mode weights - optimized for profit-taking and decay detection
    EXIT_MODE_WEIGHTS = {
        "profit_protection": 0.50,  # Have I made enough? (P&L, IV expansion, targets)
        "deterioration": 0.30,      # Is momentum fading? (decay, volume drop, OI stall)
        "time_urgency": 0.20,       # Am I running out of time? (DTE, theta burn)
    }

    # Momentum sub-weights
    PRICE_MOMENTUM_WEIGHT = 0.50
    VOLUME_OI_WEIGHT = 0.30
    OI_GROWTH_WEIGHT = 0.20

    # Value sub-weights
    IV_RANK_WEIGHT = 0.60
    SPREAD_WEIGHT = 0.40

    # Greeks sub-weights (for pre-penalty calculation)
    DELTA_WEIGHT = 0.50
    GAMMA_WEIGHT = 0.35
    VEGA_WEIGHT = 0.10
    # Note: Theta is applied as penalty (up to 40 pts), not as a weight

    # Thresholds
    OPTIMAL_DELTA_MIN = 0.40
    OPTIMAL_DELTA_MAX = 0.70
    OPTIMAL_DELTA_TARGET = 0.55
    GAMMA_EXCELLENT_THRESHOLD = 0.04
    GAMMA_GOOD_THRESHOLD = 0.02
    VOLUME_OI_STRONG = 0.20
    VOLUME_OI_NORMAL = 0.10
    SPREAD_CAP = 50.0  # Cap spread % at 50 for scoring

    # Liquidity confidence thresholds (prevents noisy low-volume signals)
    MIN_VOLUME_FOR_MOMENTUM = 10  # Below this, momentum is heavily discounted
    MIN_OI_FOR_MOMENTUM = 50  # Below this, momentum is heavily discounted
    MIN_PRICE_FOR_MOMENTUM = 0.50  # Below $0.50, momentum is unreliable
    LIQUIDITY_RAMP_VOLUME = 100  # Full confidence at this volume
    LIQUIDITY_RAMP_OI = 500  # Full confidence at this OI
    LIQUIDITY_RAMP_PRICE = 5.0  # Full confidence at this price

    # Signal thresholds
    DISCOUNT_IV_RANK_THRESHOLD = 30
    DISCOUNT_MOMENTUM_THRESHOLD = 50
    DISCOUNT_SPREAD_THRESHOLD = 2.0
    RUNNER_MOMENTUM_THRESHOLD = 75
    RUNNER_VOLUME_THRESHOLD = 100
    RUNNER_VOL_OI_THRESHOLD = 0.10
    RUNNER_SPREAD_THRESHOLD = 3.0
    GREEKS_SPREAD_THRESHOLD = 2.0
    BUY_COMPOSITE_THRESHOLD = 65

    # Temporal smoothing (reduces daily ranking churn)
    MOMENTUM_SMOOTHING_WINDOW = 3  # 3-day EMA for momentum stability
    MOMENTUM_MAX_DAILY_CHANGE = 30.0

    # 7-day underlying metrics defaults
    DEFAULT_VOLATILITY_7D = 25.0  # Default 7-day annualized volatility (%)
    TRADING_DAYS_PER_YEAR = 252  # Standard trading days for annualization

    def __init__(
        self,
        momentum_weight: float = 0.40,
        value_weight: float = 0.35,
        greeks_weight: float = 0.25,
    ):
        """Initialize ranker with configurable weights.

        Args:
            momentum_weight: Weight for momentum score (default 0.40)
            value_weight: Weight for value score (default 0.35)
            greeks_weight: Weight for Greeks score (default 0.25)
        """
        # Validate weights sum to 1
        total = momentum_weight + value_weight + greeks_weight
        if abs(total - 1.0) > 0.001:
            logger.warning(f"Weights sum to {total}, normalizing")
            momentum_weight /= total
            value_weight /= total
            greeks_weight /= total

        self.MOMENTUM_WEIGHT = momentum_weight
        self.VALUE_WEIGHT = value_weight
        self.GREEKS_WEIGHT = greeks_weight

        logger.info(
            f"OptionsMomentumRanker initialized: "
            f"Momentum={momentum_weight:.0%}, Value={value_weight:.0%}, Greeks={greeks_weight:.0%}"
        )

    def rank_options(
        self,
        options_df: pd.DataFrame,
        mode: RankingMode = RankingMode.MONITOR,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
        underlying_trend: str = "neutral",
        previous_rankings: Optional[pd.DataFrame] = None,
        underlying_metrics: Optional[dict] = None,
        entry_data: Optional[dict] = None,
        strategy_intent: Optional[StrategyIntent] = None,
        term_structure_features: Optional[dict] = None,
        earnings_jump_features: Optional[dict] = None,
        menthorq_features: Optional[dict] = None,
    ) -> pd.DataFrame:
        """
        Rank options with mode-specific optimization.

        Args:
            options_df: Current options chain with columns:
                - strike, side (call/put), expiration
                - bid, ask, last, mark
                - volume, openInterest (or open_interest)
                - delta, gamma, theta, vega, rho
                - impliedVolatility (or implied_vol, iv)
            mode: Ranking mode (ENTRY, EXIT, or MONITOR)
            iv_stats: 52-week IV statistics for the underlying
            options_history: Historical options data (5+ days) for momentum
            underlying_trend: bullish/neutral/bearish
            previous_rankings: Previous day's rankings for temporal smoothing
            underlying_metrics: 7-day underlying metrics (ret_7d, vol_7d, etc.)
            entry_data: Entry data for EXIT mode (entry_price, entry_iv, price_target)
            strategy_intent: long_premium (buyer) vs short_premium (seller) for IV/VRP/term-structure scoring
            term_structure_features: forward_vol, term_structure_regime (contango/backwardation), low_confidence
            earnings_jump_features: event_1sigma_move, isolated or heuristic
            menthorq_features: GEX/DEX buckets, VRP, skew_proxy

        Returns:
            DataFrame with mode-specific ranking columns
        """
        if options_df.empty:
            logger.warning("No options data to rank")
            return options_df

        df = self._normalize_columns(options_df.copy())
        df["ranking_mode"] = mode.value

        # Resolve strategy intent (default from env or long_premium)
        intent = strategy_intent
        if intent is None:
            import os
            env_val = (os.getenv("RANKER_STRATEGY_INTENT") or "long_premium").lower()
            intent = StrategyIntent.SHORT_PREMIUM if env_val == "short_premium" else StrategyIntent.LONG_PREMIUM
        df["_strategy_intent"] = intent.value

        # Mode-specific ranking calculation
        if mode == RankingMode.ENTRY:
            # ENTRY MODE: Value 40%, Catalyst 35%, Greeks 25%
            logger.info("Calculating ENTRY ranking (Value 40%, Catalyst 35%, Greeks 25%)")
            df = self._rank_for_entry(df, iv_stats, options_history)
            # Also calculate monitor rank for comparison
            df = self._calculate_value_scores(df, iv_stats, strategy_intent=intent, menthorq_features=menthorq_features)
            df = self._calculate_momentum_scores(df, options_history)
            df = self._calculate_greeks_scores(df, underlying_trend)
            df["composite_rank"] = (
                df["momentum_score"] * self.MOMENTUM_WEIGHT +
                df["value_score"] * self.VALUE_WEIGHT +
                df["greeks_score"] * self.GREEKS_WEIGHT
            ).clip(0, 100)
            
        elif mode == RankingMode.EXIT:
            # EXIT MODE: Profit 50%, Deterioration 30%, Time 20%
            logger.info("Calculating EXIT ranking (Profit 50%, Deterioration 30%, Time 20%)")
            df = self._rank_for_exit(df, options_history, entry_data)
            # Also calculate monitor rank for comparison
            df = self._calculate_value_scores(df, iv_stats, strategy_intent=intent, menthorq_features=menthorq_features)
            df = self._calculate_momentum_scores(df, options_history)
            df = self._calculate_greeks_scores(df, underlying_trend)
            df["composite_rank"] = (
                df["momentum_score"] * self.MOMENTUM_WEIGHT +
                df["value_score"] * self.VALUE_WEIGHT +
                df["greeks_score"] * self.GREEKS_WEIGHT
            ).clip(0, 100)
            
        else:  # MONITOR MODE (default)
            # MONITOR MODE: Momentum 40%, Value 35%, Greeks 25%
            logger.info("Calculating MONITOR ranking (Momentum 40%, Value 35%, Greeks 25%)")
            df = self._calculate_value_scores(df, iv_stats, strategy_intent=intent, menthorq_features=menthorq_features)
            df = self._calculate_momentum_scores(df, options_history)
            df = self._calculate_greeks_scores(df, underlying_trend)
            
            # Integrate 7-day underlying metrics into momentum score
            if underlying_metrics is not None:
                vol_7d = underlying_metrics.get("vol_7d", self.DEFAULT_VOLATILITY_7D) or self.DEFAULT_VOLATILITY_7D
                vol_regime = "low" if vol_7d < 20.0 else "normal" if vol_7d < 40.0 else "high"
                df = self._integrate_underlying_metrics(df, underlying_metrics, underlying_trend, vol_regime)
            
            # Apply IV staleness penalty if applicable
            if iv_stats and iv_stats.is_stale:
                staleness_penalty = iv_stats.staleness_penalty
                logger.warning(f"IV data is stale, applying {staleness_penalty:.1%} penalty")
                df["value_score"] = (df["value_score"] * (1 - staleness_penalty)).clip(0, 100)
            
            df["composite_rank"] = (
                df["momentum_score"] * self.MOMENTUM_WEIGHT +
                df["value_score"] * self.VALUE_WEIGHT +
                df["greeks_score"] * self.GREEKS_WEIGHT
            ).clip(0, 100)

        df["composite_rank"] = df["composite_rank"].clip(0, 100)

        # Apply temporal smoothing if previous rankings available
        if previous_rankings is not None and not previous_rankings.empty:
            df = self._apply_temporal_smoothing(df, previous_rankings)

        df["ranking_stability_score"] = self._calculate_ranking_stability_score(
            df, previous_rankings
        )

        # Generate signals
        df = self._generate_signals(df)

        # Sort by composite rank
        df = df.sort_values("composite_rank", ascending=False).reset_index(drop=True)

        # Log statistics
        self._log_ranking_stats(df)

        return df

    def _apply_temporal_smoothing(
        self,
        current: pd.DataFrame,
        previous: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        Apply EMA smoothing to momentum scores to reduce churn.

        Formula: smoothed = alpha * current + (1 - alpha) * previous
        Where alpha = 2 / (window + 1) for EMA

        Only smooths momentum_score and composite_rank, NOT raw Greeks.
        """
        alpha = 2 / (self.MOMENTUM_SMOOTHING_WINDOW + 1)  # ~0.5 for 3-day

        # Match contracts between current and previous
        for idx, row in current.iterrows():
            # FIX: Include expiration in fallback key to prevent cross-expiry collisions
            expiration = row.get("expiration", "")
            contract_id = row.get(
                "contract_symbol", f"{row['strike']}_{row['side']}_{expiration}"
            )

            # Find previous momentum score
            if "contract_symbol" in previous.columns:
                prev_match = previous[previous["contract_symbol"] == contract_id]
            else:
                # FIX: Also match on expiration to avoid blending unrelated contracts
                if "expiration" in previous.columns and expiration:
                    prev_match = previous[
                        (previous["strike"] == row["strike"])
                        & (previous["side"] == row["side"])
                        & (previous["expiration"] == expiration)
                    ]
                else:
                    prev_match = previous[
                        (previous["strike"] == row["strike"]) & (previous["side"] == row["side"])
                    ]

            if len(prev_match) > 0:
                prev_momentum = prev_match.iloc[0].get("momentum_score", row["momentum_score"])

                # EMA smoothing on momentum score
                smoothed = alpha * row["momentum_score"] + (1 - alpha) * prev_momentum

                if abs(smoothed - prev_momentum) > self.MOMENTUM_MAX_DAILY_CHANGE:
                    smoothed = prev_momentum + (
                        np.sign(smoothed - prev_momentum) * self.MOMENTUM_MAX_DAILY_CHANGE
                    )
                current.at[idx, "momentum_score"] = np.clip(smoothed, 0, 100)

                # Recalculate composite with smoothed momentum
                mode = str(row.get("ranking_mode", "entry") or "entry").lower()
                if mode == "exit":
                    # For EXIT mode: profit + deterioration + time_urgency
                    # Note: temporal smoothing applies to momentum which affects deterioration
                    weights = self.EXIT_MODE_WEIGHTS
                    # EXIT uses different component names
                    composite = (
                        row.get("profit_protection_score", 0) * weights["profit_protection"] +
                        row.get("deterioration_score", 0) * weights["deterioration"] +
                        row.get("time_urgency_score", 0) * weights["time_urgency"]
                    )
                elif mode == "entry":
                    # For ENTRY mode: value + catalyst + greeks
                    weights = self.ENTRY_MODE_WEIGHTS
                    composite = (
                        row.get("entry_value_score", 0) * weights["value"] +
                        row.get("catalyst_score", 0) * weights["catalyst"] +
                        row.get("greeks_score", 0) * weights["greeks"]
                    )
                else:
                    # MONITOR mode: momentum + value + greeks
                    composite = (
                        smoothed * self.MOMENTUM_WEIGHT
                        + row["value_score"] * self.VALUE_WEIGHT
                        + row["greeks_score"] * self.GREEKS_WEIGHT
                    )

                if mode == "entry":
                    entry_diff = row.get("entry_difficulty_score", 50.0)
                    rel_val = row.get("relative_value_score", 50.0)
                    if composite > self.BUY_COMPOSITE_THRESHOLD:
                        composite = composite * 0.90 + entry_diff * 0.10
                    composite = composite * 0.90 + rel_val * 0.10
                elif mode == "exit":
                    theta_bonus = self._calculate_theta_bonus(current.loc[[idx]]).iloc[0]
                    composite = composite * 0.90 + theta_bonus * 0.10

                current.at[idx, "composite_rank"] = float(max(0.0, min(100.0, composite)))

        return current

    def _calculate_relative_value_score(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        if options_history is None or options_history.empty:
            return pd.Series(50.0, index=df.index)

        hist = options_history.copy()
        if "contract_symbol" not in hist.columns:
            return pd.Series(50.0, index=df.index)

        if "iv" not in hist.columns:
            if "implied_vol" in hist.columns:
                hist["iv"] = hist["implied_vol"]
            elif "impliedVolatility" in hist.columns:
                hist["iv"] = hist["impliedVolatility"]

        if "iv" not in hist.columns:
            return pd.Series(50.0, index=df.index)

        if "bid" not in hist.columns or "ask" not in hist.columns:
            hist["spread_pct"] = np.nan
        else:
            mid = (hist["bid"] + hist["ask"]) / 2
            spread = hist["ask"] - hist["bid"]
            hist["spread_pct"] = np.where(mid > 0, (spread / mid) * 100, np.nan)

        iv_avg = hist.groupby("contract_symbol")["iv"].mean()
        spread_avg = hist.groupby("contract_symbol")["spread_pct"].mean()

        contract_ids = df.get("contract_symbol", pd.Series("", index=df.index))
        hist_iv_avg = contract_ids.map(iv_avg)
        hist_spread_avg = contract_ids.map(spread_avg)

        current_iv = df.get("iv", pd.Series(np.nan, index=df.index))
        current_spread = df.get("spread_pct", pd.Series(np.nan, index=df.index))

        iv_discount = np.where(
            (hist_iv_avg > 0) & np.isfinite(hist_iv_avg) & np.isfinite(current_iv),
            1.0 - (current_iv / hist_iv_avg),
            0.0,
        )
        spread_discount = np.where(
            (hist_spread_avg > 0) & np.isfinite(hist_spread_avg) & np.isfinite(current_spread),
            1.0 - (current_spread / hist_spread_avg),
            0.0,
        )

        score = (0.70 * iv_discount + 0.30 * spread_discount) * 100
        return pd.Series(score, index=df.index).clip(0, 100)

    def _calculate_entry_difficulty_score(self, df: pd.DataFrame) -> pd.Series:
        bid = df.get("bid", pd.Series(0.0, index=df.index))
        ask = df.get("ask", pd.Series(0.0, index=df.index))
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(0, index=df.index))

        mid = (bid + ask) / 2
        spread = ask - bid
        spread_pct = np.where(mid > 0, (spread / mid) * 100, 100.0)
        vol_oi_ratio = np.where(oi > 0, volume / oi, 0.0)

        spread_penalty = np.clip(spread_pct / 2.0, 0.0, 1.0)
        vol_bonus = np.minimum(vol_oi_ratio, 1.0) * 0.5

        score = (1.0 - spread_penalty + vol_bonus) * 100
        return pd.Series(score, index=df.index).clip(0, 100)

    def _calculate_theta_bonus(self, df: pd.DataFrame) -> pd.Series:
        theta = df.get("theta", pd.Series(0.0, index=df.index)).abs()
        mid_price = df.get("mid", df.get("mark", pd.Series(1.0, index=df.index)))
        theta_pct = np.where(mid_price > 0, (theta / mid_price) * 100, 100.0)
        bonus = 100 - np.minimum(theta_pct * 10, 100)
        return pd.Series(bonus, index=df.index).clip(0, 100)

    def _calculate_ranking_stability_score(
        self,
        current: pd.DataFrame,
        previous: Optional[pd.DataFrame],
    ) -> pd.Series:
        if previous is None or previous.empty:
            return pd.Series(np.nan, index=current.index)

        scores = pd.Series(np.nan, index=current.index)

        for idx, row in current.iterrows():
            # FIX: Include expiration in fallback key for consistent matching
            expiration = row.get("expiration", "")
            contract_id = row.get(
                "contract_symbol", f"{row['strike']}_{row['side']}_{expiration}"
            )

            if "contract_symbol" in previous.columns:
                prev_match = previous[previous["contract_symbol"] == contract_id]
            else:
                # FIX: Also match on expiration to avoid cross-expiry collisions
                if "expiration" in previous.columns and expiration:
                    prev_match = previous[
                        (previous["strike"] == row["strike"])
                        & (previous["side"] == row["side"])
                        & (previous["expiration"] == expiration)
                    ]
                else:
                    prev_match = previous[
                        (previous["strike"] == row["strike"]) & (previous["side"] == row["side"])
                    ]

            if len(prev_match) > 0:
                prev_rank = prev_match.iloc[0].get("composite_rank", np.nan)
                curr_rank = row.get("composite_rank", np.nan)
                if np.isfinite(prev_rank) and np.isfinite(curr_rank):
                    delta = abs(curr_rank - prev_rank)
                    scores[idx] = max(0.0, min(100.0, 100.0 - (delta * 2.0)))

        return scores

    def _normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalize column names for consistent processing."""
        rename_map = {
            "impliedVolatility": "iv",
            "implied_vol": "iv",
            "implied_volatility": "iv",
            "openInterest": "open_interest",
            "option_type": "side",
        }
        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

        # Ensure required columns exist
        if "mid" not in df.columns and "bid" in df.columns and "ask" in df.columns:
            df["mid"] = (df["bid"] + df["ask"]) / 2
        if "mark" not in df.columns:
            df["mark"] = df.get("mid", df.get("last", 0))

        return df

    # =========================================================================
    # VALUE SCORING (35% of total)
    # =========================================================================

    def _calculate_value_scores(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        strategy_intent: Optional[StrategyIntent] = None,
        menthorq_features: Optional[dict] = None,
    ) -> pd.DataFrame:
        """Calculate value score from IV Rank and bid-ask spread.

        Dual-intent mapping:
        - long_premium (buyer): iv_value_score = 100 - iv_rank; favor backwardation, negative VRP
        - short_premium (seller): iv_value_score = iv_rank; favor contango, positive VRP

        For spread:
            spread_penalty = min(spread_% × 2, 50)
            spread_score = 100 - spread_penalty

        Value Score = iv_value_score × 0.60 + spread_score × 0.40 (+ VRP/term-structure adjustment)
        """
        # IV Rank component (60% of value)
        df["iv_rank"] = self._calculate_iv_rank(df, iv_stats)
        intent = strategy_intent or StrategyIntent.LONG_PREMIUM
        if intent == StrategyIntent.SHORT_PREMIUM:
            df["iv_rank_score"] = df["iv_rank"]  # High IV = good for seller
        else:
            df["iv_rank_score"] = 100 - df["iv_rank"]  # Lower IV = good for buyer

        # Bid-Ask Spread component (40% of value)
        df["spread_pct"] = self._calculate_spread_pct(df)
        df["spread_score"] = self._calculate_spread_score(df["spread_pct"])

        # Combined Value Score (0-100)
        df["value_score"] = (
            df["iv_rank_score"] * self.IV_RANK_WEIGHT + df["spread_score"] * self.SPREAD_WEIGHT
        ).clip(0, 100)

        # VRP/term-structure adjustment (context, small weight)
        if menthorq_features and "vrp" in menthorq_features:
            vrp = menthorq_features["vrp"]
            vrp_score = 50 + vrp * 50 if np.isfinite(vrp) else 50.0
            vrp_score = np.clip(vrp_score, 0, 100)
            if intent == StrategyIntent.SHORT_PREMIUM:
                df["value_score"] = (df["value_score"] * 0.90 + vrp_score * 0.10).clip(0, 100)
            else:
                df["value_score"] = (df["value_score"] * 0.90 + (100 - vrp_score) * 0.10).clip(0, 100)

        return df

    def _calculate_iv_rank(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
    ) -> pd.Series:
        """Calculate IV Rank: (IV_current - IV_52low) / (IV_52high - IV_52low) × 100.

        If no IV stats provided, estimate from current chain.
        Also adds iv_rank_source column to track the source of IV rank calculation.
        """
        if "iv" not in df.columns:
            logger.warning("No IV column found, defaulting to 50")
            df["iv_rank_source"] = "default"
            return pd.Series(50.0, index=df.index)

        if iv_stats:
            # Use provided 52-week statistics (preferred method)
            iv_range = iv_stats.iv_high - iv_stats.iv_low
            if iv_range > 0:
                iv_rank = ((df["iv"] - iv_stats.iv_low) / iv_range) * 100
                df["iv_rank_source"] = "rpc"  # From database RPC with historical stats
            else:
                iv_rank = pd.Series(50.0, index=df.index)
                df["iv_rank_source"] = "rpc_no_range"
        else:
            # FIX: Track that we're using chain estimate (less reliable)
            # Estimate from current chain (ATM IV as proxy)
            iv_min = df["iv"].min()
            iv_max = df["iv"].max()
            iv_range = iv_max - iv_min

            if iv_range > 0:
                iv_rank = ((df["iv"] - iv_min) / iv_range) * 100
                df["iv_rank_source"] = "chain_estimate"  # Estimated from current chain
            else:
                iv_rank = pd.Series(50.0, index=df.index)
                df["iv_rank_source"] = "chain_no_range"

            logger.warning(
                f"Using chain-estimated IV Rank (less reliable): "
                f"min={iv_min:.2%}, max={iv_max:.2%}"
            )

        return iv_rank.clip(0, 100)

    def _calculate_spread_pct(self, df: pd.DataFrame) -> pd.Series:
        """Calculate bid-ask spread as percentage of mid price.

        spread_% = (ask - bid) / mid × 100
        """
        if "bid" not in df.columns or "ask" not in df.columns:
            return pd.Series(5.0, index=df.index)  # Default 5% spread

        mid = (df["bid"] + df["ask"]) / 2
        spread = df["ask"] - df["bid"]

        # Avoid division by zero
        spread_pct = np.where(mid > 0, (spread / mid) * 100, 100)

        return pd.Series(spread_pct, index=df.index)

    def _calculate_spread_score(self, spread_pct: pd.Series) -> pd.Series:
        """Calculate spread score from spread percentage.

        UPDATED 2026-01-23: Exponential penalty curve to more heavily penalize wide spreads
        
        Formula (exponential curve):
            if spread ≤ 2%: penalty = spread × 2
            elif spread ≤ 5%: penalty = 4 + (spread - 2) × 4
            elif spread ≤ 10%: penalty = 16 + (spread - 5) × 5
            else: penalty = min(41 + (spread - 10) × 2, 50)
            
            spread_score = 100 - penalty

        Examples:
            - 0% spread → penalty 0 → score 100
            - 1% spread → penalty 2 → score 98  (no change)
            - 3% spread → penalty 8 → score 92  (was 94 with linear)
            - 5% spread → penalty 16 → score 84  (was 90 with linear)
            - 10% spread → penalty 41 → score 59  (was 80 with linear)
            - 15%+ spread → penalty 50 → score 50 (capped)
        """
        def calculate_penalty(spread):
            if spread <= 2.0:
                return spread * 2
            elif spread <= 5.0:
                return 4.0 + (spread - 2.0) * 4.0
            elif spread <= 10.0:
                return 16.0 + (spread - 5.0) * 5.0
            else:
                return min(41.0 + (spread - 10.0) * 2.0, 50.0)
        
        spread_penalty = spread_pct.apply(calculate_penalty)
        return 100 - spread_penalty

    # =========================================================================
    # ENHANCED VALUE SCORING FOR ENTRY MODE
    # =========================================================================
    
    def _calculate_iv_percentile(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
    ) -> pd.Series:
        """Calculate IV Percentile: what % of days had higher IV?
        
        More robust than IV Rank as it's based on distribution, not just high/low.
        
        Returns:
            Percentile 0-100, where low percentile = cheap IV
        """
        if "iv" not in df.columns:
            logger.warning("No IV column found for percentile calculation")
            return pd.Series(50.0, index=df.index)
        
        if iv_stats and hasattr(iv_stats, 'iv_percentile'):
            # If we have pre-calculated percentile from stats
            return pd.Series(iv_stats.iv_percentile, index=df.index)
        
        # Fallback: approximate from IV rank
        # IV Rank is close to percentile for uniform distributions
        iv_rank = self._calculate_iv_rank(df, iv_stats)
        return iv_rank.clip(0, 100)
    
    def _calculate_iv_historical_discount(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate how discounted current IV is vs contract's own history.
        
        Formula:
            discount_pct = (avg_iv_20d - current_iv) / avg_iv_20d × 100
            score = max(0, discount_pct × 5)  # 20% discount = 100 score
        
        Returns:
            Score 0-100, where 100 = deeply discounted IV
        """
        if options_history is None or options_history.empty:
            return pd.Series(50.0, index=df.index)
        
        if "contract_symbol" not in df.columns:
            return pd.Series(50.0, index=df.index)
        
        # Get average IV per contract from history
        # Handle both 'iv' and 'impliedVolatility' column names
        iv_col = 'iv' if 'iv' in options_history.columns else 'impliedVolatility'
        if iv_col not in options_history.columns:
            logger.warning("No IV column found in options_history, returning default discount score")
            return pd.Series(50.0, index=df.index)
        
        hist_iv_avg = options_history.groupby('contract_symbol')[iv_col].mean()
        
        # Map to current contracts
        current_iv = df.get('iv', pd.Series(np.nan, index=df.index))
        hist_avg = df['contract_symbol'].map(hist_iv_avg)
        
        # Calculate discount percentage
        discount_pct = np.where(
            (hist_avg > 0) & np.isfinite(hist_avg) & np.isfinite(current_iv),
            ((hist_avg - current_iv) / hist_avg) * 100,
            0.0
        )
        
        # Score: 20% discount = 100 score, 10% = 50, 0% = 0
        score = np.clip(discount_pct * 5, 0, 100)
        
        return pd.Series(score, index=df.index)

    # =========================================================================
    # MOMENTUM SCORING (40% of total)
    # =========================================================================

    def _calculate_liquidity_confidence(self, df: pd.DataFrame) -> pd.Series:
        """Calculate liquidity confidence multiplier (0.1 to 1.0).

        Low-volume/low-price options get dampened momentum scores because
        percentage changes are noisy and unreliable.

        Formula uses geometric mean of three confidence factors:
        - Volume confidence: ramps from 0.1 at vol=0 to 1.0 at vol=100
        - OI confidence: ramps from 0.1 at OI=0 to 1.0 at OI=500
        - Price confidence: ramps from 0.1 at $0 to 1.0 at $5

        Examples:
        - Vol=5, OI=20, Price=$0.50 → ~0.25 confidence (heavily dampened)
        - Vol=50, OI=200, Price=$2 → ~0.65 confidence (moderately dampened)
        - Vol=100+, OI=500+, Price=$5+ → 1.0 confidence (full weight)
        """
        # Get volume, OI, and price
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(0, index=df.index))
        price = df.get("mid", df.get("mark", pd.Series(1, index=df.index)))

        # Calculate individual confidence factors (0.1 to 1.0)
        min_conf = 0.1  # Floor to avoid zeroing out completely

        # Volume confidence: linear ramp from min to 1.0
        vol_conf = np.clip(
            min_conf + (1 - min_conf) * volume / self.LIQUIDITY_RAMP_VOLUME, min_conf, 1.0
        )

        # OI confidence
        oi_conf = np.clip(min_conf + (1 - min_conf) * oi / self.LIQUIDITY_RAMP_OI, min_conf, 1.0)

        # Price confidence
        price_conf = np.clip(
            min_conf + (1 - min_conf) * price / self.LIQUIDITY_RAMP_PRICE, min_conf, 1.0
        )

        # Geometric mean of all three factors
        confidence = (vol_conf * oi_conf * price_conf) ** (1 / 3)

        return pd.Series(confidence, index=df.index)

    def _calculate_momentum_scores(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """Calculate momentum score from price change, Vol/OI, and OI growth.

        Momentum Score = (Price_Mom × 0.50 + Vol/OI × 0.30 + OI_Growth × 0.20)
                         × Liquidity_Confidence

        The liquidity confidence multiplier dampens scores for low-volume,
        low-OI, or low-price options where percentage changes are noisy.
        """
        # Calculate liquidity confidence multiplier first
        df["liquidity_confidence"] = self._calculate_liquidity_confidence(df)

        # Price Momentum (50% of momentum)
        df["price_momentum"] = self._calculate_price_momentum(df, options_history)
        df["price_momentum_score"] = self._normalize_momentum(df["price_momentum"])

        # Volume/OI Ratio (30% of momentum)
        df["vol_oi_ratio"] = self._calculate_vol_oi_ratio(df)
        df["vol_oi_score"] = self._score_vol_oi(df["vol_oi_ratio"])

        # OI Growth (20% of momentum)
        df["oi_growth"] = self._calculate_oi_growth(df, options_history)
        df["oi_growth_score"] = self._normalize_oi_growth(df["oi_growth"])

        # Combined Momentum Score with liquidity dampening
        raw_momentum = (
            df["price_momentum_score"] * self.PRICE_MOMENTUM_WEIGHT
            + df["vol_oi_score"] * self.VOLUME_OI_WEIGHT
            + df["oi_growth_score"] * self.OI_GROWTH_WEIGHT
        )

        # Apply liquidity confidence - low liquidity dampens toward neutral
        # Formula: score = 50 + (raw_score - 50) * confidence
        # Pulls extreme scores toward 50 for illiquid options
        liq_conf = df["liquidity_confidence"]
        df["momentum_score"] = (50 + (raw_momentum - 50) * liq_conf).clip(0, 100)

        return df

    def _calculate_underlying_7d_score(
        self,
        underlying_metrics: Optional[dict] = None,
        trend_regime: str = "neutral",
        vol_regime: str = "normal",
    ) -> float:
        """
        Calculate score from underlying 7-day metrics.

        Integrates return, volatility, drawdown, and gap count from
        the underlying asset's 7-day price history.

        Args:
            underlying_metrics: Dict with ret_7d, vol_7d, drawdown_7d, gap_count
            trend_regime: Current trend regime (bullish, bearish, neutral)
            vol_regime: Current volatility regime (low, normal, high)

        Returns:
            Score from 0-100 based on underlying performance
        """
        if underlying_metrics is None:
            return 50.0  # Neutral score if no metrics available

        # Extract metrics with defaults
        ret_7d = underlying_metrics.get("ret_7d", 0.0) or 0.0
        vol_7d = underlying_metrics.get("vol_7d", self.DEFAULT_VOLATILITY_7D) or self.DEFAULT_VOLATILITY_7D
        drawdown_7d = underlying_metrics.get("drawdown_7d", 0.0) or 0.0
        gap_count = underlying_metrics.get("gap_count", 0) or 0

        # Score return component (0-100)
        # >10% return = 100, 0% = 50, <-10% = 0
        if ret_7d >= 10.0:
            return_score = 100.0
        elif ret_7d >= 5.0:
            return_score = 75.0 + (ret_7d - 5.0) * 5.0
        elif ret_7d >= 0.0:
            return_score = 50.0 + ret_7d * 5.0
        elif ret_7d >= -5.0:
            return_score = 50.0 + ret_7d * 5.0
        elif ret_7d >= -10.0:
            return_score = 25.0 + (ret_7d + 5.0) * 5.0
        else:
            return_score = max(0.0, 25.0 + (ret_7d + 10.0) * 2.5)

        # Score volatility component (0-100)
        # Low volatility during uptrend = good, high volatility = risky
        if vol_7d < 15.0:
            vol_score = 80.0  # Low volatility = stable
        elif vol_7d < 30.0:
            vol_score = 60.0  # Normal volatility
        elif vol_7d < 50.0:
            vol_score = 40.0  # Elevated volatility
        else:
            vol_score = 20.0  # High volatility = risky

        # Score drawdown component (0-100)
        # Lower drawdown = better
        if drawdown_7d < 3.0:
            drawdown_score = 90.0  # Minor drawdown
        elif drawdown_7d < 7.0:
            drawdown_score = 70.0  # Moderate drawdown
        elif drawdown_7d < 15.0:
            drawdown_score = 40.0  # Significant drawdown
        else:
            drawdown_score = 20.0  # Severe drawdown

        # Score gap count component (0-100)
        # Fewer gaps = more stable price action
        if gap_count <= 1:
            gap_score = 90.0  # Very stable
        elif gap_count <= 3:
            gap_score = 60.0  # Some discontinuity
        else:
            gap_score = 30.0  # High discontinuity

        # Combine scores with weights from config
        # Default weights: return=0.40, volatility=0.30, drawdown=0.20, gaps=0.10
        combined_score = (
            return_score * 0.40
            + vol_score * 0.30
            + drawdown_score * 0.20
            + gap_score * 0.10
        )

        # Apply regime multipliers if in specific regime
        regime_key = f"{trend_regime}_{vol_regime}".lower()
        regime_multipliers = {
            "bullish_low": 1.1,
            "bullish_normal": 1.0,
            "bullish_high": 0.95,
            "bearish_low": 0.9,
            "bearish_normal": 0.85,
            "bearish_high": 0.8,
            "neutral_low": 1.0,
            "neutral_normal": 1.0,
            "neutral_high": 0.9,
        }

        multiplier = regime_multipliers.get(regime_key, 1.0)
        final_score = combined_score * multiplier

        return max(0.0, min(100.0, final_score))

    def _integrate_underlying_metrics(
        self,
        df: pd.DataFrame,
        underlying_metrics: Optional[dict] = None,
        trend_regime: str = "neutral",
        vol_regime: str = "normal",
    ) -> pd.DataFrame:
        """
        Integrate underlying 7-day metrics into the momentum score.

        Blends the underlying performance score with the existing
        option-specific momentum score.

        Args:
            df: DataFrame with momentum_score column
            underlying_metrics: Dict with ret_7d, vol_7d, drawdown_7d, gap_count
            trend_regime: Current trend regime
            vol_regime: Current volatility regime

        Returns:
            DataFrame with updated momentum_score incorporating underlying metrics
        """
        if underlying_metrics is None:
            # No underlying metrics available, return unchanged
            df["underlying_7d_score"] = 50.0
            return df

        # Calculate underlying 7-day score
        underlying_score = self._calculate_underlying_7d_score(
            underlying_metrics, trend_regime, vol_regime
        )
        df["underlying_7d_score"] = underlying_score

        # Store individual metric values for transparency
        df["underlying_ret_7d"] = underlying_metrics.get("ret_7d", 0.0)
        df["underlying_vol_7d"] = underlying_metrics.get("vol_7d", 0.0)
        df["underlying_drawdown_7d"] = underlying_metrics.get("drawdown_7d", 0.0)
        df["underlying_gap_count"] = underlying_metrics.get("gap_count", 0)

        # Blend underlying score into momentum
        # Default blend: 80% option momentum + 20% underlying momentum
        underlying_weight = 0.20
        df["momentum_score"] = (
            df["momentum_score"] * (1 - underlying_weight)
            + underlying_score * underlying_weight
        ).clip(0, 100)

        return df

    def _calculate_price_momentum(
        self,
        df: pd.DataFrame,
        history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate 5-day price momentum.

        Returns percentage change from 5 days ago.
        """
        if history is None or history.empty:
            # Without history, use volume as momentum proxy
            if "volume" in df.columns:
                vol_normalized = df["volume"] / (df["volume"].max() + 1)
                return vol_normalized * 20 - 10  # Range -10% to +10%
            return pd.Series(0.0, index=df.index)

        # Match current options to history by contract_symbol or strike+side+expiration
        momentum = pd.Series(0.0, index=df.index)

        for idx, row in df.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            # Find matching historical data
            if "contract_symbol" in history.columns:
                hist_match = history[history["contract_symbol"] == contract_id]
            else:
                hist_match = history[
                    (history["strike"] == row["strike"]) & (history["side"] == row["side"])
                ]

            if len(hist_match) >= 2:
                # Calculate price change
                old_price = hist_match.iloc[0].get("last", hist_match.iloc[0].get("mark", 0))
                new_price = row.get("last", row.get("mark", 0))

                if old_price > 0:
                    momentum[idx] = ((new_price - old_price) / old_price) * 100

        return momentum

    def _normalize_momentum(self, momentum: pd.Series) -> pd.Series:
        """Normalize price momentum to 0-100 score.

        Formula: min(max(Momentum × 2 + 50, 0), 100)
        - 100% change → Score ~95-100
        - 25% change → Score ~70-75
        - 0% change → Score 50
        - -25% change → Score ~25-30
        """
        return (momentum * 2 + 50).clip(0, 100)

    def _calculate_vol_oi_ratio(self, df: pd.DataFrame) -> pd.Series:
        """Calculate Volume / Open Interest ratio."""
        volume = df.get("volume", pd.Series(0, index=df.index))
        oi = df.get("open_interest", pd.Series(1, index=df.index))

        # Avoid division by zero
        ratio = np.where(oi > 0, volume / oi, 0)

        return pd.Series(ratio, index=df.index)

    def _score_vol_oi(self, ratio: pd.Series) -> pd.Series:
        """Score Volume/OI ratio.

        Formula: min(ratio / 0.20 × 100, 100)
        - Ratio > 0.20: Score 100 (very active)
        - Ratio 0.10: Score 50 (normal)
        - Ratio 0.05: Score 25 (low)
        """
        return (ratio / self.VOLUME_OI_STRONG * 100).clip(0, 100)

    def _calculate_oi_growth(
        self,
        df: pd.DataFrame,
        history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate Open Interest growth over 5 days."""
        if history is None or history.empty:
            return pd.Series(0.0, index=df.index)

        growth = pd.Series(0.0, index=df.index)

        for idx, row in df.iterrows():
            contract_id = row.get("contract_symbol", f"{row['strike']}_{row['side']}")

            if "contract_symbol" in history.columns:
                hist_match = history[history["contract_symbol"] == contract_id]
            else:
                hist_match = history[
                    (history["strike"] == row["strike"]) & (history["side"] == row["side"])
                ]

            if len(hist_match) >= 1:
                old_oi = hist_match.iloc[0].get("open_interest", 0)
                new_oi = row.get("open_interest", 0)

                if old_oi > 0:
                    growth[idx] = ((new_oi - old_oi) / old_oi) * 100

        return growth

    def _normalize_oi_growth(self, growth: pd.Series) -> pd.Series:
        """Normalize OI growth to 0-100 score.

        Formula: min(max(growth + 50, 0), 100)
        - 50% growth → Score 100
        - 0% growth → Score 50
        - -50% growth → Score 0
        """
        return (growth + 50).clip(0, 100)

    # =========================================================================
    # CATALYST SCORING FOR ENTRY MODE
    # =========================================================================
    
    def _calculate_volume_surge(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate volume surge as ratio vs historical average.
        
        Formula:
            vol_ratio = current_vol / avg_vol_20d
            score: 1× = 50, 2× = 75, 4× = 100, >4× = 100
        
        Returns:
            Score 0-100, where 100 = 4× average volume (strong surge)
        """
        if options_history is None or options_history.empty:
            # No history, use vol/OI as proxy
            vol_oi = self._calculate_vol_oi_ratio(df)
            return self._score_vol_oi(vol_oi)
        
        if "contract_symbol" not in df.columns:
            return pd.Series(50.0, index=df.index)
        
        # Get 20-day average volume per contract
        hist_vol_avg = options_history.groupby('contract_symbol')['volume'].mean()
        
        current_vol = df.get('volume', pd.Series(0, index=df.index))
        avg_vol = df['contract_symbol'].map(hist_vol_avg).fillna(10)  # Assume 10 if no history
        
        # Volume surge ratio
        vol_ratio = current_vol / avg_vol.clip(lower=1)
        
        # Score: 1× = 50, 2× = 75, 4× = 100, >4× = 100
        score = np.where(
            vol_ratio >= 4.0, 100,
            np.where(vol_ratio >= 2.0, 50 + (vol_ratio - 2.0) * 12.5,
                     50 + (vol_ratio - 1.0) * 25)
        )
        
        return pd.Series(score, index=df.index).clip(0, 100)
    
    def _calculate_catalyst_score(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate catalyst score: why will this option move?
        
        Components:
        - Price Momentum (40%): Directional move in underlying
        - Volume Surge (35%): Unusual activity = information flow
        - OI Build (25%): Smart money accumulation
        
        Returns:
            Score 0-100, where 100 = strong catalyst present
        """
        # Price Momentum (40%)
        price_mom = self._calculate_price_momentum(df, options_history)
        price_mom_score = self._normalize_momentum(price_mom)
        
        # Volume Surge (35%)
        volume_surge_score = self._calculate_volume_surge(df, options_history)
        
        # OI Build (25%)
        oi_growth = self._calculate_oi_growth(df, options_history)
        oi_build_score = self._normalize_oi_growth(oi_growth)
        
        # Combined catalyst score
        catalyst_score = (
            price_mom_score * 0.40 +
            volume_surge_score * 0.35 +
            oi_build_score * 0.25
        )
        
        return catalyst_score.clip(0, 100)

    # =========================================================================
    # GREEKS SCORING (25% of total)
    # =========================================================================

    def _calculate_greeks_scores(
        self,
        df: pd.DataFrame,
        underlying_trend: str = "neutral",
    ) -> pd.DataFrame:
        """Calculate Greeks-based score for directional alignment.

        Formulas:
            delta_score = 100 - 100 × |Δ - 0.55| (target 0.55 for calls, -0.55 for puts)
            gamma_score = min(γ / 0.04 × 100, 100)
            vega_score = min(vega / 0.30 × 100, 100)
            theta_penalty = min(|θ%| × 10, 40) where θ% = θ/mid × 100

        Greeks Pre-Score = Delta × 0.50 + Gamma × 0.35 + Vega × 0.10
        Greeks Score = clip(Greeks_Pre - Theta_Penalty, 0, 100)
        """
        # Delta Score (50%)
        df["delta_score"] = self._score_delta(df, underlying_trend)

        # Gamma Score (35%)
        df["gamma_score"] = self._score_gamma(df)

        # Vega Score (10%)
        df["vega_score"] = self._score_vega(df)

        # Theta Penalty
        df["theta_penalty"] = self._calculate_theta_penalty(df)

        # Combined Greeks Score
        df["greeks_score"] = (
            df["delta_score"] * self.DELTA_WEIGHT
            + df["gamma_score"] * self.GAMMA_WEIGHT
            + df["vega_score"] * self.VEGA_WEIGHT
            - df["theta_penalty"]
        ).clip(0, 100)

        return df

    def _score_delta(self, df: pd.DataFrame, trend: str) -> pd.Series:
        """Score delta based on distance from optimal target.

        UPDATED 2026-01-23: Dynamic delta target based on DTE
        
        Formula:
            Target delta varies by DTE:
            - DTE > 45: target = 0.50 (longer-term, lower delta)
            - DTE 21-45: target = 0.55 (sweet spot)
            - DTE 7-21: target = 0.60 (near-term, higher delta)
            - DTE < 7: target = 0.65 (very short-term, deeper ITM)
            
            delta_score = 100 - 100 × |Δ - target|

        Examples for calls with 30 DTE (target 0.55):
            - Delta 0.55 → score = 100 - 0 = 100
            - Delta 0.45 → score = 100 - 10 = 90
            - Delta 0.70 → score = 100 - 15 = 85

        Trend alignment provides additional multiplier.
        """
        scores = pd.Series(50.0, index=df.index)

        for idx, row in df.iterrows():
            delta = row.get("delta", 0.5)
            side = row.get("side", "call")
            
            # Get DTE (days to expiration)
            dte = row.get("dte", row.get("days_to_expiry", 30))
            
            # Dynamic target based on DTE
            if dte > 45:
                base_target = 0.50
            elif dte > 21:
                base_target = 0.55
            elif dte > 7:
                base_target = 0.60
            else:
                base_target = 0.65

            # Target delta based on side
            if side == "call":
                target = base_target
            else:
                target = -base_target

            # Core formula: delta_score = 100 - 100 × |Δ - target|
            deviation = abs(delta - target)
            base_score = 100 - (100 * deviation)

            # Trend alignment multiplier
            if trend == "bullish" and side == "call":
                alignment_mult = 1.0
            elif trend == "bearish" and side == "put":
                alignment_mult = 1.0
            elif trend == "neutral":
                alignment_mult = 0.90  # Small penalty for no trend
            else:
                # Counter-trend (bullish but put, or bearish but call)
                alignment_mult = 0.70

            scores[idx] = max(0, min(100, base_score * alignment_mult))

        return scores

    def _score_gamma(self, df: pd.DataFrame) -> pd.Series:
        """Score gamma based on acceleration potential.

        Formula: min(gamma / 0.04 × 100, 100)
        - Gamma > 0.04: Score 100 (excellent)
        - Gamma 0.02: Score 50 (good)
        - Gamma < 0.01: Score 25 (weak)
        """
        gamma = df.get("gamma", pd.Series(0.02, index=df.index))

        return (gamma / self.GAMMA_EXCELLENT_THRESHOLD * 100).clip(0, 100)

    def _score_vega(self, df: pd.DataFrame) -> pd.Series:
        """Score vega for volatility exposure.

        For buying low IV: higher vega = better (benefits from IV expansion)
        Formula: min(vega / 0.30 × 100, 100)
        """
        vega = df.get("vega", pd.Series(0.15, index=df.index))

        return (vega / 0.30 * 100).clip(0, 100)

    def _calculate_theta_penalty(self, df: pd.DataFrame) -> pd.Series:
        """Calculate theta decay penalty.

        UPDATED 2026-01-23: Dynamic penalty cap based on DTE
        
        Formula:
            θ% = (θ / mid) × 100 (daily decay as % of price)
            theta_penalty = min(|θ%| × 10, cap)
            
            Cap varies by DTE:
            - DTE > 45: cap = 25 (less sensitive for long-term)
            - DTE 21-45: cap = 40 (standard)
            - DTE < 21: cap = 50 (more sensitive near expiration)

        Examples with 30 DTE (cap = 40):
            - θ = -$0.02, mid = $2.00 → θ% = -1% → penalty = 10
            - θ = -$0.05, mid = $1.00 → θ% = -5% → penalty = 40 (capped)
            
        Examples with 10 DTE (cap = 50):
            - θ = -$0.05, mid = $1.00 → θ% = -5% → penalty = 50 (capped)
        """
        theta = df.get("theta", pd.Series(-0.10, index=df.index)).abs()
        mid_price = df.get("mid", df.get("mark", pd.Series(1.0, index=df.index)))
        
        # Get DTE (days to expiration)
        dte = df.get("dte", df.get("days_to_expiry", pd.Series(30, index=df.index)))

        # Dynamic cap based on DTE
        def get_theta_cap(d):
            if d > 45:
                return 25.0
            elif d > 21:
                return 40.0
            else:
                return 50.0
        
        # Calculate cap per row
        if isinstance(dte, pd.Series):
            theta_cap = dte.apply(get_theta_cap)
        else:
            theta_cap = get_theta_cap(dte)

        # Avoid division by zero
        theta_pct = np.where(mid_price > 0, (theta / mid_price) * 100, 10)

        # penalty = min(|θ%| × 10, cap)
        penalty = (pd.Series(theta_pct, index=df.index).abs() * 10).clip(0, theta_cap)

        return penalty

    # =========================================================================
    # SIGNAL GENERATION
    # =========================================================================

    def _generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:
        """Generate trading signals based on framework criteria."""

        # Discount Signal: Low IV + momentum + liquid
        df["signal_discount"] = (
            (df["iv_rank"] < self.DISCOUNT_IV_RANK_THRESHOLD)
            & (df["momentum_score"] > self.DISCOUNT_MOMENTUM_THRESHOLD)
            & (df["spread_pct"] < self.DISCOUNT_SPREAD_THRESHOLD)
        )

        # Runner Signal: High momentum + volume + activity
        df["signal_runner"] = (
            (df["momentum_score"] > self.RUNNER_MOMENTUM_THRESHOLD)
            & (df.get("volume", 0) > self.RUNNER_VOLUME_THRESHOLD)
            & (df["vol_oi_ratio"] > self.RUNNER_VOL_OI_THRESHOLD)
            & (df["spread_pct"] < self.RUNNER_SPREAD_THRESHOLD)
        )

        # Greeks-Aligned Signal: Optimal delta/gamma + liquid
        df["signal_greeks"] = (
            (df["delta"].abs() >= self.OPTIMAL_DELTA_MIN)
            & (df["delta"].abs() <= self.OPTIMAL_DELTA_MAX)
            & (df.get("gamma", 0) > self.GAMMA_GOOD_THRESHOLD)
            & (df["spread_pct"] < self.GREEKS_SPREAD_THRESHOLD)
        )

        # Buy Signal: Composite threshold + at least one signal
        df["signal_buy"] = (df["composite_rank"] > self.BUY_COMPOSITE_THRESHOLD) & (
            df["signal_discount"] | df["signal_runner"] | df["signal_greeks"]
        )

        # Create signal summary
        df["signals"] = df.apply(lambda row: self._summarize_signals(row), axis=1)

        return df

    def _summarize_signals(self, row: pd.Series) -> str:
        """Create comma-separated signal summary."""
        signals = []
        if row.get("signal_discount", False):
            signals.append("DISCOUNT")
        if row.get("signal_runner", False):
            signals.append("RUNNER")
        if row.get("signal_greeks", False):
            signals.append("GREEKS")
        if row.get("signal_buy", False):
            signals.append("BUY")
        return ",".join(signals) if signals else ""

    # =========================================================================
    # ENTRY MODE RANKING (Value 40%, Catalyst 35%, Greeks 25%)
    # =========================================================================
    
    def _calculate_entry_value_score(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """Calculate entry-optimized value score.
        
        Components:
        - IV Percentile (40%): Is IV low relative to history?
        - IV Historical Discount (30%): Is it cheaper than usual?
        - Spread Quality (30%): Can you enter/exit profitably?
        
        Returns:
            DataFrame with entry_value_score column (0-100)
        """
        # IV Percentile (40%)
        iv_percentile = self._calculate_iv_percentile(df, iv_stats)
        iv_percentile_score = 100 - iv_percentile  # Low percentile = high score
        df["iv_percentile"] = iv_percentile
        df["iv_percentile_score"] = iv_percentile_score
        
        # IV Historical Discount (30%)
        iv_discount_score = self._calculate_iv_historical_discount(df, options_history)
        df["iv_discount_score"] = iv_discount_score
        
        # Spread Quality (30%)
        if "spread_pct" not in df.columns:
            df["spread_pct"] = self._calculate_spread_pct(df)
        spread_score = self._calculate_spread_score(df["spread_pct"])
        df["spread_score"] = spread_score
        
        # Combined entry value score
        df["entry_value_score"] = (
            iv_percentile_score * 0.40 +
            iv_discount_score * 0.30 +
            spread_score * 0.30
        ).clip(0, 100)
        
        return df
    
    def _rank_for_entry(
        self,
        df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """Calculate entry-optimized ranking.
        
        Weights:
        - Value: 40% (is it cheap?)
        - Catalyst: 35% (why will it move?)
        - Greeks: 25% (is it positioned right?)
        
        Returns:
            DataFrame with entry_rank column (0-100)
        """
        # Entry Value Score (40%)
        df = self._calculate_entry_value_score(df, iv_stats, options_history)
        
        # Catalyst Score (35%)
        df["catalyst_score"] = self._calculate_catalyst_score(df, options_history)
        
        # Greeks Score (25%) - reuse existing method
        if "greeks_score" not in df.columns:
            df = self._calculate_greeks_scores(df)
        
        # Combined entry rank
        df["entry_rank"] = (
            df["entry_value_score"] * self.ENTRY_MODE_WEIGHTS["value"] +
            df["catalyst_score"] * self.ENTRY_MODE_WEIGHTS["catalyst"] +
            df["greeks_score"] * self.ENTRY_MODE_WEIGHTS["greeks"]
        ).clip(0, 100)
        
        return df

    # =========================================================================
    # EXIT MODE RANKING (Profit 50%, Deterioration 30%, Time 20%)
    # =========================================================================
    
    def _calculate_profit_protection_score(
        self,
        df: pd.DataFrame,
        entry_data: Optional[dict] = None,
    ) -> pd.Series:
        """Calculate profit protection score: have I made enough?
        
        Components:
        - P&L % vs entry (50%): Profit achieved
        - IV Expansion (30%): Did IV expand as expected?
        - Price Target Hit (20%): Is underlying at target?
        
        Note: entry_data can be passed with fields:
            - entry_price: Position entry price
            - entry_iv: IV at entry
            - price_target: Target price for underlying
        
        Returns:
            Score 0-100, where 100 = strong exit signal (take profit!)
        """
        # Get entry price (if available, else estimate)
        if entry_data and 'entry_price' in entry_data:
            entry_price = entry_data['entry_price']
        else:
            # Estimate: assume 30% gain if at current price
            entry_price = df.get('mid', 1.0) * 0.70
        
        current_price = df.get('mid', pd.Series(1.0, index=df.index))
        
        # Calculate P&L %
        pnl_pct = ((current_price - entry_price) / entry_price) * 100
        
        # Score thresholds: <10%: 20, 25%: 60, 50%: 80, 100%: 95, >100%: 100
        pnl_score = np.where(
            pnl_pct >= 100, 100,
            np.where(pnl_pct >= 50, 80 + (pnl_pct - 50) * 0.3,
            np.where(pnl_pct >= 25, 60 + (pnl_pct - 25) * 0.8,
            np.where(pnl_pct >= 10, 40 + (pnl_pct - 10) * 1.33,
                     20 + pnl_pct * 2)))
        )
        
        # IV expansion bonus (30% of score)
        if entry_data and 'entry_iv' in entry_data:
            entry_iv = entry_data['entry_iv']
        else:
            entry_iv = df.get('iv', 0.30) * 0.9  # Assume IV expanded 10%
        
        current_iv = df.get('iv', pd.Series(0.30, index=df.index))
        iv_expansion = ((current_iv - entry_iv) / entry_iv) * 100
        iv_bonus = np.clip(iv_expansion * 2, 0, 30)
        
        # Price target hit bonus (20% of score)
        if entry_data and 'price_target' in entry_data:
            price_target = entry_data['price_target']
            underlying_price = df.get('underlying_price', 0)
            target_bonus = np.where(underlying_price >= price_target, 20, 0)
        else:
            target_bonus = 0
        
        # Combined profit protection score
        total_score = pnl_score * 0.50 + iv_bonus * 0.30 + target_bonus * 0.20
        
        return pd.Series(total_score, index=df.index).clip(0, 100)
    
    def _calculate_deterioration_score(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
    ) -> pd.Series:
        """Calculate deterioration score: is momentum fading?
        
        Components:
        - Momentum Level (40%): Low momentum = deteriorating
        - Volume Drying Up (30%): Volume decreasing from average
        - OI Stalling (30%): OI growth slowing/negative
        
        Returns:
            Score 0-100, where 100 = strong deterioration (exit now!)
        """
        # Momentum Decay (40%)
        # Use existing momentum score - low momentum = high deterioration
        if "momentum_score" in df.columns:
            # Invert: low momentum score = high deterioration
            decay_score = 100 - df["momentum_score"]
        else:
            # Calculate momentum if not already done
            price_mom = self._calculate_price_momentum(df, options_history)
            mom_score = self._normalize_momentum(price_mom)
            decay_score = 100 - mom_score
        
        # Volume Drying Up (30%)
        volume_surge = self._calculate_volume_surge(df, options_history)
        # Invert: low volume surge = high deterioration score
        volume_score = 100 - volume_surge
        
        # OI Stalling (30%)
        oi_growth = self._calculate_oi_growth(df, options_history)
        oi_growth_score = self._normalize_oi_growth(oi_growth)
        # Invert: low OI growth = high deterioration
        oi_score = 100 - oi_growth_score
        
        # Combined deterioration score
        deterioration = (
            decay_score * 0.40 +
            volume_score * 0.30 +
            oi_score * 0.30
        )
        
        return deterioration.clip(0, 100)
    
    def _calculate_time_urgency_score(self, df: pd.DataFrame) -> pd.Series:
        """Calculate time urgency score: am I running out of time?
        
        Components:
        - DTE Urgency (60%): Days remaining until expiration
        - Theta Burn Rate (40%): Daily decay as % of position value
        
        Returns:
            Score 0-100, where 100 = urgent exit (expiration soon!)
        """
        # DTE Urgency (60%)
        dte = df.get('dte', df.get('days_to_expiry', pd.Series(30, index=df.index)))
        
        # DTE score: >30: 20, 14-30: 40, 7-14: 70, <7: 95
        dte_score = np.where(
            dte < 7, 95,
            np.where(dte < 14, 70 + (14 - dte) * 3.57,
            np.where(dte < 30, 40 + (30 - dte) * 1.875,
                     20))
        )
        
        # Theta Burn Rate (40%)
        theta = df.get('theta', pd.Series(-0.10, index=df.index)).abs()
        mid_price = df.get('mid', pd.Series(1.0, index=df.index))
        theta_pct = (theta / mid_price.clip(lower=0.01)) * 100  # Daily decay %
        
        # Theta score: >3%: 100, 2-3%: 80, 1-2%: 60, <1%: 30
        theta_score = np.where(
            theta_pct >= 3.0, 100,
            np.where(theta_pct >= 2.0, 80 + (theta_pct - 2.0) * 20,
            np.where(theta_pct >= 1.0, 60 + (theta_pct - 1.0) * 20,
                     30 + theta_pct * 30))
        )
        
        # Combined time urgency score
        time_urgency = (
            pd.Series(dte_score, index=df.index) * 0.60 +
            pd.Series(theta_score, index=df.index) * 0.40
        )
        
        return time_urgency.clip(0, 100)
    
    def _rank_for_exit(
        self,
        df: pd.DataFrame,
        options_history: Optional[pd.DataFrame] = None,
        entry_data: Optional[dict] = None,
    ) -> pd.DataFrame:
        """Calculate exit-optimized ranking.
        
        Weights:
        - Profit Protection: 50% (have I made enough?)
        - Deterioration: 30% (is momentum fading?)
        - Time Urgency: 20% (am I running out of time?)
        
        Returns:
            DataFrame with exit_rank column (0-100)
        """
        # Calculate momentum score if needed for deterioration detection
        if "momentum_score" not in df.columns:
            df = self._calculate_momentum_scores(df, options_history)
        
        # Profit Protection Score (50%)
        df["profit_protection_score"] = self._calculate_profit_protection_score(df, entry_data)
        
        # Deterioration Score (30%) - relies on momentum_score
        df["deterioration_score"] = self._calculate_deterioration_score(df, options_history)
        
        # Time Urgency Score (20%)
        df["time_urgency_score"] = self._calculate_time_urgency_score(df)
        
        # Combined exit rank
        df["exit_rank"] = (
            df["profit_protection_score"] * self.EXIT_MODE_WEIGHTS["profit_protection"] +
            df["deterioration_score"] * self.EXIT_MODE_WEIGHTS["deterioration"] +
            df["time_urgency_score"] * self.EXIT_MODE_WEIGHTS["time_urgency"]
        ).clip(0, 100)
        
        return df

    # =========================================================================
    # UTILITY METHODS
    # =========================================================================

    def _log_ranking_stats(self, df: pd.DataFrame) -> None:
        """Log ranking statistics."""
        logger.info(f"Ranked {len(df)} options contracts")
        logger.info(
            f"Composite range: {df['composite_rank'].min():.1f} - {df['composite_rank'].max():.1f}"
        )

        # Signal counts
        discount_count = df["signal_discount"].sum()
        runner_count = df["signal_runner"].sum()
        greeks_count = df["signal_greeks"].sum()
        buy_count = df["signal_buy"].sum()

        logger.info(
            f"Signals: DISCOUNT={discount_count}, RUNNER={runner_count}, "
            f"GREEKS={greeks_count}, BUY={buy_count}"
        )

        # Top 3 opportunities
        if len(df) >= 3:
            top3 = df.head(3)
            logger.info("Top 3 opportunities:")
            for _, row in top3.iterrows():
                logger.info(
                    f"  {row.get('contract_symbol', 'N/A')}: "
                    f"Rank={row['composite_rank']:.1f}, "
                    f"Signals=[{row['signals']}]"
                )

    def get_interpretation(self, composite_rank: float) -> str:
        """Get human-readable interpretation of composite rank."""
        if composite_rank >= 80:
            return "Exceptional opportunity (all dimensions strong)"
        elif composite_rank >= 65:
            return "Good opportunity (two+ dimensions strong)"
        elif composite_rank >= 50:
            return "Fair opportunity (mixed signals)"
        else:
            return "Poor opportunity (one+ dimension weak)"

    def create_aggressive_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for aggressive momentum trading."""
        return OptionsMomentumRanker(
            momentum_weight=0.60,
            value_weight=0.25,
            greeks_weight=0.15,
        )

    def create_conservative_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for conservative value hunting."""
        return OptionsMomentumRanker(
            momentum_weight=0.20,
            value_weight=0.60,
            greeks_weight=0.20,
        )

    def create_greeks_focused_config(self) -> "OptionsMomentumRanker":
        """Create ranker configured for Greeks-focused risk management."""
        return OptionsMomentumRanker(
            momentum_weight=0.25,
            value_weight=0.30,
            greeks_weight=0.45,
        )


class CalibratedMomentumRanker(OptionsMomentumRanker):
    """
    Extended ranker with calibration and regime-conditioning.

    Implements Perplexity's recommendations:
    1. Calibration-to-forward-return using isotonic regression
    2. Regime-conditioning weights based on trend/vol regime
    3. Integration with ranking monitor for alerts
    """

    def __init__(
        self,
        momentum_weight: float = 0.40,
        value_weight: float = 0.35,
        greeks_weight: float = 0.25,
        enable_calibration: bool = True,
        enable_regime_conditioning: bool = True,
    ):
        """
        Initialize calibrated ranker.

        Args:
            momentum_weight: Base momentum weight
            value_weight: Base value weight
            greeks_weight: Base greeks weight
            enable_calibration: Apply isotonic calibration
            enable_regime_conditioning: Adjust weights by regime
        """
        super().__init__(momentum_weight, value_weight, greeks_weight)

        self.enable_calibration = enable_calibration
        self.enable_regime_conditioning = enable_regime_conditioning

        self._calibrator = None
        self._regime_conditioner = None

        if enable_calibration:
            from .ranking_calibrator import IsotonicCalibrator

            self._calibrator = IsotonicCalibrator()

        if enable_regime_conditioning:
            from .regime_conditioner import RegimeConditioner

            self._regime_conditioner = RegimeConditioner()

    def fit_calibrator(
        self,
        historical_scores: np.ndarray,
        forward_returns: np.ndarray,
    ) -> None:
        """
        Fit the calibrator on historical data.

        Should be called periodically (e.g., weekly) with recent data.

        Args:
            historical_scores: Past composite_rank values
            forward_returns: Corresponding forward returns
        """
        if self._calibrator is None:
            logger.warning("Calibration disabled, skipping fit")
            return

        result = self._calibrator.fit(historical_scores, forward_returns)
        logger.info(f"Calibrator fitted: {result}")

    def rank_options_calibrated(
        self,
        options_df: pd.DataFrame,
        iv_stats: Optional[IVStatistics] = None,
        options_history: Optional[pd.DataFrame] = None,
        underlying_df: Optional[pd.DataFrame] = None,
        underlying_trend: str = "neutral",
        previous_rankings: Optional[pd.DataFrame] = None,
        ranking_mode: str = "entry",
        underlying_metrics: Optional[dict] = None,
        mode: Optional[RankingMode] = None,
        entry_data: Optional[dict] = None,
        strategy_intent: Optional[StrategyIntent] = None,
        term_structure_features: Optional[dict] = None,
        earnings_jump_features: Optional[dict] = None,
        menthorq_features: Optional[dict] = None,
    ) -> pd.DataFrame:
        """
        Rank options with calibration and regime-conditioning.

        This is the enhanced entry point that:
        1. Detects market regime from underlying OHLC
        2. Adjusts weights based on regime
        3. Applies base ranking with 7-day underlying metrics
        4. Calibrates scores to forward return percentiles

        Args:
            options_df: Options chain data
            iv_stats: IV statistics
            options_history: Historical options data
            underlying_df: OHLC data for regime detection
            underlying_trend: Fallback trend if no OHLC
            previous_rankings: Previous rankings for smoothing
            ranking_mode: 'entry', 'exit', or 'monitor' (string, legacy)
            underlying_metrics: 7-day underlying metrics (ret_7d, vol_7d, etc.)
            mode: RankingMode enum (takes precedence over ranking_mode string)
            entry_data: Entry data for EXIT mode (entry_price, entry_iv, price_target)
            strategy_intent: long_premium vs short_premium for dual-intent scoring
            term_structure_features: forward_vol, term_structure_regime, low_confidence
            earnings_jump_features: event_1sigma_move
            menthorq_features: GEX/DEX buckets, VRP, skew_proxy

        Returns:
            DataFrame with calibrated rankings
        """
        # Convert ranking_mode string to RankingMode enum if mode not provided
        if mode is None:
            mode = RankingMode[ranking_mode.upper()]
        # Step 1: Detect regime and get conditioned weights
        regime_state = None
        if self.enable_regime_conditioning and underlying_df is not None:
            if len(underlying_df) >= 20:
                regime_state = self._regime_conditioner.detect_regime(underlying_df)
                weights = self._regime_conditioner.get_regime_weights(regime_state)

                # Temporarily override weights
                orig_momentum = self.MOMENTUM_WEIGHT
                orig_value = self.VALUE_WEIGHT
                orig_greeks = self.GREEKS_WEIGHT

                self.MOMENTUM_WEIGHT = weights.momentum
                self.VALUE_WEIGHT = weights.value
                self.GREEKS_WEIGHT = weights.greeks

                logger.info(
                    f"Regime: {regime_state.trend_regime.value}/"
                    f"{regime_state.vol_regime.value}, "
                    f"Weights: M={weights.momentum:.0%}, "
                    f"V={weights.value:.0%}, G={weights.greeks:.0%}"
                )

        # Step 2: Apply base ranking with underlying metrics
        ranked_df = self.rank_options(
            options_df,
            mode=mode,
            iv_stats=iv_stats,
            options_history=options_history,
            underlying_trend=underlying_trend,
            previous_rankings=previous_rankings,
            underlying_metrics=underlying_metrics,
            entry_data=entry_data,
            strategy_intent=strategy_intent,
            term_structure_features=term_structure_features,
            earnings_jump_features=earnings_jump_features,
            menthorq_features=menthorq_features,
        )

        # Restore original weights if we changed them
        if regime_state is not None:
            self.MOMENTUM_WEIGHT = orig_momentum
            self.VALUE_WEIGHT = orig_value
            self.GREEKS_WEIGHT = orig_greeks

        # Step 3: Apply calibration
        if self.enable_calibration and self._calibrator._is_fitted:
            ranked_df = self._calibrator.calibrate_rankings(ranked_df, score_col="composite_rank")

            # Use calibrated rank as primary sort
            ranked_df = ranked_df.sort_values("calibrated_rank", ascending=False).reset_index(
                drop=True
            )

            logger.info(
                f"Calibrated: range {ranked_df['calibrated_rank'].min():.1f}-"
                f"{ranked_df['calibrated_rank'].max():.1f}, "
                f"P(+) range {ranked_df['calibrated_positive_prob'].min():.2f}-"
                f"{ranked_df['calibrated_positive_prob'].max():.2f}"
            )
        else:
            # Add placeholder columns if not calibrated
            ranked_df["calibrated_return_pct"] = ranked_df["composite_rank"]
            ranked_df["calibrated_positive_prob"] = 0.5
            ranked_df["calibrated_rank"] = ranked_df["composite_rank"]

        # Step 4: Add regime metadata
        if regime_state is not None:
            ranked_df["trend_regime"] = regime_state.trend_regime.value
            ranked_df["vol_regime"] = regime_state.vol_regime.value
            ranked_df["regime_adx"] = regime_state.adx
            ranked_df["regime_atr_pct"] = regime_state.atr_pct

            # Apply signal emphasis based on regime
            emphasis = self._regime_conditioner.get_signal_emphasis(regime_state)
            ranked_df["signal_emphasis_runner"] = emphasis["runner"]
            ranked_df["signal_emphasis_discount"] = emphasis["discount"]
            ranked_df["signal_emphasis_greeks"] = emphasis["greeks"]

        return ranked_df

    def save_calibrator(self, path: str) -> None:
        """Save calibrator state to file."""
        if self._calibrator is not None:
            self._calibrator.save(path)

    def load_calibrator(self, path: str) -> None:
        """Load calibrator state from file."""
        if self._calibrator is not None:
            from .ranking_calibrator import IsotonicCalibrator

            self._calibrator = IsotonicCalibrator.load(path)


class IVHistoryCalculator:
    """Calculate IV statistics from historical data."""

    @staticmethod
    def calculate_iv_stats(
        iv_history: pd.DataFrame,
        current_iv: float,
        lookback_days: int = 252,
    ) -> IVStatistics:
        """Calculate 52-week IV statistics.

        Args:
            iv_history: DataFrame with 'date' and 'iv' columns
            current_iv: Current implied volatility
            lookback_days: Days to look back (default 252 = 1 year)

        Returns:
            IVStatistics with high, low, median, current, and rank
        """
        if iv_history is None or iv_history.empty:
            # Return default stats if no history
            return IVStatistics(
                iv_high=current_iv * 1.5,
                iv_low=current_iv * 0.5,
                iv_median=current_iv,
                iv_current=current_iv,
                days_of_data=0,
            )

        # Sort by date and take last N days
        df = iv_history.copy()
        if "date" in df.columns:
            df = df.sort_values("date", ascending=False)
        df = df.head(lookback_days)

        iv_col = "iv" if "iv" in df.columns else "implied_volatility"

        return IVStatistics(
            iv_high=df[iv_col].max(),
            iv_low=df[iv_col].min(),
            iv_median=df[iv_col].median(),
            iv_current=current_iv,
            days_of_data=len(df),
        )

    @staticmethod
    def calculate_iv_percentile(
        iv_history: pd.DataFrame,
        current_iv: float,
    ) -> float:
        """Calculate IV Percentile: % of days IV was below current.

        IVP = (Number of days with IV < current) / Total days × 100
        """
        if iv_history is None or iv_history.empty:
            return 50.0

        iv_col = "iv" if "iv" in iv_history.columns else "implied_volatility"
        below_count = (iv_history[iv_col] < current_iv).sum()
        total = len(iv_history)

        if total == 0:
            return 50.0

        return (below_count / total) * 100
